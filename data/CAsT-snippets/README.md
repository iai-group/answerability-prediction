# CAsT-snippets dataset

This directory contains data from an existing dataset, referred to as **CAsT-snippets**[^1] which extends the TREC CAsT'20 and '22 datasets with snippet-level annotations for the top-retrieved results. Specifically, it contains annotations of information nuggets defined as _minimal, atomic units of relevant information_[^2], representing key pieces of information required to answer the given question. Snippets are identified for every question in the 5 most relevant passages according to ground truth relevance judgments. 

Data from CAsT-snippets dataset is aggregated and preprocessed to be used for answerability prediction. For sentence-level answerability, each sentence that overlaps with an information nugget, as per annotations in the originating CAsT-snippets dataset, is labeled as 1 (answer in the sentence), otherwise as 0 (no answer in the sentence). All the functions for building sentence-level answerability predictions training data can be found in [training_data.py](../../answerability_prediction/sentence_classification/training_data.py). Functions for the processing of passage-level answerability data can be found in [data_processing.py](../../answerability_prediction/utils/data_processing.py).

The following files can be found in this directory:
  - [2020_snippets_data.csv](2020/2020_snippets_data.csv) - all annotated data from CAsT-snippets dataset for 2020 aggregated into one file (analogous file for 2022 data - [2022_snippets_data.csv](2022/2022_snippets_data.csv)).
  - [snippets_data.csv](snippets_data.csv) - a concatenation of [2020_snippets_data.csv](2020/2020_snippets_data.csv) and [2022_snippets_data.csv](2022/2022_snippets_data.csv) files.
  - [2020_snippets_data_sentence-answerability.csv](2020/2020_snippets_data_sentence-answerability.csv) - annotated data from CAsT-snippets dataset for 2020 with additional information about sentences that were selected by annotators in CAsT-snippets dataset and sentences that were not selected (analogous file for 2022 data - [2022_snippets_data_sentence-answerability.csv](2022/2022_snippets_data_sentence-answerability.csv)).
  - [snippets_data_sentence-answerability.csv](snippets_data_sentence-answerability.csv) - a concatenation of [2020_snippets_data_sentence-answerability.csv](2020/2020_snippets_data_sentence-answerability.csv) and [2022_snippets_data_sentence-answerability.csv](2022/2022_snippets_data_sentence-answerability.csv) files extended with partition information obtained from sentence-level training data, information about number of annotators that selected "no answer" for each query-passage pair, and binary passage-level answerability score (query-passage pair is considered answerable if at least one snippet is annotated). 
  - [training_data.csv](training_data.csv) - sentence-level training data generated from [snippets_data_sentence-answerability.csv](snippets_data_sentence-answerability.csv) divided into 3 partitions (across topics) where each data sample has a form of `query [SEP] sentence` and the labels are answerability scores based on snippet annotations from CAsT-snippets. Each sentence that overlaps with an information nugget is labeled as 1 (answer in the sentence), otherwise as 0 (no answer in the sentence).

[^1]: Reference and link to the GitHub repository are removed to preserve anonymity; it will be added upon paper acceptance.

[^2]: Virgil Pavlu, Shahzad Rajput, Peter B. Golbus, and Javed A. Aslam. 2012. Ir system evaluation using nugget-based test collections. In Proceedings of the fifth ACM international conference on Web search and data mining, WSDM ’12, pages 393–402