HITId,HITTypeId,Title,Description,Keywords,Reward,CreationTime,MaxAssignments,RequesterAnnotation,AssignmentDurationInSeconds,AutoApprovalDelayInSeconds,Expiration,NumberOfSimilarHITs,LifetimeInSeconds,AssignmentId,WorkerId,AssignmentStatus,AcceptTime,SubmitTime,AutoApprovalTime,ApprovalTime,RejectionTime,RequesterFeedback,WorkTimeInSeconds,LifetimeApprovalRate,Last30DaysApprovalRate,Last7DaysApprovalRate,Input.turn_id,Input.Q0,Input.passage_id,Input.relevance_score,Input.passage,Input.labels,Input.query,Answer.taskAnswers,Approve,Reject,text_spans_2
34KYK9TV3VNU4WYPA5LC5O5HTKISB8,3I3MQQJ31WXKIN6QF6OQSRUDQS81J6,Identify relevant text spans in text passage,Identify all the text spans that contain key pieces of the answer to a given question,"text, text spans, relevance",$0.30,Fri May 19 03:52:59 PDT 2023,3,BatchId:5076839;OriginalHitTemplateId:929448123;,21600,259200,Mon May 22 03:52:59 PDT 2023,,,3COPXFW7XGR17RKLE3ZDY6FN7UFPK8,worker_119,Submitted,Fri May 19 14:29:43 PDT 2023,Fri May 19 14:31:30 PDT 2023,Mon May 22 14:31:30 PDT 2023,,,,107,100% (270/271),100% (179/179),100% (178/178),104_1,0,CAR_2564e6dfc8f5f799e30393abc532775e8886e415,3,"In a paper to be published in proceedings the 2010 ACM Special Interest Group on Information Retrieval Conference, White and Horvitz present research on predicting escalations in medical concerns based on the structure and content of Web pages encountered during medical search sessions. They construct and then characterize the performance of classifiers that predict whether an escalation will occur in issued queries following the visit to a page. Their findings show that features such as serious illness preceding benign explanations in page (e.g., cancer is mentioned before caffeine in pages pertaining to headaches), serious illness vs. benign explanation appears in page title or near beginning of page, page from Web forum, and page has external verification are all important predictors of subsequent escalation (or non-escalation).",,Who are some of the well-known Information Retrieval researchers?,"[{""answer_confidence"":{""high"":True,""low"":False,""medium"":False,""very_high"":False,""very_low"":False},""passage-snippets-annotation-qualification-task"":{""entities"":[{""endOffset"":120,""label"":""relevant-text-span"",""startOffset"":115},{""endOffset"":132,""label"":""relevant-text-span"",""startOffset"":125}]}}]",x,,"['White', 'Horvitz']"
34KYK9TV3VNU4WYPA5LC5O5HTKISB8,3I3MQQJ31WXKIN6QF6OQSRUDQS81J6,Identify relevant text spans in text passage,Identify all the text spans that contain key pieces of the answer to a given question,"text, text spans, relevance",$0.30,Fri May 19 03:52:59 PDT 2023,3,BatchId:5076839;OriginalHitTemplateId:929448123;,21600,259200,Mon May 22 03:52:59 PDT 2023,,,3TK8OJTYM60FN2EXD7DK0FQ4NTCVPS,worker_232,Submitted,Fri May 19 18:45:19 PDT 2023,Fri May 19 18:56:40 PDT 2023,Mon May 22 18:56:40 PDT 2023,,,,681,100% (208/208),100% (179/179),100% (178/178),104_1,0,CAR_2564e6dfc8f5f799e30393abc532775e8886e415,3,"In a paper to be published in proceedings the 2010 ACM Special Interest Group on Information Retrieval Conference, White and Horvitz present research on predicting escalations in medical concerns based on the structure and content of Web pages encountered during medical search sessions. They construct and then characterize the performance of classifiers that predict whether an escalation will occur in issued queries following the visit to a page. Their findings show that features such as serious illness preceding benign explanations in page (e.g., cancer is mentioned before caffeine in pages pertaining to headaches), serious illness vs. benign explanation appears in page title or near beginning of page, page from Web forum, and page has external verification are all important predictors of subsequent escalation (or non-escalation).",,Who are some of the well-known Information Retrieval researchers?,"[{""answer_confidence"":{""high"":False,""low"":False,""medium"":True,""very_high"":False,""very_low"":False},""passage-snippets-annotation-qualification-task"":{""entities"":[{""endOffset"":287,""label"":""relevant-text-span"",""startOffset"":115}]}}]",x,,['White and Horvitz present research on predicting escalations in medical concerns based on the structure and content of Web pages encountered during medical search sessions.']
34KYK9TV3VNU4WYPA5LC5O5HTKISB8,3I3MQQJ31WXKIN6QF6OQSRUDQS81J6,Identify relevant text spans in text passage,Identify all the text spans that contain key pieces of the answer to a given question,"text, text spans, relevance",$0.30,Fri May 19 03:52:59 PDT 2023,3,BatchId:5076839;OriginalHitTemplateId:929448123;,21600,259200,Mon May 22 03:52:59 PDT 2023,,,3137ONMDKLKS8EZCFYYN62OQM5QEG1,worker_249,Submitted,Fri May 19 08:23:30 PDT 2023,Fri May 19 08:24:18 PDT 2023,Mon May 22 08:24:18 PDT 2023,,,,48,100% (194/194),100% (191/191),100% (190/190),104_1,0,CAR_2564e6dfc8f5f799e30393abc532775e8886e415,3,"In a paper to be published in proceedings the 2010 ACM Special Interest Group on Information Retrieval Conference, White and Horvitz present research on predicting escalations in medical concerns based on the structure and content of Web pages encountered during medical search sessions. They construct and then characterize the performance of classifiers that predict whether an escalation will occur in issued queries following the visit to a page. Their findings show that features such as serious illness preceding benign explanations in page (e.g., cancer is mentioned before caffeine in pages pertaining to headaches), serious illness vs. benign explanation appears in page title or near beginning of page, page from Web forum, and page has external verification are all important predictors of subsequent escalation (or non-escalation).",,Who are some of the well-known Information Retrieval researchers?,"[{""answer_confidence"":{""high"":False,""low"":False,""medium"":False,""very_high"":True,""very_low"":False},""passage-snippets-annotation-qualification-task"":{""entities"":[{""endOffset"":121,""label"":""relevant-text-span"",""startOffset"":114},{""endOffset"":133,""label"":""relevant-text-span"",""startOffset"":125}]}}]",x,,"[' White ', 'Horvitz ']"
3N3WJQXEMW5DC63373V0S9OI7KXL2L,3I3MQQJ31WXKIN6QF6OQSRUDQS81J6,Identify relevant text spans in text passage,Identify all the text spans that contain key pieces of the answer to a given question,"text, text spans, relevance",$0.30,Fri May 19 03:52:59 PDT 2023,3,BatchId:5076839;OriginalHitTemplateId:929448123;,21600,259200,Mon May 22 03:52:59 PDT 2023,,,34X6J5FLPYDRWJFIWVEQXW5KX12QJ2,worker_119,Submitted,Fri May 19 14:41:58 PDT 2023,Fri May 19 14:43:53 PDT 2023,Mon May 22 14:43:53 PDT 2023,,,,115,100% (270/271),100% (179/179),100% (178/178),104_1,0,CAR_3fbea6438e17d2872370fd2ab22a9a1deb25205d,4,"Wherever there have been large collections of information, whether on paper or in computers, scholars have faced a challenge in pinpointing the items they seek. The use of classification schemes to arrange the documents in order was only a partial solution. Another approach was to index the contents of the documents using words or terms, rather than classification codes. In the 1940s and 1950s some pioneers, such as Calvin Mooers, Charles L. Bernier, Evan J. Crane and Hans Peter Luhn, collected up their index terms in various kinds of list that they called a “thesaurus” (by analogy with the well known thesaurus developed by Peter Roget). The first such list put seriously to use in information retrieval was the thesaurus developed in 1959 at the E I Dupont de Nemours Company.",,Who are some of the well-known Information Retrieval researchers?,"[{""answer_confidence"":{""high"":True,""low"":False,""medium"":False,""very_high"":False,""very_low"":False},""passage-snippets-annotation-qualification-task"":{""entities"":[{""endOffset"":433,""label"":""relevant-text-span"",""startOffset"":420},{""endOffset"":453,""label"":""relevant-text-span"",""startOffset"":435},{""endOffset"":468,""label"":""relevant-text-span"",""startOffset"":455},{""endOffset"":488,""label"":""relevant-text-span"",""startOffset"":473}]}}]",x,,"['Calvin Mooers', 'Charles L. Bernier', 'Evan J. Crane', 'Hans Peter Luhn']"
3N3WJQXEMW5DC63373V0S9OI7KXL2L,3I3MQQJ31WXKIN6QF6OQSRUDQS81J6,Identify relevant text spans in text passage,Identify all the text spans that contain key pieces of the answer to a given question,"text, text spans, relevance",$0.30,Fri May 19 03:52:59 PDT 2023,3,BatchId:5076839;OriginalHitTemplateId:929448123;,21600,259200,Mon May 22 03:52:59 PDT 2023,,,3B837J3LDTBK7ND8W4C4UN6CP76SRC,worker_232,Submitted,Fri May 19 18:45:06 PDT 2023,Fri May 19 18:48:43 PDT 2023,Mon May 22 18:48:43 PDT 2023,,,,217,100% (208/208),100% (179/179),100% (178/178),104_1,0,CAR_3fbea6438e17d2872370fd2ab22a9a1deb25205d,4,"Wherever there have been large collections of information, whether on paper or in computers, scholars have faced a challenge in pinpointing the items they seek. The use of classification schemes to arrange the documents in order was only a partial solution. Another approach was to index the contents of the documents using words or terms, rather than classification codes. In the 1940s and 1950s some pioneers, such as Calvin Mooers, Charles L. Bernier, Evan J. Crane and Hans Peter Luhn, collected up their index terms in various kinds of list that they called a “thesaurus” (by analogy with the well known thesaurus developed by Peter Roget). The first such list put seriously to use in information retrieval was the thesaurus developed in 1959 at the E I Dupont de Nemours Company.",,Who are some of the well-known Information Retrieval researchers?,"[{""answer_confidence"":{""high"":True,""low"":False,""medium"":False,""very_high"":False,""very_low"":False},""passage-snippets-annotation-qualification-task"":{""entities"":[{""endOffset"":576,""label"":""relevant-text-span"",""startOffset"":376}]}}]",x,,"[' the 1940s and 1950s some pioneers, such as Calvin Mooers, Charles L. Bernier, Evan J. Crane and Hans Peter Luhn, collected up their index terms in various kinds of list that they called a thesaurus']"
3N3WJQXEMW5DC63373V0S9OI7KXL2L,3I3MQQJ31WXKIN6QF6OQSRUDQS81J6,Identify relevant text spans in text passage,Identify all the text spans that contain key pieces of the answer to a given question,"text, text spans, relevance",$0.30,Fri May 19 03:52:59 PDT 2023,3,BatchId:5076839;OriginalHitTemplateId:929448123;,21600,259200,Mon May 22 03:52:59 PDT 2023,,,3MX2NQ3YCE93YH19T75OQUI8C2N5XJ,worker_249,Submitted,Fri May 19 08:21:39 PDT 2023,Fri May 19 08:23:27 PDT 2023,Mon May 22 08:23:27 PDT 2023,,,,108,100% (194/194),100% (191/191),100% (190/190),104_1,0,CAR_3fbea6438e17d2872370fd2ab22a9a1deb25205d,4,"Wherever there have been large collections of information, whether on paper or in computers, scholars have faced a challenge in pinpointing the items they seek. The use of classification schemes to arrange the documents in order was only a partial solution. Another approach was to index the contents of the documents using words or terms, rather than classification codes. In the 1940s and 1950s some pioneers, such as Calvin Mooers, Charles L. Bernier, Evan J. Crane and Hans Peter Luhn, collected up their index terms in various kinds of list that they called a “thesaurus” (by analogy with the well known thesaurus developed by Peter Roget). The first such list put seriously to use in information retrieval was the thesaurus developed in 1959 at the E I Dupont de Nemours Company.",,Who are some of the well-known Information Retrieval researchers?,"[{""answer_confidence"":{""high"":False,""low"":False,""medium"":False,""very_high"":True,""very_low"":False},""passage-snippets-annotation-qualification-task"":{""entities"":[{""endOffset"":433,""label"":""relevant-text-span"",""startOffset"":420},{""endOffset"":453,""label"":""relevant-text-span"",""startOffset"":435},{""endOffset"":468,""label"":""relevant-text-span"",""startOffset"":454},{""endOffset"":488,""label"":""relevant-text-span"",""startOffset"":473},{""endOffset"":643,""label"":""relevant-text-span"",""startOffset"":632}]}}]",x,,"['Calvin Mooers', 'Charles L. Bernier', ' Evan J. Crane', 'Hans Peter Luhn', 'Peter Roget']"
37SOB9Z0TWC2XOZMB395BJTADYXL3W,3I3MQQJ31WXKIN6QF6OQSRUDQS81J6,Identify relevant text spans in text passage,Identify all the text spans that contain key pieces of the answer to a given question,"text, text spans, relevance",$0.30,Fri May 19 03:52:59 PDT 2023,3,BatchId:5076839;OriginalHitTemplateId:929448123;,21600,259200,Mon May 22 03:52:59 PDT 2023,,,31QTRG6Q2YSWRW92II047EUY596YPU,worker_119,Submitted,Fri May 19 11:14:34 PDT 2023,Fri May 19 11:28:49 PDT 2023,Mon May 22 11:28:49 PDT 2023,,,,855,100% (270/271),100% (179/179),100% (178/178),104_1,0,CAR_89b60de1e4179304c422dce672387e3ba88620aa,3,"Because of its emphasis in using human intelligence in the information retrieval process, HCIR requires different evaluation models – one that combines evaluation of the IR and HCI components of the system. A key area of research in HCIR involves evaluation of these systems. Early work on interactive information retrieval, such as Juergen Koenemann and Nicholas J. Belkin's 1996 study of different levels of interaction for automatic query reformulation, leverage the standard IR measures of precision and recall but apply them to the results of multiple iterations of user interaction, rather than to a single query response. Other HCIR research, such as Pia Borlund's IIR evaluation model, applies a methodology more reminiscent of HCI, focusing on the characteristics of users, the details of experimental design, etc.",,Who are some of the well-known Information Retrieval researchers?,"[{""answer_confidence"":{""high"":True,""low"":False,""medium"":False,""very_high"":False,""very_low"":False},""passage-snippets-annotation-qualification-task"":{""entities"":[{""endOffset"":350,""label"":""relevant-text-span"",""startOffset"":333},{""endOffset"":373,""label"":""relevant-text-span"",""startOffset"":355},{""endOffset"":669,""label"":""relevant-text-span"",""startOffset"":658}]}}]",x,,"['Juergen Koenemann', 'Nicholas J. Belkin', 'Pia Borlund']"
37SOB9Z0TWC2XOZMB395BJTADYXL3W,3I3MQQJ31WXKIN6QF6OQSRUDQS81J6,Identify relevant text spans in text passage,Identify all the text spans that contain key pieces of the answer to a given question,"text, text spans, relevance",$0.30,Fri May 19 03:52:59 PDT 2023,3,BatchId:5076839;OriginalHitTemplateId:929448123;,21600,259200,Mon May 22 03:52:59 PDT 2023,,,336KAV9KYV70ZP8WGKPSB1M5U0MY2H,worker_232,Submitted,Fri May 19 18:45:05 PDT 2023,Fri May 19 18:48:21 PDT 2023,Mon May 22 18:48:21 PDT 2023,,,,196,100% (208/208),100% (179/179),100% (178/178),104_1,0,CAR_89b60de1e4179304c422dce672387e3ba88620aa,3,"Because of its emphasis in using human intelligence in the information retrieval process, HCIR requires different evaluation models – one that combines evaluation of the IR and HCI components of the system. A key area of research in HCIR involves evaluation of these systems. Early work on interactive information retrieval, such as Juergen Koenemann and Nicholas J. Belkin's 1996 study of different levels of interaction for automatic query reformulation, leverage the standard IR measures of precision and recall but apply them to the results of multiple iterations of user interaction, rather than to a single query response. Other HCIR research, such as Pia Borlund's IIR evaluation model, applies a methodology more reminiscent of HCI, focusing on the characteristics of users, the details of experimental design, etc.",,Who are some of the well-known Information Retrieval researchers?,"[{""answer_confidence"":{""high"":False,""low"":False,""medium"":True,""very_high"":False,""very_low"":False},""passage-snippets-annotation-qualification-task"":{""entities"":[{""endOffset"":455,""label"":""relevant-text-span"",""startOffset"":276},{""endOffset"":692,""label"":""relevant-text-span"",""startOffset"":629}]}}]",x,,"[""Early work on interactive information retrieval, such as Juergen Koenemann and Nicholas J. Belkin's 1996 study of different levels of interaction for automatic query reformulation"", ""Other HCIR research, such as Pia Borlund's IIR evaluation model""]"
37SOB9Z0TWC2XOZMB395BJTADYXL3W,3I3MQQJ31WXKIN6QF6OQSRUDQS81J6,Identify relevant text spans in text passage,Identify all the text spans that contain key pieces of the answer to a given question,"text, text spans, relevance",$0.30,Fri May 19 03:52:59 PDT 2023,3,BatchId:5076839;OriginalHitTemplateId:929448123;,21600,259200,Mon May 22 03:52:59 PDT 2023,,,3URFVVM16AX9ORR3VQ1HVRQZJJDUZM,worker_249,Submitted,Fri May 19 08:18:52 PDT 2023,Fri May 19 08:19:31 PDT 2023,Mon May 22 08:19:31 PDT 2023,,,,39,100% (194/194),100% (191/191),100% (190/190),104_1,0,CAR_89b60de1e4179304c422dce672387e3ba88620aa,3,"Because of its emphasis in using human intelligence in the information retrieval process, HCIR requires different evaluation models – one that combines evaluation of the IR and HCI components of the system. A key area of research in HCIR involves evaluation of these systems. Early work on interactive information retrieval, such as Juergen Koenemann and Nicholas J. Belkin's 1996 study of different levels of interaction for automatic query reformulation, leverage the standard IR measures of precision and recall but apply them to the results of multiple iterations of user interaction, rather than to a single query response. Other HCIR research, such as Pia Borlund's IIR evaluation model, applies a methodology more reminiscent of HCI, focusing on the characteristics of users, the details of experimental design, etc.",,Who are some of the well-known Information Retrieval researchers?,"[{""answer_confidence"":{""high"":False,""low"":False,""medium"":False,""very_high"":True,""very_low"":False},""passage-snippets-annotation-qualification-task"":{""entities"":[{""endOffset"":350,""label"":""relevant-text-span"",""startOffset"":332},{""endOffset"":376,""label"":""relevant-text-span"",""startOffset"":354},{""endOffset"":671,""label"":""relevant-text-span"",""startOffset"":657}]}}]",x,,"[' Juergen Koenemann', "" Nicholas J. Belkin's "", "" Pia Borlund's""]"
37VE3DA4ZYW5ENK9ZB9D1AYFK91BH6,3I3MQQJ31WXKIN6QF6OQSRUDQS81J6,Identify relevant text spans in text passage,Identify all the text spans that contain key pieces of the answer to a given question,"text, text spans, relevance",$0.30,Fri May 19 03:52:59 PDT 2023,3,BatchId:5076839;OriginalHitTemplateId:929448123;,21600,259200,Mon May 22 03:52:59 PDT 2023,,,3P1L2B7AD64U6XQ2K3YZO6NCMP6LO8,worker_119,Submitted,Fri May 19 11:14:35 PDT 2023,Fri May 19 12:27:33 PDT 2023,Mon May 22 12:27:33 PDT 2023,,,,4378,100% (270/271),100% (179/179),100% (178/178),104_1,0,CAR_e75e52cf36d3124d875c33fd6c999072e2fce6dc,3,"The research in (iv) had a deep impact on the understanding and initial development of a formalism to obtain semantic information when dealing with concepts, their combinations and variable contexts in a corpus of unstructured documents. This conundrum of natural language processing (NLP) and information retrieval (IR) on the web – and data bases in general – can be addressed using the mathematical formalism of quantum theory. As basic steps, (a) the seminal book 'The Geometry of Information Retrieval' by K. Van Rijsbergen introduced a quantum structure approach to IR, (b) Widdows and Peters utilised a quantum logical negation for a concrete search system, and Aerts and Czachor identified quantum structure in semantic space theories, such as latent semantic analysis. Since then, the employment of techniques and procedures induced from the mathematical formalisms of quantum theory – Hilbert space, quantum logic and probability, non-commutative algebras, etc. – in fields such as IR and NLP, has produced significant results.",,Who are some of the well-known Information Retrieval researchers?,"[{""answer_confidence"":{""high"":True,""low"":False,""medium"":False,""very_high"":False,""very_low"":False},""passage-snippets-annotation-qualification-task"":{""entities"":[{""endOffset"":528,""label"":""relevant-text-span"",""startOffset"":511},{""endOffset"":587,""label"":""relevant-text-span"",""startOffset"":580},{""endOffset"":598,""label"":""relevant-text-span"",""startOffset"":592},{""endOffset"":674,""label"":""relevant-text-span"",""startOffset"":669},{""endOffset"":686,""label"":""relevant-text-span"",""startOffset"":679}]}}]",x,,"['K. Van Rijsbergen', 'Widdows', 'Peters', 'Aerts', 'Czachor']"
37VE3DA4ZYW5ENK9ZB9D1AYFK91BH6,3I3MQQJ31WXKIN6QF6OQSRUDQS81J6,Identify relevant text spans in text passage,Identify all the text spans that contain key pieces of the answer to a given question,"text, text spans, relevance",$0.30,Fri May 19 03:52:59 PDT 2023,3,BatchId:5076839;OriginalHitTemplateId:929448123;,21600,259200,Mon May 22 03:52:59 PDT 2023,,,34S6N1K2Z0YKEG4FGQ394XQBA40LHU,worker_232,Submitted,Fri May 19 18:44:52 PDT 2023,Fri May 19 18:46:52 PDT 2023,Mon May 22 18:46:52 PDT 2023,,,,120,100% (208/208),100% (179/179),100% (178/178),104_1,0,CAR_e75e52cf36d3124d875c33fd6c999072e2fce6dc,3,"The research in (iv) had a deep impact on the understanding and initial development of a formalism to obtain semantic information when dealing with concepts, their combinations and variable contexts in a corpus of unstructured documents. This conundrum of natural language processing (NLP) and information retrieval (IR) on the web – and data bases in general – can be addressed using the mathematical formalism of quantum theory. As basic steps, (a) the seminal book 'The Geometry of Information Retrieval' by K. Van Rijsbergen introduced a quantum structure approach to IR, (b) Widdows and Peters utilised a quantum logical negation for a concrete search system, and Aerts and Czachor identified quantum structure in semantic space theories, such as latent semantic analysis. Since then, the employment of techniques and procedures induced from the mathematical formalisms of quantum theory – Hilbert space, quantum logic and probability, non-commutative algebras, etc. – in fields such as IR and NLP, has produced significant results.",,Who are some of the well-known Information Retrieval researchers?,"[{""answer_confidence"":{""high"":False,""low"":False,""medium"":True,""very_high"":False,""very_low"":False},""passage-snippets-annotation-qualification-task"":{""entities"":[{""endOffset"":574,""label"":""relevant-text-span"",""startOffset"":451},{""endOffset"":663,""label"":""relevant-text-span"",""startOffset"":580},{""endOffset"":776,""label"":""relevant-text-span"",""startOffset"":669}]}}]",x,,"[""the seminal book 'The Geometry of Information Retrieval' by K. Van Rijsbergen introduced a quantum structure approach to IR"", 'Widdows and Peters utilised a quantum logical negation for a concrete search system', 'Aerts and Czachor identified quantum structure in semantic space theories, such as latent semantic analysis']"
37VE3DA4ZYW5ENK9ZB9D1AYFK91BH6,3I3MQQJ31WXKIN6QF6OQSRUDQS81J6,Identify relevant text spans in text passage,Identify all the text spans that contain key pieces of the answer to a given question,"text, text spans, relevance",$0.30,Fri May 19 03:52:59 PDT 2023,3,BatchId:5076839;OriginalHitTemplateId:929448123;,21600,259200,Mon May 22 03:52:59 PDT 2023,,,3E4GGUZ1TD65FKJFC5O12T7UWJJ2KS,worker_249,Submitted,Fri May 19 08:28:53 PDT 2023,Fri May 19 08:29:47 PDT 2023,Mon May 22 08:29:47 PDT 2023,,,,54,100% (194/194),100% (191/191),100% (190/190),104_1,0,CAR_e75e52cf36d3124d875c33fd6c999072e2fce6dc,3,"The research in (iv) had a deep impact on the understanding and initial development of a formalism to obtain semantic information when dealing with concepts, their combinations and variable contexts in a corpus of unstructured documents. This conundrum of natural language processing (NLP) and information retrieval (IR) on the web – and data bases in general – can be addressed using the mathematical formalism of quantum theory. As basic steps, (a) the seminal book 'The Geometry of Information Retrieval' by K. Van Rijsbergen introduced a quantum structure approach to IR, (b) Widdows and Peters utilised a quantum logical negation for a concrete search system, and Aerts and Czachor identified quantum structure in semantic space theories, such as latent semantic analysis. Since then, the employment of techniques and procedures induced from the mathematical formalisms of quantum theory – Hilbert space, quantum logic and probability, non-commutative algebras, etc. – in fields such as IR and NLP, has produced significant results.",,Who are some of the well-known Information Retrieval researchers?,"[{""answer_confidence"":{""high"":False,""low"":False,""medium"":False,""very_high"":True,""very_low"":False},""passage-snippets-annotation-qualification-task"":{""entities"":[{""endOffset"":528,""label"":""relevant-text-span"",""startOffset"":510},{""endOffset"":588,""label"":""relevant-text-span"",""startOffset"":580},{""endOffset"":598,""label"":""relevant-text-span"",""startOffset"":592},{""endOffset"":674,""label"":""relevant-text-span"",""startOffset"":669},{""endOffset"":686,""label"":""relevant-text-span"",""startOffset"":679}]}}]",x,,"[' K. Van Rijsbergen', 'Widdows ', 'Peters', 'Aerts', 'Czachor']"
3JU8CV4BSPRHAY76MTM4G305DZUPOT,3I3MQQJ31WXKIN6QF6OQSRUDQS81J6,Identify relevant text spans in text passage,Identify all the text spans that contain key pieces of the answer to a given question,"text, text spans, relevance",$0.30,Fri May 19 03:52:59 PDT 2023,3,BatchId:5076839;OriginalHitTemplateId:929448123;,21600,259200,Mon May 22 03:52:59 PDT 2023,,,3J88R45B2LD7RRJSDME2O0NP322PX3,worker_119,Submitted,Fri May 19 09:32:39 PDT 2023,Fri May 19 09:40:12 PDT 2023,Mon May 22 09:40:12 PDT 2023,,,,453,100% (270/271),100% (179/179),100% (178/178),104_1,0,CAR_fdaff8f22d6b0811a7d07c76405f1705f413163e,3,"Center for Intelligent Information Retrieval (CIIR) is a research center at the Department of Computer Science, University of Massachusetts Amherst. It is a leading research center in the area of Information Retrieval and Information Extraction. CIIR is led by Distinguished Professor W. Bruce Croft and Professor James Allan.",,Who are some of the well-known Information Retrieval researchers?,"[{""answer_confidence"":{""high"":True,""low"":False,""medium"":False,""very_high"":False,""very_low"":False},""passage-snippets-annotation-qualification-task"":{""entities"":[{""endOffset"":299,""label"":""relevant-text-span"",""startOffset"":275},{""endOffset"":325,""label"":""relevant-text-span"",""startOffset"":304}]}}]",x,,"['Professor W. Bruce Croft', 'Professor James Allan']"
3JU8CV4BSPRHAY76MTM4G305DZUPOT,3I3MQQJ31WXKIN6QF6OQSRUDQS81J6,Identify relevant text spans in text passage,Identify all the text spans that contain key pieces of the answer to a given question,"text, text spans, relevance",$0.30,Fri May 19 03:52:59 PDT 2023,3,BatchId:5076839;OriginalHitTemplateId:929448123;,21600,259200,Mon May 22 03:52:59 PDT 2023,,,3EFVCAY5L8OLQFFMA1XQB5KK3W4J8A,worker_232,Submitted,Fri May 19 18:33:06 PDT 2023,Fri May 19 18:43:38 PDT 2023,Mon May 22 18:43:38 PDT 2023,,,,632,100% (208/208),100% (179/179),100% (178/178),104_1,0,CAR_fdaff8f22d6b0811a7d07c76405f1705f413163e,3,"Center for Intelligent Information Retrieval (CIIR) is a research center at the Department of Computer Science, University of Massachusetts Amherst. It is a leading research center in the area of Information Retrieval and Information Extraction. CIIR is led by Distinguished Professor W. Bruce Croft and Professor James Allan.",,Who are some of the well-known Information Retrieval researchers?,"[{""answer_confidence"":{""high"":False,""low"":False,""medium"":True,""very_high"":False,""very_low"":False},""passage-snippets-annotation-qualification-task"":{""entities"":[{""endOffset"":326,""label"":""relevant-text-span"",""startOffset"":246}]}}]",x,,['CIIR is led by Distinguished Professor W. Bruce Croft and Professor James Allan.']
3JU8CV4BSPRHAY76MTM4G305DZUPOT,3I3MQQJ31WXKIN6QF6OQSRUDQS81J6,Identify relevant text spans in text passage,Identify all the text spans that contain key pieces of the answer to a given question,"text, text spans, relevance",$0.30,Fri May 19 03:52:59 PDT 2023,3,BatchId:5076839;OriginalHitTemplateId:929448123;,21600,259200,Mon May 22 03:52:59 PDT 2023,,,3JRJSWSMQM0RE2NOKJR3N8ZHIZ73E2,worker_249,Approved,Fri May 19 04:50:52 PDT 2023,Fri May 19 04:52:19 PDT 2023,Mon May 22 04:52:19 PDT 2023,Mon May 22 04:53:19 PDT 2023,,,87,100% (194/194),100% (191/191),100% (190/190),104_1,0,CAR_fdaff8f22d6b0811a7d07c76405f1705f413163e,3,"Center for Intelligent Information Retrieval (CIIR) is a research center at the Department of Computer Science, University of Massachusetts Amherst. It is a leading research center in the area of Information Retrieval and Information Extraction. CIIR is led by Distinguished Professor W. Bruce Croft and Professor James Allan.",,Who are some of the well-known Information Retrieval researchers?,"[{""answer_confidence"":{""high"":False,""low"":False,""medium"":False,""very_high"":True,""very_low"":False},""passage-snippets-annotation-qualification-task"":{""entities"":[{""endOffset"":299,""label"":""relevant-text-span"",""startOffset"":261},{""endOffset"":325,""label"":""relevant-text-span"",""startOffset"":304}]}}]",x,,"['Distinguished Professor W. Bruce Croft', 'Professor James Allan']"
3R5OYNIC3GOXNQCEIA83AD6SC7BPTF,3I3MQQJ31WXKIN6QF6OQSRUDQS81J6,Identify relevant text spans in text passage,Identify all the text spans that contain key pieces of the answer to a given question,"text, text spans, relevance",$0.30,Fri May 19 03:53:01 PDT 2023,3,BatchId:5076839;OriginalHitTemplateId:929448123;,21600,259200,Mon May 22 03:53:01 PDT 2023,,,30BXRYBRP9C0PAGEK4ABD87R89MHWX,worker_119,Submitted,Fri May 19 09:31:59 PDT 2023,Fri May 19 09:39:52 PDT 2023,Mon May 22 09:39:52 PDT 2023,,,,473,100% (270/271),100% (179/179),100% (178/178),104_10,0,CAR_c56480057a9a2b42a30ab6c7208ffd6cfc2b7074,1,"The TRECVID evaluation meetings are an on-going series of workshops focusing on a list of different information retrieval (IR) research areas in content-based retrieval and exploitation of digital video. TRECVID is co-sponsored by the National Institute of Standards and Technology (NIST) and other US government agencies. Various participating research groups make significant contributions. The goal of the workshops is to encourage research in content-based video retrieval and analysis by providing large test collections, realistic system tasks, uniform scoring procedures, and a forum for organizations interested in comparing their results.",,What are the important components of an information retrieval test collection?,"[{""answer_confidence"":{""high"":False,""low"":False,""medium"":False,""very_high"":False,""very_low"":True},""passage-snippets-annotation-qualification-task"":{""entities"":[]}}]",,Chosen text spans are too long (longer than half of the passage). The requirements say that text spans should be concise and as short as possible.,[]
3R5OYNIC3GOXNQCEIA83AD6SC7BPTF,3I3MQQJ31WXKIN6QF6OQSRUDQS81J6,Identify relevant text spans in text passage,Identify all the text spans that contain key pieces of the answer to a given question,"text, text spans, relevance",$0.30,Fri May 19 03:53:01 PDT 2023,3,BatchId:5076839;OriginalHitTemplateId:929448123;,21600,259200,Mon May 22 03:53:01 PDT 2023,,,3R2PKQ87N1N4GTXKMKM94M6FAM5IMM,worker_232,Submitted,Fri May 19 18:45:13 PDT 2023,Fri May 19 18:53:38 PDT 2023,Mon May 22 18:53:38 PDT 2023,,,,505,100% (208/208),100% (179/179),100% (178/178),104_10,0,CAR_c56480057a9a2b42a30ab6c7208ffd6cfc2b7074,1,"The TRECVID evaluation meetings are an on-going series of workshops focusing on a list of different information retrieval (IR) research areas in content-based retrieval and exploitation of digital video. TRECVID is co-sponsored by the National Institute of Standards and Technology (NIST) and other US government agencies. Various participating research groups make significant contributions. The goal of the workshops is to encourage research in content-based video retrieval and analysis by providing large test collections, realistic system tasks, uniform scoring procedures, and a forum for organizations interested in comparing their results.",,What are the important components of an information retrieval test collection?,"[{""answer_confidence"":{""high"":False,""low"":False,""medium"":True,""very_high"":False,""very_low"":False},""passage-snippets-annotation-qualification-task"":{""entities"":[]}}]",,Chosen text spans are too long (longer than half of the passage). The requirements say that text spans should be concise and as short as possible.,[]
3R5OYNIC3GOXNQCEIA83AD6SC7BPTF,3I3MQQJ31WXKIN6QF6OQSRUDQS81J6,Identify relevant text spans in text passage,Identify all the text spans that contain key pieces of the answer to a given question,"text, text spans, relevance",$0.30,Fri May 19 03:53:01 PDT 2023,3,BatchId:5076839;OriginalHitTemplateId:929448123;,21600,259200,Mon May 22 03:53:01 PDT 2023,,,3XM0HYN6NPEYLRSBIHOA0B2INWWEPO,worker_249,Submitted,Fri May 19 06:20:57 PDT 2023,Fri May 19 06:22:15 PDT 2023,Mon May 22 06:22:15 PDT 2023,,,,78,100% (194/194),100% (191/191),100% (190/190),104_10,0,CAR_c56480057a9a2b42a30ab6c7208ffd6cfc2b7074,1,"The TRECVID evaluation meetings are an on-going series of workshops focusing on a list of different information retrieval (IR) research areas in content-based retrieval and exploitation of digital video. TRECVID is co-sponsored by the National Institute of Standards and Technology (NIST) and other US government agencies. Various participating research groups make significant contributions. The goal of the workshops is to encourage research in content-based video retrieval and analysis by providing large test collections, realistic system tasks, uniform scoring procedures, and a forum for organizations interested in comparing their results.",,What are the important components of an information retrieval test collection?,"[{""answer_confidence"":{""high"":True,""low"":False,""medium"":False,""very_high"":False,""very_low"":False},""passage-snippets-annotation-qualification-task"":{""entities"":[]}}]",,Chosen text spans are too long (longer than half of the passage). The requirements say that text spans should be concise and as short as possible.,[]
36GJS3V79Z5XWANA3X4CEY5RDEXGJX,3I3MQQJ31WXKIN6QF6OQSRUDQS81J6,Identify relevant text spans in text passage,Identify all the text spans that contain key pieces of the answer to a given question,"text, text spans, relevance",$0.30,Fri May 19 03:53:01 PDT 2023,3,BatchId:5076839;OriginalHitTemplateId:929448123;,21600,259200,Mon May 22 03:53:01 PDT 2023,,,3TXMY6UCAJ34O59CM3X4ASSP7G0CQF,worker_119,Submitted,Fri May 19 09:31:54 PDT 2023,Fri May 19 09:35:26 PDT 2023,Mon May 22 09:35:26 PDT 2023,,,,212,100% (270/271),100% (179/179),100% (178/178),104_10,0,CAR_caadc5b1d3560cab0db70c6ffa486540cb97fd59,1,Precision and recall have been two of the traditional performance measures for evaluating information retrieval systems.  Precision is the fraction of the retrieved result documents that are relevant to the user's information need.  Recall is defined as the fraction of relevant documents in the entire collection that are returned as result documents.,,What are the important components of an information retrieval test collection?,"[{""answer_confidence"":{""high"":True,""low"":False,""medium"":False,""very_high"":False,""very_low"":False},""passage-snippets-annotation-qualification-task"":{""entities"":[{""endOffset"":9,""label"":""relevant-text-span"",""startOffset"":0},{""endOffset"":20,""label"":""relevant-text-span"",""startOffset"":14},{""endOffset"":131,""label"":""relevant-text-span"",""startOffset"":122},{""endOffset"":239,""label"":""relevant-text-span"",""startOffset"":233}]}}]",x,,"['Precision', 'recall', 'Precision', 'Recall']"
36GJS3V79Z5XWANA3X4CEY5RDEXGJX,3I3MQQJ31WXKIN6QF6OQSRUDQS81J6,Identify relevant text spans in text passage,Identify all the text spans that contain key pieces of the answer to a given question,"text, text spans, relevance",$0.30,Fri May 19 03:53:01 PDT 2023,3,BatchId:5076839;OriginalHitTemplateId:929448123;,21600,259200,Mon May 22 03:53:01 PDT 2023,,,3WOKGM4L76VH911GVACGQMVOJS30OO,worker_232,Submitted,Fri May 19 18:33:01 PDT 2023,Fri May 19 18:41:08 PDT 2023,Mon May 22 18:41:08 PDT 2023,,,,487,100% (208/208),100% (179/179),100% (178/178),104_10,0,CAR_caadc5b1d3560cab0db70c6ffa486540cb97fd59,1,Precision and recall have been two of the traditional performance measures for evaluating information retrieval systems.  Precision is the fraction of the retrieved result documents that are relevant to the user's information need.  Recall is defined as the fraction of relevant documents in the entire collection that are returned as result documents.,,What are the important components of an information retrieval test collection?,"[{""answer_confidence"":{""high"":True,""low"":False,""medium"":False,""very_high"":False,""very_low"":False},""passage-snippets-annotation-qualification-task"":{""entities"":[{""endOffset"":120,""label"":""relevant-text-span"",""startOffset"":0}]}}]",x,,['Precision and recall have been two of the traditional performance measures for evaluating information retrieval systems.']
36GJS3V79Z5XWANA3X4CEY5RDEXGJX,3I3MQQJ31WXKIN6QF6OQSRUDQS81J6,Identify relevant text spans in text passage,Identify all the text spans that contain key pieces of the answer to a given question,"text, text spans, relevance",$0.30,Fri May 19 03:53:01 PDT 2023,3,BatchId:5076839;OriginalHitTemplateId:929448123;,21600,259200,Mon May 22 03:53:01 PDT 2023,,,3IRIK4HM3FZBENQS7UJ64NBY7FAC64,worker_249,Submitted,Fri May 19 08:24:21 PDT 2023,Fri May 19 08:26:50 PDT 2023,Mon May 22 08:26:50 PDT 2023,,,,149,100% (194/194),100% (191/191),100% (190/190),104_10,0,CAR_caadc5b1d3560cab0db70c6ffa486540cb97fd59,1,Precision and recall have been two of the traditional performance measures for evaluating information retrieval systems.  Precision is the fraction of the retrieved result documents that are relevant to the user's information need.  Recall is defined as the fraction of relevant documents in the entire collection that are returned as result documents.,,What are the important components of an information retrieval test collection?,"[{""answer_confidence"":{""high"":False,""low"":False,""medium"":False,""very_high"":True,""very_low"":False},""passage-snippets-annotation-qualification-task"":{""entities"":[{""endOffset"":230,""label"":""relevant-text-span"",""startOffset"":155},{""endOffset"":351,""label"":""relevant-text-span"",""startOffset"":270}]}}]",,The agreement rate not met. None of the text span chosen by you has been selected by other workers.,"[""retrieved result documents that are relevant to the user's information need"", 'relevant documents in the entire collection that are returned as result documents']"
3WKGUBL7T31NFKIQBHEOLOLGRYTL4H,3I3MQQJ31WXKIN6QF6OQSRUDQS81J6,Identify relevant text spans in text passage,Identify all the text spans that contain key pieces of the answer to a given question,"text, text spans, relevance",$0.30,Fri May 19 03:53:01 PDT 2023,3,BatchId:5076839;OriginalHitTemplateId:929448123;,21600,259200,Mon May 22 03:53:01 PDT 2023,,,33CUSNVVNSRW6YJ3JX0ZMFB6NKL88O,worker_119,Submitted,Fri May 19 14:29:47 PDT 2023,Fri May 19 14:32:39 PDT 2023,Mon May 22 14:32:39 PDT 2023,,,,172,100% (270/271),100% (179/179),100% (178/178),104_10,0,CAR_cef18912f5c9b7c722a3cf83451a2222f7520afa,2,"The evaluation of an information retrieval system is the process of assessing how well a system meets the information needs of its users. Traditional evaluation metrics, designed for Boolean retrieval or top-k retrieval, include precision and recall.  Many more measures for evaluating the performance of information retrieval systems have also been proposed. In general, measurement considers a collection of documents to be searched and a search query. All common measures described here assume a ground truth notion of relevancy: every document is known to be either relevant or non-relevant to a particular query. In practice, queries may be ill-posed and there may be different shades of relevancy.",,What are the important components of an information retrieval test collection?,"[{""answer_confidence"":{""high"":False,""low"":False,""medium"":False,""very_high"":False,""very_low"":True},""passage-snippets-annotation-qualification-task"":{""entities"":[{""endOffset"":238,""label"":""relevant-text-span"",""startOffset"":229},{""endOffset"":249,""label"":""relevant-text-span"",""startOffset"":243},{""endOffset"":454,""label"":""relevant-text-span"",""startOffset"":359}]}}]",x,,"['precision', 'recall', ' In general, measurement considers a collection of documents to be searched and a search query.']"
3WKGUBL7T31NFKIQBHEOLOLGRYTL4H,3I3MQQJ31WXKIN6QF6OQSRUDQS81J6,Identify relevant text spans in text passage,Identify all the text spans that contain key pieces of the answer to a given question,"text, text spans, relevance",$0.30,Fri May 19 03:53:01 PDT 2023,3,BatchId:5076839;OriginalHitTemplateId:929448123;,21600,259200,Mon May 22 03:53:01 PDT 2023,,,31QTRG6Q2YSWRW92II047EUY5ADPYU,worker_232,Submitted,Fri May 19 18:33:04 PDT 2023,Fri May 19 18:42:04 PDT 2023,Mon May 22 18:42:04 PDT 2023,,,,540,100% (208/208),100% (179/179),100% (178/178),104_10,0,CAR_cef18912f5c9b7c722a3cf83451a2222f7520afa,2,"The evaluation of an information retrieval system is the process of assessing how well a system meets the information needs of its users. Traditional evaluation metrics, designed for Boolean retrieval or top-k retrieval, include precision and recall.  Many more measures for evaluating the performance of information retrieval systems have also been proposed. In general, measurement considers a collection of documents to be searched and a search query. All common measures described here assume a ground truth notion of relevancy: every document is known to be either relevant or non-relevant to a particular query. In practice, queries may be ill-posed and there may be different shades of relevancy.",,What are the important components of an information retrieval test collection?,"[{""answer_confidence"":{""high"":False,""low"":False,""medium"":True,""very_high"":False,""very_low"":False},""passage-snippets-annotation-qualification-task"":{""entities"":[{""endOffset"":250,""label"":""relevant-text-span"",""startOffset"":138}]}}]",x,,"['Traditional evaluation metrics, designed for Boolean retrieval or top-k retrieval, include precision and recall.']"
3WKGUBL7T31NFKIQBHEOLOLGRYTL4H,3I3MQQJ31WXKIN6QF6OQSRUDQS81J6,Identify relevant text spans in text passage,Identify all the text spans that contain key pieces of the answer to a given question,"text, text spans, relevance",$0.30,Fri May 19 03:53:01 PDT 2023,3,BatchId:5076839;OriginalHitTemplateId:929448123;,21600,259200,Mon May 22 03:53:01 PDT 2023,,,3B837J3LDTBK7ND8W4C4UN6CP4VSRV,worker_249,Approved,Fri May 19 04:48:14 PDT 2023,Fri May 19 04:50:49 PDT 2023,Mon May 22 04:50:49 PDT 2023,Mon May 22 04:51:19 PDT 2023,,,155,100% (194/194),100% (191/191),100% (190/190),104_10,0,CAR_cef18912f5c9b7c722a3cf83451a2222f7520afa,2,"The evaluation of an information retrieval system is the process of assessing how well a system meets the information needs of its users. Traditional evaluation metrics, designed for Boolean retrieval or top-k retrieval, include precision and recall.  Many more measures for evaluating the performance of information retrieval systems have also been proposed. In general, measurement considers a collection of documents to be searched and a search query. All common measures described here assume a ground truth notion of relevancy: every document is known to be either relevant or non-relevant to a particular query. In practice, queries may be ill-posed and there may be different shades of relevancy.",,What are the important components of an information retrieval test collection?,"[{""answer_confidence"":{""high"":False,""low"":False,""medium"":False,""very_high"":True,""very_low"":False},""passage-snippets-annotation-qualification-task"":{""entities"":[{""endOffset"":531,""label"":""relevant-text-span"",""startOffset"":496},{""endOffset"":616,""label"":""relevant-text-span"",""startOffset"":533}]}}]",,The agreement rate not met. None of the text span chosen by you has been selected by other workers.,"[' a ground truth notion of relevancy', 'every document is known to be either relevant or non-relevant to a particular query']"
3N7PQ0KLJ94ORIPUUHM1YO3Q41WE32,3I3MQQJ31WXKIN6QF6OQSRUDQS81J6,Identify relevant text spans in text passage,Identify all the text spans that contain key pieces of the answer to a given question,"text, text spans, relevance",$0.30,Fri May 19 03:53:01 PDT 2023,3,BatchId:5076839;OriginalHitTemplateId:929448123;,21600,259200,Mon May 22 03:53:01 PDT 2023,,,37FMASSAYH68X2TNVVW0Q6VLYMCIB6,worker_119,Submitted,Fri May 19 14:42:01 PDT 2023,Fri May 19 14:45:04 PDT 2023,Mon May 22 14:45:04 PDT 2023,,,,183,100% (270/271),100% (179/179),100% (178/178),104_10,0,CAR_f041a09c412533d214cc4756f05f05ed28f356c9,1,"In order to evaluate how well an information retrieval system retrieved topically relevant results, the relevance of retrieved results must be quantified. In Cranfield-style evaluations, this typically involves assigning a relevance level to each retrieved result, a process known as relevance assessment. Relevance levels can be binary (indicating a result is relevant or that it is not relevant), or graded (indicating results have a varying degree of match between the topic of the result and the information need).   Once relevance levels have been assigned to the retrieved results, information retrieval performance measures can be used to assess the quality of a retrieval system's output.",,What are the important components of an information retrieval test collection?,"[{""answer_confidence"":{""high"":False,""low"":True,""medium"":False,""very_high"":False,""very_low"":False},""passage-snippets-annotation-qualification-task"":{""entities"":[{""endOffset"":305,""label"":""relevant-text-span"",""startOffset"":155},{""endOffset"":696,""label"":""relevant-text-span"",""startOffset"":588}]}}]",x,,"['In Cranfield-style evaluations, this typically involves assigning a relevance level to each retrieved result, a process known as relevance assessment.', ""information retrieval performance measures can be used to assess the quality of a retrieval system's output.""]"
3N7PQ0KLJ94ORIPUUHM1YO3Q41WE32,3I3MQQJ31WXKIN6QF6OQSRUDQS81J6,Identify relevant text spans in text passage,Identify all the text spans that contain key pieces of the answer to a given question,"text, text spans, relevance",$0.30,Fri May 19 03:53:01 PDT 2023,3,BatchId:5076839;OriginalHitTemplateId:929448123;,21600,259200,Mon May 22 03:53:01 PDT 2023,,,3OUYGIZWRCDZU1DHAAPVFM0ETX00P6,worker_232,Submitted,Fri May 19 18:45:16 PDT 2023,Fri May 19 18:55:33 PDT 2023,Mon May 22 18:55:33 PDT 2023,,,,617,100% (208/208),100% (179/179),100% (178/178),104_10,0,CAR_f041a09c412533d214cc4756f05f05ed28f356c9,1,"In order to evaluate how well an information retrieval system retrieved topically relevant results, the relevance of retrieved results must be quantified. In Cranfield-style evaluations, this typically involves assigning a relevance level to each retrieved result, a process known as relevance assessment. Relevance levels can be binary (indicating a result is relevant or that it is not relevant), or graded (indicating results have a varying degree of match between the topic of the result and the information need).   Once relevance levels have been assigned to the retrieved results, information retrieval performance measures can be used to assess the quality of a retrieval system's output.",,What are the important components of an information retrieval test collection?,"[{""answer_confidence"":{""high"":False,""low"":False,""medium"":True,""very_high"":False,""very_low"":False},""passage-snippets-annotation-qualification-task"":{""entities"":[]}}]",,Chosen text spans are too long (longer than half of the passage). The requirements say that text spans should be concise and as short as possible.,[]
3N7PQ0KLJ94ORIPUUHM1YO3Q41WE32,3I3MQQJ31WXKIN6QF6OQSRUDQS81J6,Identify relevant text spans in text passage,Identify all the text spans that contain key pieces of the answer to a given question,"text, text spans, relevance",$0.30,Fri May 19 03:53:01 PDT 2023,3,BatchId:5076839;OriginalHitTemplateId:929448123;,21600,259200,Mon May 22 03:53:01 PDT 2023,,,34MAJL3QP929M0QVVDFQEMNQ5BP43B,worker_249,Submitted,Fri May 19 07:58:22 PDT 2023,Fri May 19 08:06:50 PDT 2023,Mon May 22 08:06:50 PDT 2023,,,,508,100% (194/194),100% (191/191),100% (190/190),104_10,0,CAR_f041a09c412533d214cc4756f05f05ed28f356c9,1,"In order to evaluate how well an information retrieval system retrieved topically relevant results, the relevance of retrieved results must be quantified. In Cranfield-style evaluations, this typically involves assigning a relevance level to each retrieved result, a process known as relevance assessment. Relevance levels can be binary (indicating a result is relevant or that it is not relevant), or graded (indicating results have a varying degree of match between the topic of the result and the information need).   Once relevance levels have been assigned to the retrieved results, information retrieval performance measures can be used to assess the quality of a retrieval system's output.",,What are the important components of an information retrieval test collection?,"[{""answer_confidence"":{""high"":False,""low"":False,""medium"":True,""very_high"":False,""very_low"":False},""passage-snippets-annotation-qualification-task"":{""entities"":[{""endOffset"":134,""label"":""relevant-text-span"",""startOffset"":104},{""endOffset"":263,""label"":""relevant-text-span"",""startOffset"":211},{""endOffset"":304,""label"":""relevant-text-span"",""startOffset"":284},{""endOffset"":322,""label"":""relevant-text-span"",""startOffset"":306},{""endOffset"":586,""label"":""relevant-text-span"",""startOffset"":525}]}}]",x,,"['relevance of retrieved results', 'assigning a relevance level to each retrieved result', 'relevance assessment', 'Relevance levels', ' relevance levels have been assigned to the retrieved results']"
3MVY4USGCA24R4CVGFDTRB9UOA2SIH,3I3MQQJ31WXKIN6QF6OQSRUDQS81J6,Identify relevant text spans in text passage,Identify all the text spans that contain key pieces of the answer to a given question,"text, text spans, relevance",$0.30,Fri May 19 03:53:01 PDT 2023,3,BatchId:5076839;OriginalHitTemplateId:929448123;,21600,259200,Mon May 22 03:53:01 PDT 2023,,,37XITHEIS1O406OCZIM4FI2U322CRG,worker_119,Submitted,Fri May 19 09:31:55 PDT 2023,Fri May 19 09:36:50 PDT 2023,Mon May 22 09:36:50 PDT 2023,,,,295,100% (270/271),100% (179/179),100% (178/178),104_10,0,MARCO_5245403,1,"An information retrieval process begins when a user enters a query into the system. Queries are formal statements of information needs, for example search strings in web search engines.In information retrieval a query does not uniquely identify a single object in the collection.Instead, several objects may match the query, perhaps with different degrees of relevancy. An object is an entity that is represented by information in a database.earches can be based on metadata or on full-text (or other content-based) indexing. Automated information retrieval systems are used to reduce what has been called  information overload . Many universities and public libraries use IR systems to provide access to books, journals and other documents.",,What are the important components of an information retrieval test collection?,"[{""answer_confidence"":{""high"":False,""low"":False,""medium"":False,""very_high"":False,""very_low"":True},""passage-snippets-annotation-qualification-task"":{""entities"":[]}}]",,Chosen text spans are too long (longer than half of the passage). The requirements say that text spans should be concise and as short as possible.,[]
3MVY4USGCA24R4CVGFDTRB9UOA2SIH,3I3MQQJ31WXKIN6QF6OQSRUDQS81J6,Identify relevant text spans in text passage,Identify all the text spans that contain key pieces of the answer to a given question,"text, text spans, relevance",$0.30,Fri May 19 03:53:01 PDT 2023,3,BatchId:5076839;OriginalHitTemplateId:929448123;,21600,259200,Mon May 22 03:53:01 PDT 2023,,,3QY5DC2MXWZ3JA03UE7ASZAH7GGFUK,worker_232,Submitted,Fri May 19 18:45:14 PDT 2023,Fri May 19 18:54:25 PDT 2023,Mon May 22 18:54:25 PDT 2023,,,,551,100% (208/208),100% (179/179),100% (178/178),104_10,0,MARCO_5245403,1,"An information retrieval process begins when a user enters a query into the system. Queries are formal statements of information needs, for example search strings in web search engines.In information retrieval a query does not uniquely identify a single object in the collection.Instead, several objects may match the query, perhaps with different degrees of relevancy. An object is an entity that is represented by information in a database.earches can be based on metadata or on full-text (or other content-based) indexing. Automated information retrieval systems are used to reduce what has been called  information overload . Many universities and public libraries use IR systems to provide access to books, journals and other documents.",,What are the important components of an information retrieval test collection?,"[{""answer_confidence"":{""high"":False,""low"":False,""medium"":True,""very_high"":False,""very_low"":False},""passage-snippets-annotation-qualification-task"":{""entities"":[{""endOffset"":84,""label"":""relevant-text-span"",""startOffset"":0},{""endOffset"":441,""label"":""relevant-text-span"",""startOffset"":370},{""endOffset"":526,""label"":""relevant-text-span"",""startOffset"":442}]}}]",,The agreement rate not met. None of the text span chosen by you has been selected by other workers.,"['An information retrieval process begins when a user enters a query into the system. ', 'An object is an entity that is represented by information in a database', 'earches can be based on metadata or on full-text (or other content-based) indexing. ']"
3MVY4USGCA24R4CVGFDTRB9UOA2SIH,3I3MQQJ31WXKIN6QF6OQSRUDQS81J6,Identify relevant text spans in text passage,Identify all the text spans that contain key pieces of the answer to a given question,"text, text spans, relevance",$0.30,Fri May 19 03:53:01 PDT 2023,3,BatchId:5076839;OriginalHitTemplateId:929448123;,21600,259200,Mon May 22 03:53:01 PDT 2023,,,3AUQQEL7UA8CZLADWMYULEPWFJMV03,worker_249,Submitted,Fri May 19 06:16:27 PDT 2023,Fri May 19 06:19:25 PDT 2023,Mon May 22 06:19:25 PDT 2023,,,,178,100% (194/194),100% (191/191),100% (190/190),104_10,0,MARCO_5245403,1,"An information retrieval process begins when a user enters a query into the system. Queries are formal statements of information needs, for example search strings in web search engines.In information retrieval a query does not uniquely identify a single object in the collection.Instead, several objects may match the query, perhaps with different degrees of relevancy. An object is an entity that is represented by information in a database.earches can be based on metadata or on full-text (or other content-based) indexing. Automated information retrieval systems are used to reduce what has been called  information overload . Many universities and public libraries use IR systems to provide access to books, journals and other documents.",,What are the important components of an information retrieval test collection?,"[{""answer_confidence"":{""high"":True,""low"":False,""medium"":False,""very_high"":False,""very_low"":False},""passage-snippets-annotation-qualification-task"":{""entities"":[]}}]",,Chosen text spans are too long (longer than half of the passage). The requirements say that text spans should be concise and as short as possible.,[]
33TGB4G0MTW2WZE541IK4QSVE6KXTB,3I3MQQJ31WXKIN6QF6OQSRUDQS81J6,Identify relevant text spans in text passage,Identify all the text spans that contain key pieces of the answer to a given question,"text, text spans, relevance",$0.30,Fri May 19 03:53:01 PDT 2023,3,BatchId:5076839;OriginalHitTemplateId:929448123;,21600,259200,Mon May 22 03:53:01 PDT 2023,,,3Z9WI9EOZ439UAMXVQR8PXDWL3NHKD,worker_119,Submitted,Fri May 19 11:14:33 PDT 2023,Fri May 19 11:29:41 PDT 2023,Mon May 22 11:29:41 PDT 2023,,,,908,100% (270/271),100% (179/179),100% (178/178),104_12,0,CAR_7b40a5261bd93814b2454a1b1fff919b8f56dad0,2,"The F-score is often used in the field of information retrieval for measuring search, document classification, and query classification performance. Earlier works focused primarily on the F score, but with the proliferation of large scale search engines, performance goals changed to place more emphasis on either precision or recall and so",0.0,What are important evaluation measures for web search engines?,"[{""answer_confidence"":{""high"":False,""low"":False,""medium"":True,""very_high"":False,""very_low"":False},""passage-snippets-annotation-qualification-task"":{""entities"":[{""endOffset"":84,""label"":""relevant-text-span"",""startOffset"":0},{""endOffset"":195,""label"":""relevant-text-span"",""startOffset"":184},{""endOffset"":323,""label"":""relevant-text-span"",""startOffset"":314},{""endOffset"":333,""label"":""relevant-text-span"",""startOffset"":327}]}}]",x,,"['The F-score is often used in the field of information retrieval for measuring search', 'the F score', 'precision', 'recall']"
33TGB4G0MTW2WZE541IK4QSVE6KXTB,3I3MQQJ31WXKIN6QF6OQSRUDQS81J6,Identify relevant text spans in text passage,Identify all the text spans that contain key pieces of the answer to a given question,"text, text spans, relevance",$0.30,Fri May 19 03:53:01 PDT 2023,3,BatchId:5076839;OriginalHitTemplateId:929448123;,21600,259200,Mon May 22 03:53:01 PDT 2023,,,3F0BG9B9MU27LQ52MW6N7S3D5G7Y7W,worker_232,Submitted,Fri May 19 18:33:05 PDT 2023,Fri May 19 18:42:37 PDT 2023,Mon May 22 18:42:37 PDT 2023,,,,572,100% (208/208),100% (179/179),100% (178/178),104_12,0,CAR_7b40a5261bd93814b2454a1b1fff919b8f56dad0,2,"The F-score is often used in the field of information retrieval for measuring search, document classification, and query classification performance. Earlier works focused primarily on the F score, but with the proliferation of large scale search engines, performance goals changed to place more emphasis on either precision or recall and so",0.0,What are important evaluation measures for web search engines?,"[{""answer_confidence"":{""high"":True,""low"":False,""medium"":False,""very_high"":False,""very_low"":False},""passage-snippets-annotation-qualification-task"":{""entities"":[{""endOffset"":148,""label"":""relevant-text-span"",""startOffset"":0}]}}]",x,,"['The F-score is often used in the field of information retrieval for measuring search, document classification, and query classification performance.']"
33TGB4G0MTW2WZE541IK4QSVE6KXTB,3I3MQQJ31WXKIN6QF6OQSRUDQS81J6,Identify relevant text spans in text passage,Identify all the text spans that contain key pieces of the answer to a given question,"text, text spans, relevance",$0.30,Fri May 19 03:53:01 PDT 2023,3,BatchId:5076839;OriginalHitTemplateId:929448123;,21600,259200,Mon May 22 03:53:01 PDT 2023,,,32UTUBMZ7LBDJ8DNU2VJ5VLF6NPVBV,worker_249,Submitted,Fri May 19 07:07:10 PDT 2023,Fri May 19 07:13:44 PDT 2023,Mon May 22 07:13:44 PDT 2023,,,,394,100% (194/194),100% (191/191),100% (190/190),104_12,0,CAR_7b40a5261bd93814b2454a1b1fff919b8f56dad0,2,"The F-score is often used in the field of information retrieval for measuring search, document classification, and query classification performance. Earlier works focused primarily on the F score, but with the proliferation of large scale search engines, performance goals changed to place more emphasis on either precision or recall and so",0.0,What are important evaluation measures for web search engines?,"[{""answer_confidence"":{""high"":False,""low"":False,""medium"":False,""very_high"":True,""very_low"":False},""passage-snippets-annotation-qualification-task"":{""entities"":[{""endOffset"":109,""label"":""relevant-text-span"",""startOffset"":86},{""endOffset"":135,""label"":""relevant-text-span"",""startOffset"":115},{""endOffset"":324,""label"":""relevant-text-span"",""startOffset"":313},{""endOffset"":333,""label"":""relevant-text-span"",""startOffset"":326}]}}]",x,,"['document classification', 'query classification', ' precision ', ' recall']"
3ZG552ORBQJG8BFXL7X6FIMKYIZV2E,3I3MQQJ31WXKIN6QF6OQSRUDQS81J6,Identify relevant text spans in text passage,Identify all the text spans that contain key pieces of the answer to a given question,"text, text spans, relevance",$0.30,Fri May 19 03:53:01 PDT 2023,3,BatchId:5076839;OriginalHitTemplateId:929448123;,21600,259200,Mon May 22 03:53:01 PDT 2023,,,3GNA64GUZJJJPK0X7TAWWMMFF065QU,worker_119,Submitted,Fri May 19 14:29:45 PDT 2023,Fri May 19 14:30:43 PDT 2023,Mon May 22 14:30:43 PDT 2023,,,,58,100% (270/271),100% (179/179),100% (178/178),104_12,0,CAR_caadc5b1d3560cab0db70c6ffa486540cb97fd59,3,Precision and recall have been two of the traditional performance measures for evaluating information retrieval systems.  Precision is the fraction of the retrieved result documents that are relevant to the user's information need.  Recall is defined as the fraction of relevant documents in the entire collection that are returned as result documents.,,What are important evaluation measures for web search engines?,"[{""answer_confidence"":{""high"":False,""low"":False,""medium"":True,""very_high"":False,""very_low"":False},""passage-snippets-annotation-qualification-task"":{""entities"":[{""endOffset"":9,""label"":""relevant-text-span"",""startOffset"":0},{""endOffset"":20,""label"":""relevant-text-span"",""startOffset"":14},{""endOffset"":230,""label"":""relevant-text-span"",""startOffset"":122},{""endOffset"":351,""label"":""relevant-text-span"",""startOffset"":233}]}}]",x,,"['Precision', 'recall', ""Precision is the fraction of the retrieved result documents that are relevant to the user's information need"", 'Recall is defined as the fraction of relevant documents in the entire collection that are returned as result documents']"
3ZG552ORBQJG8BFXL7X6FIMKYIZV2E,3I3MQQJ31WXKIN6QF6OQSRUDQS81J6,Identify relevant text spans in text passage,Identify all the text spans that contain key pieces of the answer to a given question,"text, text spans, relevance",$0.30,Fri May 19 03:53:01 PDT 2023,3,BatchId:5076839;OriginalHitTemplateId:929448123;,21600,259200,Mon May 22 03:53:01 PDT 2023,,,3WOKGM4L76VH911GVACGQMVOJS3O0C,worker_232,Submitted,Fri May 19 18:45:15 PDT 2023,Fri May 19 18:55:03 PDT 2023,Mon May 22 18:55:03 PDT 2023,,,,588,100% (208/208),100% (179/179),100% (178/178),104_12,0,CAR_caadc5b1d3560cab0db70c6ffa486540cb97fd59,3,Precision and recall have been two of the traditional performance measures for evaluating information retrieval systems.  Precision is the fraction of the retrieved result documents that are relevant to the user's information need.  Recall is defined as the fraction of relevant documents in the entire collection that are returned as result documents.,,What are important evaluation measures for web search engines?,"[{""answer_confidence"":{""high"":False,""low"":False,""medium"":False,""very_high"":True,""very_low"":False},""passage-snippets-annotation-qualification-task"":{""entities"":[{""endOffset"":121,""label"":""relevant-text-span"",""startOffset"":0}]}}]",x,,['Precision and recall have been two of the traditional performance measures for evaluating information retrieval systems. ']
3ZG552ORBQJG8BFXL7X6FIMKYIZV2E,3I3MQQJ31WXKIN6QF6OQSRUDQS81J6,Identify relevant text spans in text passage,Identify all the text spans that contain key pieces of the answer to a given question,"text, text spans, relevance",$0.30,Fri May 19 03:53:01 PDT 2023,3,BatchId:5076839;OriginalHitTemplateId:929448123;,21600,259200,Mon May 22 03:53:01 PDT 2023,,,3PW9OPU9PVZ39JXT4VIJXEBVND0216,worker_249,Submitted,Fri May 19 06:19:27 PDT 2023,Fri May 19 06:20:54 PDT 2023,Mon May 22 06:20:54 PDT 2023,,,,87,100% (194/194),100% (191/191),100% (190/190),104_12,0,CAR_caadc5b1d3560cab0db70c6ffa486540cb97fd59,3,Precision and recall have been two of the traditional performance measures for evaluating information retrieval systems.  Precision is the fraction of the retrieved result documents that are relevant to the user's information need.  Recall is defined as the fraction of relevant documents in the entire collection that are returned as result documents.,,What are important evaluation measures for web search engines?,"[{""answer_confidence"":{""high"":False,""low"":False,""medium"":False,""very_high"":True,""very_low"":False},""passage-snippets-annotation-qualification-task"":{""entities"":[{""endOffset"":9,""label"":""relevant-text-span"",""startOffset"":0},{""endOffset"":20,""label"":""relevant-text-span"",""startOffset"":13},{""endOffset"":131,""label"":""relevant-text-span"",""startOffset"":121},{""endOffset"":230,""label"":""relevant-text-span"",""startOffset"":135},{""endOffset"":240,""label"":""relevant-text-span"",""startOffset"":232},{""endOffset"":352,""label"":""relevant-text-span"",""startOffset"":254}]}}]",x,,"['Precision', ' recall', ' Precision', ""the fraction of the retrieved result documents that are relevant to the user's information need"", ' Recall ', 'the fraction of relevant documents in the entire collection that are returned as result documents.']"
3MG8450X3SPCN3R3IFYDF9SUVLIPU6,3I3MQQJ31WXKIN6QF6OQSRUDQS81J6,Identify relevant text spans in text passage,Identify all the text spans that contain key pieces of the answer to a given question,"text, text spans, relevance",$0.30,Fri May 19 03:53:01 PDT 2023,3,BatchId:5076839;OriginalHitTemplateId:929448123;,21600,259200,Mon May 22 03:53:01 PDT 2023,,,3WT783CTPGWHKZ7N3L0EF3XQLH9CBF,worker_119,Submitted,Fri May 19 11:14:36 PDT 2023,Fri May 19 12:28:19 PDT 2023,Mon May 22 12:28:19 PDT 2023,,,,4423,100% (270/271),100% (179/179),100% (178/178),104_12,0,CAR_cef18912f5c9b7c722a3cf83451a2222f7520afa,3,"The evaluation of an information retrieval system is the process of assessing how well a system meets the information needs of its users. Traditional evaluation metrics, designed for Boolean retrieval or top-k retrieval, include precision and recall.  Many more measures for evaluating the performance of information retrieval systems have also been proposed. In general, measurement considers a collection of documents to be searched and a search query. All common measures described here assume a ground truth notion of relevancy: every document is known to be either relevant or non-relevant to a particular query. In practice, queries may be ill-posed and there may be different shades of relevancy.",,What are important evaluation measures for web search engines?,"[{""answer_confidence"":{""high"":False,""low"":False,""medium"":True,""very_high"":False,""very_low"":False},""passage-snippets-annotation-qualification-task"":{""entities"":[{""endOffset"":238,""label"":""relevant-text-span"",""startOffset"":229},{""endOffset"":249,""label"":""relevant-text-span"",""startOffset"":243},{""endOffset"":454,""label"":""relevant-text-span"",""startOffset"":360}]}}]",x,,"['precision', 'recall', 'In general, measurement considers a collection of documents to be searched and a search query.']"
3MG8450X3SPCN3R3IFYDF9SUVLIPU6,3I3MQQJ31WXKIN6QF6OQSRUDQS81J6,Identify relevant text spans in text passage,Identify all the text spans that contain key pieces of the answer to a given question,"text, text spans, relevance",$0.30,Fri May 19 03:53:01 PDT 2023,3,BatchId:5076839;OriginalHitTemplateId:929448123;,21600,259200,Mon May 22 03:53:01 PDT 2023,,,337RC3OW0AHPWHZZLFKDJYHKIJAVLT,worker_232,Submitted,Fri May 19 18:33:07 PDT 2023,Fri May 19 18:43:58 PDT 2023,Mon May 22 18:43:58 PDT 2023,,,,651,100% (208/208),100% (179/179),100% (178/178),104_12,0,CAR_cef18912f5c9b7c722a3cf83451a2222f7520afa,3,"The evaluation of an information retrieval system is the process of assessing how well a system meets the information needs of its users. Traditional evaluation metrics, designed for Boolean retrieval or top-k retrieval, include precision and recall.  Many more measures for evaluating the performance of information retrieval systems have also been proposed. In general, measurement considers a collection of documents to be searched and a search query. All common measures described here assume a ground truth notion of relevancy: every document is known to be either relevant or non-relevant to a particular query. In practice, queries may be ill-posed and there may be different shades of relevancy.",,What are important evaluation measures for web search engines?,"[{""answer_confidence"":{""high"":False,""low"":False,""medium"":True,""very_high"":False,""very_low"":False},""passage-snippets-annotation-qualification-task"":{""entities"":[{""endOffset"":250,""label"":""relevant-text-span"",""startOffset"":138}]}}]",x,,"['Traditional evaluation metrics, designed for Boolean retrieval or top-k retrieval, include precision and recall.']"
3MG8450X3SPCN3R3IFYDF9SUVLIPU6,3I3MQQJ31WXKIN6QF6OQSRUDQS81J6,Identify relevant text spans in text passage,Identify all the text spans that contain key pieces of the answer to a given question,"text, text spans, relevance",$0.30,Fri May 19 03:53:01 PDT 2023,3,BatchId:5076839;OriginalHitTemplateId:929448123;,21600,259200,Mon May 22 03:53:01 PDT 2023,,,3HSYG7LRBOD0W0N51BU52PTODJ0KKR,worker_249,Submitted,Fri May 19 07:51:11 PDT 2023,Fri May 19 07:54:02 PDT 2023,Mon May 22 07:54:02 PDT 2023,,,,171,100% (194/194),100% (191/191),100% (190/190),104_12,0,CAR_cef18912f5c9b7c722a3cf83451a2222f7520afa,3,"The evaluation of an information retrieval system is the process of assessing how well a system meets the information needs of its users. Traditional evaluation metrics, designed for Boolean retrieval or top-k retrieval, include precision and recall.  Many more measures for evaluating the performance of information retrieval systems have also been proposed. In general, measurement considers a collection of documents to be searched and a search query. All common measures described here assume a ground truth notion of relevancy: every document is known to be either relevant or non-relevant to a particular query. In practice, queries may be ill-posed and there may be different shades of relevancy.",,What are important evaluation measures for web search engines?,"[{""answer_confidence"":{""high"":False,""low"":False,""medium"":False,""very_high"":True,""very_low"":False},""passage-snippets-annotation-qualification-task"":{""entities"":[{""endOffset"":238,""label"":""relevant-text-span"",""startOffset"":228},{""endOffset"":250,""label"":""relevant-text-span"",""startOffset"":242},{""endOffset"":531,""label"":""relevant-text-span"",""startOffset"":521}]}}]",x,,"[' precision', ' recall.', ' relevancy']"
3KL228NDNZ1S7UTSLI4OFD54S3XGKW,3I3MQQJ31WXKIN6QF6OQSRUDQS81J6,Identify relevant text spans in text passage,Identify all the text spans that contain key pieces of the answer to a given question,"text, text spans, relevance",$0.30,Fri May 19 03:53:01 PDT 2023,3,BatchId:5076839;OriginalHitTemplateId:929448123;,21600,259200,Mon May 22 03:53:01 PDT 2023,,,3SNVL38CI97ID2BH9YD7E6GG8B0KCP,worker_119,Submitted,Fri May 19 14:29:48 PDT 2023,Fri May 19 14:33:06 PDT 2023,Mon May 22 14:33:06 PDT 2023,,,,198,100% (270/271),100% (179/179),100% (178/178),104_12,0,CAR_f33ab18d9f433da1dd64162661d0a4a86b1018b7,2,"Together, these components form an information retrieval test collection. The test collection serves as a standard for testing retrieval approaches, and the success of each approach is measured in terms of two measures: precision and recall. Test collections and evaluation measures based on precision and recall are driving forces behind modern research on search systems. Cleverdon's approach formed a blueprint for the successful Text Retrieval Conference series that began in 1992.",1.0,What are important evaluation measures for web search engines?,"[{""answer_confidence"":{""high"":False,""low"":True,""medium"":False,""very_high"":False,""very_low"":False},""passage-snippets-annotation-qualification-task"":{""entities"":[{""endOffset"":229,""label"":""relevant-text-span"",""startOffset"":220},{""endOffset"":240,""label"":""relevant-text-span"",""startOffset"":234},{""endOffset"":373,""label"":""relevant-text-span"",""startOffset"":242}]}}]",x,,"['precision', 'recall', 'Test collections and evaluation measures based on precision and recall are driving forces behind modern research on search systems.']"
3KL228NDNZ1S7UTSLI4OFD54S3XGKW,3I3MQQJ31WXKIN6QF6OQSRUDQS81J6,Identify relevant text spans in text passage,Identify all the text spans that contain key pieces of the answer to a given question,"text, text spans, relevance",$0.30,Fri May 19 03:53:01 PDT 2023,3,BatchId:5076839;OriginalHitTemplateId:929448123;,21600,259200,Mon May 22 03:53:01 PDT 2023,,,3MAOD8E57VP7O67FX71AKBIU3PJNXR,worker_232,Submitted,Fri May 19 18:44:53 PDT 2023,Fri May 19 18:47:14 PDT 2023,Mon May 22 18:47:14 PDT 2023,,,,141,100% (208/208),100% (179/179),100% (178/178),104_12,0,CAR_f33ab18d9f433da1dd64162661d0a4a86b1018b7,2,"Together, these components form an information retrieval test collection. The test collection serves as a standard for testing retrieval approaches, and the success of each approach is measured in terms of two measures: precision and recall. Test collections and evaluation measures based on precision and recall are driving forces behind modern research on search systems. Cleverdon's approach formed a blueprint for the successful Text Retrieval Conference series that began in 1992.",1.0,What are important evaluation measures for web search engines?,"[{""answer_confidence"":{""high"":False,""low"":False,""medium"":True,""very_high"":False,""very_low"":False},""passage-snippets-annotation-qualification-task"":{""entities"":[]}}]",,Chosen text spans are too long (longer than half of the passage). The requirements say that text spans should be concise and as short as possible.,[]
3KL228NDNZ1S7UTSLI4OFD54S3XGKW,3I3MQQJ31WXKIN6QF6OQSRUDQS81J6,Identify relevant text spans in text passage,Identify all the text spans that contain key pieces of the answer to a given question,"text, text spans, relevance",$0.30,Fri May 19 03:53:01 PDT 2023,3,BatchId:5076839;OriginalHitTemplateId:929448123;,21600,259200,Mon May 22 03:53:01 PDT 2023,,,32M8BPYGAY14OJ1YBHOERHMIEXTIG8,worker_249,Approved,Fri May 19 05:15:28 PDT 2023,Fri May 19 05:16:50 PDT 2023,Mon May 22 05:16:50 PDT 2023,Mon May 22 05:17:19 PDT 2023,,,82,100% (194/194),100% (191/191),100% (190/190),104_12,0,CAR_f33ab18d9f433da1dd64162661d0a4a86b1018b7,2,"Together, these components form an information retrieval test collection. The test collection serves as a standard for testing retrieval approaches, and the success of each approach is measured in terms of two measures: precision and recall. Test collections and evaluation measures based on precision and recall are driving forces behind modern research on search systems. Cleverdon's approach formed a blueprint for the successful Text Retrieval Conference series that began in 1992.",1.0,What are important evaluation measures for web search engines?,"[{""answer_confidence"":{""high"":False,""low"":False,""medium"":False,""very_high"":True,""very_low"":False},""passage-snippets-annotation-qualification-task"":{""entities"":[{""endOffset"":230,""label"":""relevant-text-span"",""startOffset"":219},{""endOffset"":240,""label"":""relevant-text-span"",""startOffset"":234},{""endOffset"":312,""label"":""relevant-text-span"",""startOffset"":274}]}}]",x,,"[' precision ', 'recall', 'measures based on precision and recall']"
3J5XXLQDIQQ5TTBGP7BBYSRC4WZV3P,3I3MQQJ31WXKIN6QF6OQSRUDQS81J6,Identify relevant text spans in text passage,Identify all the text spans that contain key pieces of the answer to a given question,"text, text spans, relevance",$0.30,Fri May 19 03:53:02 PDT 2023,3,BatchId:5076839;OriginalHitTemplateId:929448123;,21600,259200,Mon May 22 03:53:02 PDT 2023,,,3UNH76FOCXKQ6TCPQ28CQA0303LMYA,worker_119,Submitted,Fri May 19 09:31:56 PDT 2023,Fri May 19 09:37:22 PDT 2023,Mon May 22 09:37:22 PDT 2023,,,,326,100% (270/271),100% (179/179),100% (178/178),104_12,0,CAR_f395f64260c6e48b2a4b7a04a488873d136ff5e9,2,"Virtually all modern evaluation metrics (e.g., mean average precision, discounted cumulative gain) are designed for ranked retrieval without any explicit rank cutoff, taking into account the relative order of the documents retrieved by the search engines and giving more weight to documents returned at higher ranks.",2.0,What are important evaluation measures for web search engines?,"[{""answer_confidence"":{""high"":True,""low"":False,""medium"":False,""very_high"":False,""very_low"":False},""passage-snippets-annotation-qualification-task"":{""entities"":[{""endOffset"":69,""label"":""relevant-text-span"",""startOffset"":47},{""endOffset"":97,""label"":""relevant-text-span"",""startOffset"":71}]}}]",x,,"['mean average precision', 'discounted cumulative gain']"
3J5XXLQDIQQ5TTBGP7BBYSRC4WZV3P,3I3MQQJ31WXKIN6QF6OQSRUDQS81J6,Identify relevant text spans in text passage,Identify all the text spans that contain key pieces of the answer to a given question,"text, text spans, relevance",$0.30,Fri May 19 03:53:02 PDT 2023,3,BatchId:5076839;OriginalHitTemplateId:929448123;,21600,259200,Mon May 22 03:53:02 PDT 2023,,,3JJVG1YBEGCWLEY8OBD61XRGT1TB5E,worker_232,Submitted,Fri May 19 18:14:28 PDT 2023,Fri May 19 18:32:51 PDT 2023,Mon May 22 18:32:51 PDT 2023,,,,1103,100% (208/208),100% (179/179),100% (178/178),104_12,0,CAR_f395f64260c6e48b2a4b7a04a488873d136ff5e9,2,"Virtually all modern evaluation metrics (e.g., mean average precision, discounted cumulative gain) are designed for ranked retrieval without any explicit rank cutoff, taking into account the relative order of the documents retrieved by the search engines and giving more weight to documents returned at higher ranks.",2.0,What are important evaluation measures for web search engines?,"[{""answer_confidence"":{""high"":False,""low"":False,""medium"":True,""very_high"":False,""very_low"":False},""passage-snippets-annotation-qualification-task"":{""entities"":[{""endOffset"":166,""label"":""relevant-text-span"",""startOffset"":0}]}}]",,Chosen text spans are too long (longer than half of the passage). The requirements say that text spans should be concise and as short as possible.,"['Virtually all modern evaluation metrics (e.g., mean average precision, discounted cumulative gain) are designed for ranked retrieval without any explicit rank cutoff,']"
3J5XXLQDIQQ5TTBGP7BBYSRC4WZV3P,3I3MQQJ31WXKIN6QF6OQSRUDQS81J6,Identify relevant text spans in text passage,Identify all the text spans that contain key pieces of the answer to a given question,"text, text spans, relevance",$0.30,Fri May 19 03:53:02 PDT 2023,3,BatchId:5076839;OriginalHitTemplateId:929448123;,21600,259200,Mon May 22 03:53:02 PDT 2023,,,3ZAZR5XV06XD2XA39ZGH1L4TCJOZCZ,worker_249,Submitted,Fri May 19 06:10:04 PDT 2023,Fri May 19 06:16:23 PDT 2023,Mon May 22 06:16:23 PDT 2023,,,,379,100% (194/194),100% (191/191),100% (190/190),104_12,0,CAR_f395f64260c6e48b2a4b7a04a488873d136ff5e9,2,"Virtually all modern evaluation metrics (e.g., mean average precision, discounted cumulative gain) are designed for ranked retrieval without any explicit rank cutoff, taking into account the relative order of the documents retrieved by the search engines and giving more weight to documents returned at higher ranks.",2.0,What are important evaluation measures for web search engines?,"[{""answer_confidence"":{""high"":False,""low"":False,""medium"":False,""very_high"":True,""very_low"":False},""passage-snippets-annotation-qualification-task"":{""entities"":[{""endOffset"":69,""label"":""relevant-text-span"",""startOffset"":47},{""endOffset"":97,""label"":""relevant-text-span"",""startOffset"":71},{""endOffset"":232,""label"":""relevant-text-span"",""startOffset"":200},{""endOffset"":315,""label"":""relevant-text-span"",""startOffset"":281}]}}]",x,,"['mean average precision', 'discounted cumulative gain', 'order of the documents retrieved', 'documents returned at higher ranks']"
3H6W48L9G84PF7G8DFDEXXDRG1TPW5,3I3MQQJ31WXKIN6QF6OQSRUDQS81J6,Identify relevant text spans in text passage,Identify all the text spans that contain key pieces of the answer to a given question,"text, text spans, relevance",$0.30,Fri May 19 03:53:02 PDT 2023,3,BatchId:5076839;OriginalHitTemplateId:929448123;,21600,259200,Mon May 22 03:53:02 PDT 2023,,,3MD9PLUKKNTWT17UYPPVHWY5BO9NZE,worker_119,Submitted,Fri May 19 11:14:37 PDT 2023,Fri May 19 12:35:28 PDT 2023,Mon May 22 12:35:28 PDT 2023,,,,4851,100% (270/271),100% (179/179),100% (178/178),104_13,0,CAR_0330dd95b0222318cced0ad2b9e81100c26bc56a,2,"An example of this is dwell time, which is a measure of how long a user spends viewing the page linked to in a search result. It is an indicator of how well the search result met the query intent of the user, and is used as a feedback mechanism to improve search results.Another example of this is the Surf Canyon browser extension, which advances search results from later pages of the result set based on both user interaction (clicking an icon) and time spent viewing the page linked to in a search result.",0.0,What are important online evaluation metrics for web search?,"[{""answer_confidence"":{""high"":False,""low"":False,""medium"":True,""very_high"":False,""very_low"":False},""passage-snippets-annotation-qualification-task"":{""entities"":[{""endOffset"":124,""label"":""relevant-text-span"",""startOffset"":22},{""endOffset"":508,""label"":""relevant-text-span"",""startOffset"":298}]}}]",x,,"['dwell time, which is a measure of how long a user spends viewing the page linked to in a search result', 'the Surf Canyon browser extension, which advances search results from later pages of the result set based on both user interaction (clicking an icon) and time spent viewing the page linked to in a search result']"
3H6W48L9G84PF7G8DFDEXXDRG1TPW5,3I3MQQJ31WXKIN6QF6OQSRUDQS81J6,Identify relevant text spans in text passage,Identify all the text spans that contain key pieces of the answer to a given question,"text, text spans, relevance",$0.30,Fri May 19 03:53:02 PDT 2023,3,BatchId:5076839;OriginalHitTemplateId:929448123;,21600,259200,Mon May 22 03:53:02 PDT 2023,,,3OWEPKL08ERD9R1OFUXCQC561W2N7U,worker_232,Submitted,Fri May 19 18:14:23 PDT 2023,Fri May 19 18:22:23 PDT 2023,Mon May 22 18:22:23 PDT 2023,,,,480,100% (208/208),100% (179/179),100% (178/178),104_13,0,CAR_0330dd95b0222318cced0ad2b9e81100c26bc56a,2,"An example of this is dwell time, which is a measure of how long a user spends viewing the page linked to in a search result. It is an indicator of how well the search result met the query intent of the user, and is used as a feedback mechanism to improve search results.Another example of this is the Surf Canyon browser extension, which advances search results from later pages of the result set based on both user interaction (clicking an icon) and time spent viewing the page linked to in a search result.",0.0,What are important online evaluation metrics for web search?,"[{""answer_confidence"":{""high"":False,""low"":False,""medium"":True,""very_high"":False,""very_low"":False},""passage-snippets-annotation-qualification-task"":{""entities"":[{""endOffset"":126,""label"":""relevant-text-span"",""startOffset"":21}]}}]",x,,"[' dwell time, which is a measure of how long a user spends viewing the page linked to in a search result. ']"
3H6W48L9G84PF7G8DFDEXXDRG1TPW5,3I3MQQJ31WXKIN6QF6OQSRUDQS81J6,Identify relevant text spans in text passage,Identify all the text spans that contain key pieces of the answer to a given question,"text, text spans, relevance",$0.30,Fri May 19 03:53:02 PDT 2023,3,BatchId:5076839;OriginalHitTemplateId:929448123;,21600,259200,Mon May 22 03:53:02 PDT 2023,,,3FTF2T8WLWX7A4YVMSMJ0R93PVOW9U,worker_249,Approved,Fri May 19 05:08:24 PDT 2023,Fri May 19 05:11:18 PDT 2023,Mon May 22 05:11:18 PDT 2023,Mon May 22 05:11:19 PDT 2023,,,174,100% (194/194),100% (191/191),100% (190/190),104_13,0,CAR_0330dd95b0222318cced0ad2b9e81100c26bc56a,2,"An example of this is dwell time, which is a measure of how long a user spends viewing the page linked to in a search result. It is an indicator of how well the search result met the query intent of the user, and is used as a feedback mechanism to improve search results.Another example of this is the Surf Canyon browser extension, which advances search results from later pages of the result set based on both user interaction (clicking an icon) and time spent viewing the page linked to in a search result.",0.0,What are important online evaluation metrics for web search?,"[{""answer_confidence"":{""high"":False,""low"":False,""medium"":False,""very_high"":True,""very_low"":False},""passage-snippets-annotation-qualification-task"":{""entities"":[{""endOffset"":32,""label"":""relevant-text-span"",""startOffset"":22},{""endOffset"":124,""label"":""relevant-text-span"",""startOffset"":56},{""endOffset"":447,""label"":""relevant-text-span"",""startOffset"":412},{""endOffset"":509,""label"":""relevant-text-span"",""startOffset"":452}]}}]",x,,"['dwell time', 'how long a user spends viewing the page linked to in a search result', 'user interaction (clicking an icon)', 'time spent viewing the page linked to in a search result.']"
3X0EMNLXFT4C0IG3N58IBVUD9SRPVV,3I3MQQJ31WXKIN6QF6OQSRUDQS81J6,Identify relevant text spans in text passage,Identify all the text spans that contain key pieces of the answer to a given question,"text, text spans, relevance",$0.30,Fri May 19 03:53:02 PDT 2023,3,BatchId:5076839;OriginalHitTemplateId:929448123;,21600,259200,Mon May 22 03:53:02 PDT 2023,,,3I2PTA7R3Y9M73L0EGE2YB5XCQ2QK1,worker_119,Submitted,Fri May 19 14:42:06 PDT 2023,Fri May 19 14:54:48 PDT 2023,Mon May 22 14:54:48 PDT 2023,,,,762,100% (270/271),100% (179/179),100% (178/178),104_13,0,CAR_2571b660a8d41e2c5e8439cbe749ecf22faa0a33,3,"Session success rate measures the ratio of user sessions that lead to a success. Defining 'success' is often dependent on context, but for search a successful result is often measured using dwell time as a primary factor along with secondary user interaction, for instance, the user copying the result URL is considered a successful result, as is copy/pasting from the snippet.",,What are important online evaluation metrics for web search?,"[{""answer_confidence"":{""high"":False,""low"":False,""medium"":True,""very_high"":False,""very_low"":False},""passage-snippets-annotation-qualification-task"":{""entities"":[{""endOffset"":377,""label"":""relevant-text-span"",""startOffset"":135}]}}]",,Chosen text spans are too long (longer than half of the passage). The requirements say that text spans should be concise and as short as possible.,"['for search a successful result is often measured using dwell time as a primary factor along with secondary user interaction, for instance, the user copying the result URL is considered a successful result, as is copy/pasting from the snippet.']"
3X0EMNLXFT4C0IG3N58IBVUD9SRPVV,3I3MQQJ31WXKIN6QF6OQSRUDQS81J6,Identify relevant text spans in text passage,Identify all the text spans that contain key pieces of the answer to a given question,"text, text spans, relevance",$0.30,Fri May 19 03:53:02 PDT 2023,3,BatchId:5076839;OriginalHitTemplateId:929448123;,21600,259200,Mon May 22 03:53:02 PDT 2023,,,3FPRZHYEP3M8GD9AF9GDNCN3IXIV3E,worker_232,Submitted,Fri May 19 18:14:25 PDT 2023,Fri May 19 18:29:17 PDT 2023,Mon May 22 18:29:17 PDT 2023,,,,892,100% (208/208),100% (179/179),100% (178/178),104_13,0,CAR_2571b660a8d41e2c5e8439cbe749ecf22faa0a33,3,"Session success rate measures the ratio of user sessions that lead to a success. Defining 'success' is often dependent on context, but for search a successful result is often measured using dwell time as a primary factor along with secondary user interaction, for instance, the user copying the result URL is considered a successful result, as is copy/pasting from the snippet.",,What are important online evaluation metrics for web search?,"[{""answer_confidence"":{""high"":True,""low"":False,""medium"":False,""very_high"":False,""very_low"":False},""passage-snippets-annotation-qualification-task"":{""entities"":[{""endOffset"":79,""label"":""relevant-text-span"",""startOffset"":0}]}}]",x,,['Session success rate measures the ratio of user sessions that lead to a success']
3X0EMNLXFT4C0IG3N58IBVUD9SRPVV,3I3MQQJ31WXKIN6QF6OQSRUDQS81J6,Identify relevant text spans in text passage,Identify all the text spans that contain key pieces of the answer to a given question,"text, text spans, relevance",$0.30,Fri May 19 03:53:02 PDT 2023,3,BatchId:5076839;OriginalHitTemplateId:929448123;,21600,259200,Mon May 22 03:53:02 PDT 2023,,,3YT88D1N0DDU02FYG526L3CM2XGK3I,worker_249,Submitted,Fri May 19 08:15:37 PDT 2023,Fri May 19 08:18:47 PDT 2023,Mon May 22 08:18:47 PDT 2023,,,,190,100% (194/194),100% (191/191),100% (190/190),104_13,0,CAR_2571b660a8d41e2c5e8439cbe749ecf22faa0a33,3,"Session success rate measures the ratio of user sessions that lead to a success. Defining 'success' is often dependent on context, but for search a successful result is often measured using dwell time as a primary factor along with secondary user interaction, for instance, the user copying the result URL is considered a successful result, as is copy/pasting from the snippet.",,What are important online evaluation metrics for web search?,"[{""answer_confidence"":{""high"":False,""low"":False,""medium"":True,""very_high"":False,""very_low"":False},""passage-snippets-annotation-qualification-task"":{""entities"":[{""endOffset"":20,""label"":""relevant-text-span"",""startOffset"":0},{""endOffset"":79,""label"":""relevant-text-span"",""startOffset"":29},{""endOffset"":200,""label"":""relevant-text-span"",""startOffset"":189},{""endOffset"":258,""label"":""relevant-text-span"",""startOffset"":232}]}}]",x,,"['Session success rate', ' the ratio of user sessions that lead to a success', ' dwell time', 'secondary user interaction']"
3OCZWXS70SM5B3MQPMXYQGRURUOL5M,3I3MQQJ31WXKIN6QF6OQSRUDQS81J6,Identify relevant text spans in text passage,Identify all the text spans that contain key pieces of the answer to a given question,"text, text spans, relevance",$0.30,Fri May 19 03:53:02 PDT 2023,3,BatchId:5076839;OriginalHitTemplateId:929448123;,21600,259200,Mon May 22 03:53:02 PDT 2023,,,3LOTDFNYACEGBEP8PEMBANVESV2FWS,worker_119,Submitted,Fri May 19 14:42:03 PDT 2023,Fri May 19 14:53:35 PDT 2023,Mon May 22 14:53:35 PDT 2023,,,,692,100% (270/271),100% (179/179),100% (178/178),104_13,0,CAR_41a5ee05efa9c31f95bd71253894fb65d67d2b79,3,"The most important factors determining whether customers return to a website are ease of use and the presence of user-friendly features. Usability testing is important for finding problems and improvements in a web site. Methods for evaluating usability include heuristic evaluation, cognitive walkthrough, and user testing. Each technique has its own characteristics and emphasizes different aspects of the user experience.",,What are important online evaluation metrics for web search?,"[{""answer_confidence"":{""high"":False,""low"":True,""medium"":False,""very_high"":False,""very_low"":False},""passage-snippets-annotation-qualification-task"":{""entities"":[{""endOffset"":323,""label"":""relevant-text-span"",""startOffset"":222}]}}]",x,,"['ethods for evaluating usability include heuristic evaluation, cognitive walkthrough, and user testing']"
3OCZWXS70SM5B3MQPMXYQGRURUOL5M,3I3MQQJ31WXKIN6QF6OQSRUDQS81J6,Identify relevant text spans in text passage,Identify all the text spans that contain key pieces of the answer to a given question,"text, text spans, relevance",$0.30,Fri May 19 03:53:02 PDT 2023,3,BatchId:5076839;OriginalHitTemplateId:929448123;,21600,259200,Mon May 22 03:53:02 PDT 2023,,,36ZN444YTWDSGWIWZ01MWQVK5MKIO1,worker_232,Submitted,Fri May 19 18:45:15 PDT 2023,Fri May 19 18:54:43 PDT 2023,Mon May 22 18:54:43 PDT 2023,,,,568,100% (208/208),100% (179/179),100% (178/178),104_13,0,CAR_41a5ee05efa9c31f95bd71253894fb65d67d2b79,3,"The most important factors determining whether customers return to a website are ease of use and the presence of user-friendly features. Usability testing is important for finding problems and improvements in a web site. Methods for evaluating usability include heuristic evaluation, cognitive walkthrough, and user testing. Each technique has its own characteristics and emphasizes different aspects of the user experience.",,What are important online evaluation metrics for web search?,"[{""answer_confidence"":{""high"":True,""low"":False,""medium"":False,""very_high"":False,""very_low"":False},""passage-snippets-annotation-qualification-task"":{""entities"":[{""endOffset"":325,""label"":""relevant-text-span"",""startOffset"":221}]}}]",x,,"['Methods for evaluating usability include heuristic evaluation, cognitive walkthrough, and user testing. ']"
3OCZWXS70SM5B3MQPMXYQGRURUOL5M,3I3MQQJ31WXKIN6QF6OQSRUDQS81J6,Identify relevant text spans in text passage,Identify all the text spans that contain key pieces of the answer to a given question,"text, text spans, relevance",$0.30,Fri May 19 03:53:02 PDT 2023,3,BatchId:5076839;OriginalHitTemplateId:929448123;,21600,259200,Mon May 22 03:53:02 PDT 2023,,,3QEMNNSB22E4NFABQ0J9U7TCNQPD7N,worker_249,Submitted,Fri May 19 06:22:17 PDT 2023,Fri May 19 06:26:48 PDT 2023,Mon May 22 06:26:48 PDT 2023,,,,271,100% (194/194),100% (191/191),100% (190/190),104_13,0,CAR_41a5ee05efa9c31f95bd71253894fb65d67d2b79,3,"The most important factors determining whether customers return to a website are ease of use and the presence of user-friendly features. Usability testing is important for finding problems and improvements in a web site. Methods for evaluating usability include heuristic evaluation, cognitive walkthrough, and user testing. Each technique has its own characteristics and emphasizes different aspects of the user experience.",,What are important online evaluation metrics for web search?,"[{""answer_confidence"":{""high"":False,""low"":False,""medium"":True,""very_high"":False,""very_low"":False},""passage-snippets-annotation-qualification-task"":{""entities"":[{""endOffset"":92,""label"":""relevant-text-span"",""startOffset"":81},{""endOffset"":135,""label"":""relevant-text-span"",""startOffset"":97},{""endOffset"":147,""label"":""relevant-text-span"",""startOffset"":136},{""endOffset"":253,""label"":""relevant-text-span"",""startOffset"":244},{""endOffset"":423,""label"":""relevant-text-span"",""startOffset"":403}]}}]",x,,"['ease of use', 'the presence of user-friendly features', ' Usability ', 'usability', ' the user experience']"
3GV1I4SEPD41U7M4U573IM5UNPZL62,3I3MQQJ31WXKIN6QF6OQSRUDQS81J6,Identify relevant text spans in text passage,Identify all the text spans that contain key pieces of the answer to a given question,"text, text spans, relevance",$0.30,Fri May 19 03:53:02 PDT 2023,3,BatchId:5076839;OriginalHitTemplateId:929448123;,21600,259200,Mon May 22 03:53:02 PDT 2023,,,354P56DE9PIAP4TTNQEYP6FCOTUS7R,worker_119,Submitted,Fri May 19 09:31:51 PDT 2023,Fri May 19 09:33:27 PDT 2023,Mon May 22 09:33:27 PDT 2023,,,,96,100% (270/271),100% (179/179),100% (178/178),104_13,0,CAR_41b81ffc3f09f82b7e43fe1baea23ed121019777,3,"Internet Application Management services address this “grey area” by evaluating performance using standard web application metrics such as page load times, concurrency capability, and slow queries as a starting point. These optimizations and tuning of a web application can be achieved through first creating benchmarks using tools such as ApacheBench or J-Meter through load simulation. The output of load testing runs can then be used to identify file system and application performance improvements to be made not only at the application level, but at the operating system level - something often overlooked in the development process.",,What are important online evaluation metrics for web search?,"[{""answer_confidence"":{""high"":False,""low"":False,""medium"":True,""very_high"":False,""very_low"":False},""passage-snippets-annotation-qualification-task"":{""entities"":[{""endOffset"":154,""label"":""relevant-text-span"",""startOffset"":139},{""endOffset"":178,""label"":""relevant-text-span"",""startOffset"":156},{""endOffset"":196,""label"":""relevant-text-span"",""startOffset"":184},{""endOffset"":386,""label"":""relevant-text-span"",""startOffset"":309}]}}]",x,,"['page load times', 'concurrency capability', 'slow queries', 'benchmarks using tools such as ApacheBench or J-Meter through load simulation']"
3GV1I4SEPD41U7M4U573IM5UNPZL62,3I3MQQJ31WXKIN6QF6OQSRUDQS81J6,Identify relevant text spans in text passage,Identify all the text spans that contain key pieces of the answer to a given question,"text, text spans, relevance",$0.30,Fri May 19 03:53:02 PDT 2023,3,BatchId:5076839;OriginalHitTemplateId:929448123;,21600,259200,Mon May 22 03:53:02 PDT 2023,,,3VZLGYJEYQP15VLY0VC4FYP3D3RXZI,worker_232,Submitted,Fri May 19 18:45:19 PDT 2023,Fri May 19 18:56:57 PDT 2023,Mon May 22 18:56:57 PDT 2023,,,,698,100% (208/208),100% (179/179),100% (178/178),104_13,0,CAR_41b81ffc3f09f82b7e43fe1baea23ed121019777,3,"Internet Application Management services address this “grey area” by evaluating performance using standard web application metrics such as page load times, concurrency capability, and slow queries as a starting point. These optimizations and tuning of a web application can be achieved through first creating benchmarks using tools such as ApacheBench or J-Meter through load simulation. The output of load testing runs can then be used to identify file system and application performance improvements to be made not only at the application level, but at the operating system level - something often overlooked in the development process.",,What are important online evaluation metrics for web search?,"[{""answer_confidence"":{""high"":False,""low"":False,""medium"":True,""very_high"":False,""very_low"":False},""passage-snippets-annotation-qualification-task"":{""entities"":[{""endOffset"":218,""label"":""relevant-text-span"",""startOffset"":0}]}}]",x,,"['Internet Application Management services address this grey area by evaluating performance using standard web application metrics such as page load times, concurrency capability, and slow queries as a starting point. ']"
3GV1I4SEPD41U7M4U573IM5UNPZL62,3I3MQQJ31WXKIN6QF6OQSRUDQS81J6,Identify relevant text spans in text passage,Identify all the text spans that contain key pieces of the answer to a given question,"text, text spans, relevance",$0.30,Fri May 19 03:53:02 PDT 2023,3,BatchId:5076839;OriginalHitTemplateId:929448123;,21600,259200,Mon May 22 03:53:02 PDT 2023,,,3KRVW3HTZS0TA70GRWGN8OC2JI9SMM,worker_249,Submitted,Fri May 19 08:08:24 PDT 2023,Fri May 19 08:13:04 PDT 2023,Mon May 22 08:13:04 PDT 2023,,,,280,100% (194/194),100% (191/191),100% (190/190),104_13,0,CAR_41b81ffc3f09f82b7e43fe1baea23ed121019777,3,"Internet Application Management services address this “grey area” by evaluating performance using standard web application metrics such as page load times, concurrency capability, and slow queries as a starting point. These optimizations and tuning of a web application can be achieved through first creating benchmarks using tools such as ApacheBench or J-Meter through load simulation. The output of load testing runs can then be used to identify file system and application performance improvements to be made not only at the application level, but at the operating system level - something often overlooked in the development process.",,What are important online evaluation metrics for web search?,"[{""answer_confidence"":{""high"":False,""low"":True,""medium"":False,""very_high"":False,""very_low"":False},""passage-snippets-annotation-qualification-task"":{""entities"":[]}}]",,Chosen text spans are too long (longer than half of the passage). The requirements say that text spans should be concise and as short as possible.,[]
3P4C70TRNVW3R0BA6IGPUDIJH32GLZ,3I3MQQJ31WXKIN6QF6OQSRUDQS81J6,Identify relevant text spans in text passage,Identify all the text spans that contain key pieces of the answer to a given question,"text, text spans, relevance",$0.30,Fri May 19 03:53:02 PDT 2023,3,BatchId:5076839;OriginalHitTemplateId:929448123;,21600,259200,Mon May 22 03:53:02 PDT 2023,,,3018Q3ZVON5G7RREF4QD9NC6309AR6,worker_119,Submitted,Fri May 19 14:29:49 PDT 2023,Fri May 19 14:34:27 PDT 2023,Mon May 22 14:34:27 PDT 2023,,,,278,100% (270/271),100% (179/179),100% (178/178),104_13,0,CAR_f120a021c03cfe36e2ee1fc074368169429a8792,3,"Search analytics is the use of search data to investigate particular interactions among Web searchers, the search engine, or the content during searching episodes. The resulting analysis and aggregation of search engine statistics can be used in search engine marketing (SEM) and search engine optimization (SEO). In other words, search analytics helps website owners understand and improve their performance on search engines, for example identifying highly valuable site visitors. or understanding user intent. Search analytics includes search volume trends and analysis, reverse searching (entering websites to see their keywords), keyword monitoring, search result and advertisement history, advertisement spending statistics, website comparisons, affiliate marketing statistics, multivariate ad testing, et al.",,What are important online evaluation metrics for web search?,"[{""answer_confidence"":{""high"":False,""low"":True,""medium"":False,""very_high"":False,""very_low"":False},""passage-snippets-annotation-qualification-task"":{""entities"":[{""endOffset"":481,""label"":""relevant-text-span"",""startOffset"":440},{""endOffset"":511,""label"":""relevant-text-span"",""startOffset"":486},{""endOffset"":807,""label"":""relevant-text-span"",""startOffset"":513}]}}]",x,,"['identifying highly valuable site visitors', 'understanding user intent', 'Search analytics includes search volume trends and analysis, reverse searching (entering websites to see their keywords), keyword monitoring, search result and advertisement history, advertisement spending statistics, website comparisons, affiliate marketing statistics, multivariate ad testing']"
3P4C70TRNVW3R0BA6IGPUDIJH32GLZ,3I3MQQJ31WXKIN6QF6OQSRUDQS81J6,Identify relevant text spans in text passage,Identify all the text spans that contain key pieces of the answer to a given question,"text, text spans, relevance",$0.30,Fri May 19 03:53:02 PDT 2023,3,BatchId:5076839;OriginalHitTemplateId:929448123;,21600,259200,Mon May 22 03:53:02 PDT 2023,,,3RGU30DZTFN0B4HQ4279GNLFN1TMJ1,worker_232,Submitted,Fri May 19 18:32:57 PDT 2023,Fri May 19 18:37:03 PDT 2023,Mon May 22 18:37:03 PDT 2023,,,,246,100% (208/208),100% (179/179),100% (178/178),104_13,0,CAR_f120a021c03cfe36e2ee1fc074368169429a8792,3,"Search analytics is the use of search data to investigate particular interactions among Web searchers, the search engine, or the content during searching episodes. The resulting analysis and aggregation of search engine statistics can be used in search engine marketing (SEM) and search engine optimization (SEO). In other words, search analytics helps website owners understand and improve their performance on search engines, for example identifying highly valuable site visitors. or understanding user intent. Search analytics includes search volume trends and analysis, reverse searching (entering websites to see their keywords), keyword monitoring, search result and advertisement history, advertisement spending statistics, website comparisons, affiliate marketing statistics, multivariate ad testing, et al.",,What are important online evaluation metrics for web search?,"[{""answer_confidence"":{""high"":True,""low"":False,""medium"":False,""very_high"":False,""very_low"":False},""passage-snippets-annotation-qualification-task"":{""entities"":[{""endOffset"":808,""label"":""relevant-text-span"",""startOffset"":513}]}}]",x,,"['Search analytics includes search volume trends and analysis, reverse searching (entering websites to see their keywords), keyword monitoring, search result and advertisement history, advertisement spending statistics, website comparisons, affiliate marketing statistics, multivariate ad testing,']"
3P4C70TRNVW3R0BA6IGPUDIJH32GLZ,3I3MQQJ31WXKIN6QF6OQSRUDQS81J6,Identify relevant text spans in text passage,Identify all the text spans that contain key pieces of the answer to a given question,"text, text spans, relevance",$0.30,Fri May 19 03:53:02 PDT 2023,3,BatchId:5076839;OriginalHitTemplateId:929448123;,21600,259200,Mon May 22 03:53:02 PDT 2023,,,3BF51CHDT0P7HKFTRXHQ6DYN74E0HL,worker_249,Submitted,Fri May 19 06:32:47 PDT 2023,Fri May 19 07:51:04 PDT 2023,Mon May 22 07:51:04 PDT 2023,,,,4697,100% (194/194),100% (191/191),100% (190/190),104_13,0,CAR_f120a021c03cfe36e2ee1fc074368169429a8792,3,"Search analytics is the use of search data to investigate particular interactions among Web searchers, the search engine, or the content during searching episodes. The resulting analysis and aggregation of search engine statistics can be used in search engine marketing (SEM) and search engine optimization (SEO). In other words, search analytics helps website owners understand and improve their performance on search engines, for example identifying highly valuable site visitors. or understanding user intent. Search analytics includes search volume trends and analysis, reverse searching (entering websites to see their keywords), keyword monitoring, search result and advertisement history, advertisement spending statistics, website comparisons, affiliate marketing statistics, multivariate ad testing, et al.",,What are important online evaluation metrics for web search?,"[{""answer_confidence"":{""high"":False,""low"":True,""medium"":False,""very_high"":False,""very_low"":False},""passage-snippets-annotation-qualification-task"":{""entities"":[{""endOffset"":81,""label"":""relevant-text-span"",""startOffset"":68},{""endOffset"":409,""label"":""relevant-text-span"",""startOffset"":396}]}}]",,The agreement rate not met. None of the text span chosen by you has been selected by other workers.,"[' interactions', ' performance ']"
3SNR5F7RA683HL7DFKTO2U5FNJTIE6,3I3MQQJ31WXKIN6QF6OQSRUDQS81J6,Identify relevant text spans in text passage,Identify all the text spans that contain key pieces of the answer to a given question,"text, text spans, relevance",$0.30,Fri May 19 03:52:59 PDT 2023,3,BatchId:5076839;OriginalHitTemplateId:929448123;,21600,259200,Mon May 22 03:52:59 PDT 2023,,,31JLPPHS2Z9DQT0DEXUZSKTVM223OX,worker_119,Submitted,Fri May 19 09:05:54 PDT 2023,Fri May 19 09:13:35 PDT 2023,Mon May 22 09:13:35 PDT 2023,,,,461,100% (270/271),100% (179/179),100% (178/178),104_3,0,CAR_0527e5e8fce1b419afe3e9fb954b89b9c4d00022,2,"The origin of Freenet can be traced to Ian Clarke's student project at the University of Edinburgh, which he completed as a graduation requirement in the summer of 1999. Ian Clarke's resulting unpublished report 'A distributed decentralized information storage and retrieval system' (1999) provided foundation for the seminal paper written in collaboration with other researchers, 'Freenet: A Distributed Anonymous Information Storage and Retrieval System' (2001). According to CiteSeer, it became one of the most frequently cited computer science articles in 2002.",3.0,Who are some important British Information Retrieval researchers?,"[{""answer_confidence"":{""high"":True,""low"":False,""medium"":False,""very_high"":False,""very_low"":False},""passage-snippets-annotation-qualification-task"":{""entities"":[{""endOffset"":49,""label"":""relevant-text-span"",""startOffset"":39},{""endOffset"":180,""label"":""relevant-text-span"",""startOffset"":170}]}}]",x,,"['Ian Clarke', 'Ian Clarke']"
3SNR5F7RA683HL7DFKTO2U5FNJTIE6,3I3MQQJ31WXKIN6QF6OQSRUDQS81J6,Identify relevant text spans in text passage,Identify all the text spans that contain key pieces of the answer to a given question,"text, text spans, relevance",$0.30,Fri May 19 03:52:59 PDT 2023,3,BatchId:5076839;OriginalHitTemplateId:929448123;,21600,259200,Mon May 22 03:52:59 PDT 2023,,,3G5W44VEUCXVUERMBK9Q4X1V64GKGP,worker_232,Submitted,Fri May 19 18:14:22 PDT 2023,Fri May 19 18:21:42 PDT 2023,Mon May 22 18:21:42 PDT 2023,,,,440,100% (208/208),100% (179/179),100% (178/178),104_3,0,CAR_0527e5e8fce1b419afe3e9fb954b89b9c4d00022,2,"The origin of Freenet can be traced to Ian Clarke's student project at the University of Edinburgh, which he completed as a graduation requirement in the summer of 1999. Ian Clarke's resulting unpublished report 'A distributed decentralized information storage and retrieval system' (1999) provided foundation for the seminal paper written in collaboration with other researchers, 'Freenet: A Distributed Anonymous Information Storage and Retrieval System' (2001). According to CiteSeer, it became one of the most frequently cited computer science articles in 2002.",3.0,Who are some important British Information Retrieval researchers?,"[{""answer_confidence"":{""high"":False,""low"":True,""medium"":False,""very_high"":False,""very_low"":False},""passage-snippets-annotation-qualification-task"":{""entities"":[]}}]",,Chosen text spans are too long (longer than half of the passage). The requirements say that text spans should be concise and as short as possible.,[]
3SNR5F7RA683HL7DFKTO2U5FNJTIE6,3I3MQQJ31WXKIN6QF6OQSRUDQS81J6,Identify relevant text spans in text passage,Identify all the text spans that contain key pieces of the answer to a given question,"text, text spans, relevance",$0.30,Fri May 19 03:52:59 PDT 2023,3,BatchId:5076839;OriginalHitTemplateId:929448123;,21600,259200,Mon May 22 03:52:59 PDT 2023,,,326O153BMNDPWUPLD8XF0HLT9DUEDB,worker_249,Submitted,Fri May 19 06:29:16 PDT 2023,Fri May 19 06:30:58 PDT 2023,Mon May 22 06:30:58 PDT 2023,,,,102,100% (194/194),100% (191/191),100% (190/190),104_3,0,CAR_0527e5e8fce1b419afe3e9fb954b89b9c4d00022,2,"The origin of Freenet can be traced to Ian Clarke's student project at the University of Edinburgh, which he completed as a graduation requirement in the summer of 1999. Ian Clarke's resulting unpublished report 'A distributed decentralized information storage and retrieval system' (1999) provided foundation for the seminal paper written in collaboration with other researchers, 'Freenet: A Distributed Anonymous Information Storage and Retrieval System' (2001). According to CiteSeer, it became one of the most frequently cited computer science articles in 2002.",3.0,Who are some important British Information Retrieval researchers?,"[{""answer_confidence"":{""high"":False,""low"":False,""medium"":False,""very_high"":True,""very_low"":False},""passage-snippets-annotation-qualification-task"":{""entities"":[{""endOffset"":49,""label"":""relevant-text-span"",""startOffset"":38},{""endOffset"":180,""label"":""relevant-text-span"",""startOffset"":169}]}}]",x,,"[' Ian Clarke', ' Ian Clarke']"
30U1YOGZHEBNJTW9O1CI0ETHW5USDF,3I3MQQJ31WXKIN6QF6OQSRUDQS81J6,Identify relevant text spans in text passage,Identify all the text spans that contain key pieces of the answer to a given question,"text, text spans, relevance",$0.30,Fri May 19 03:52:59 PDT 2023,3,BatchId:5076839;OriginalHitTemplateId:929448123;,21600,259200,Mon May 22 03:52:59 PDT 2023,,,3L2IS5HSFFXF72DKS2QN04JQ96DNU8,worker_119,Approved,Fri May 19 14:42:00 PDT 2023,Fri May 19 14:42:37 PDT 2023,Mon May 22 14:42:37 PDT 2023,Mon May 22 05:55:51 PDT 2023,,,37,100% (270/271),100% (179/179),100% (178/178),104_3,0,CAR_0c51063fb9e781181e9514270de3a04b3f06a3f9,2,"Frederick Wilfrid ('Wilf') Lancaster (September 4, 1933 – August 25, 2013) was a British-American information scientist. He immigrated to the USA in 1959 and worked as information specialist for the National Library of Medicine in Bethesda, Maryland from 1965–68. He was a professor at the University of Illinois, Urbana from 1972-92 and professor emeritus from 1992-2013. He continued as an honored scholar after retirement speaking on the evolution of librarianship in the 20th and 21st century.  Lancaster made notable achievements with early online retrieval systems, including evaluation studies of MEDLARS.  He published broadly in Library and Information Science over a period of four decades and continuously emerged as a visionary leader in the field, where research, writing, and teaching earned him the highest honors in the profession. Lancaster excelled at many fronts: as scholar, educator, mentor, and writer.",4.0,Who are some important British Information Retrieval researchers?,"[{""answer_confidence"":{""high"":True,""low"":False,""medium"":False,""very_high"":False,""very_low"":False},""passage-snippets-annotation-qualification-task"":{""entities"":[{""endOffset"":36,""label"":""relevant-text-span"",""startOffset"":0},{""endOffset"":508,""label"":""relevant-text-span"",""startOffset"":499},{""endOffset"":857,""label"":""relevant-text-span"",""startOffset"":848}]}}]",x,,"[""Frederick Wilfrid ('Wilf') Lancaster"", 'Lancaster', 'Lancaster']"
30U1YOGZHEBNJTW9O1CI0ETHW5USDF,3I3MQQJ31WXKIN6QF6OQSRUDQS81J6,Identify relevant text spans in text passage,Identify all the text spans that contain key pieces of the answer to a given question,"text, text spans, relevance",$0.30,Fri May 19 03:52:59 PDT 2023,3,BatchId:5076839;OriginalHitTemplateId:929448123;,21600,259200,Mon May 22 03:52:59 PDT 2023,,,317HQ483IC7AYBIKYL19H1VFFCFINM,worker_232,Approved,Fri May 19 18:45:13 PDT 2023,Fri May 19 18:50:59 PDT 2023,Mon May 22 18:50:59 PDT 2023,Mon May 22 05:55:52 PDT 2023,,,346,100% (208/208),100% (179/179),100% (178/178),104_3,0,CAR_0c51063fb9e781181e9514270de3a04b3f06a3f9,2,"Frederick Wilfrid ('Wilf') Lancaster (September 4, 1933 – August 25, 2013) was a British-American information scientist. He immigrated to the USA in 1959 and worked as information specialist for the National Library of Medicine in Bethesda, Maryland from 1965–68. He was a professor at the University of Illinois, Urbana from 1972-92 and professor emeritus from 1992-2013. He continued as an honored scholar after retirement speaking on the evolution of librarianship in the 20th and 21st century.  Lancaster made notable achievements with early online retrieval systems, including evaluation studies of MEDLARS.  He published broadly in Library and Information Science over a period of four decades and continuously emerged as a visionary leader in the field, where research, writing, and teaching earned him the highest honors in the profession. Lancaster excelled at many fronts: as scholar, educator, mentor, and writer.",4.0,Who are some important British Information Retrieval researchers?,"[{""answer_confidence"":{""high"":False,""low"":False,""medium"":True,""very_high"":False,""very_low"":False},""passage-snippets-annotation-qualification-task"":{""entities"":[{""endOffset"":120,""label"":""relevant-text-span"",""startOffset"":0}]}}]",x,,"[""Frederick Wilfrid ('Wilf') Lancaster (September 4, 1933  August 25, 2013) was a British-American information scientist.""]"
30U1YOGZHEBNJTW9O1CI0ETHW5USDF,3I3MQQJ31WXKIN6QF6OQSRUDQS81J6,Identify relevant text spans in text passage,Identify all the text spans that contain key pieces of the answer to a given question,"text, text spans, relevance",$0.30,Fri May 19 03:52:59 PDT 2023,3,BatchId:5076839;OriginalHitTemplateId:929448123;,21600,259200,Mon May 22 03:52:59 PDT 2023,,,3J4Q2Z4UT3ID23FH3UX5MMZSZWNWQD,worker_249,Approved,Fri May 19 06:26:54 PDT 2023,Fri May 19 06:27:34 PDT 2023,Mon May 22 06:27:34 PDT 2023,Mon May 22 05:55:52 PDT 2023,,,40,100% (194/194),100% (191/191),100% (190/190),104_3,0,CAR_0c51063fb9e781181e9514270de3a04b3f06a3f9,2,"Frederick Wilfrid ('Wilf') Lancaster (September 4, 1933 – August 25, 2013) was a British-American information scientist. He immigrated to the USA in 1959 and worked as information specialist for the National Library of Medicine in Bethesda, Maryland from 1965–68. He was a professor at the University of Illinois, Urbana from 1972-92 and professor emeritus from 1992-2013. He continued as an honored scholar after retirement speaking on the evolution of librarianship in the 20th and 21st century.  Lancaster made notable achievements with early online retrieval systems, including evaluation studies of MEDLARS.  He published broadly in Library and Information Science over a period of four decades and continuously emerged as a visionary leader in the field, where research, writing, and teaching earned him the highest honors in the profession. Lancaster excelled at many fronts: as scholar, educator, mentor, and writer.",4.0,Who are some important British Information Retrieval researchers?,"[{""answer_confidence"":{""high"":False,""low"":False,""medium"":False,""very_high"":True,""very_low"":False},""passage-snippets-annotation-qualification-task"":{""entities"":[{""endOffset"":36,""label"":""relevant-text-span"",""startOffset"":0}]}}]",x,,"[""Frederick Wilfrid ('Wilf') Lancaster""]"
32204AGABFR154SKFX1R2LPC0MTGHS,3I3MQQJ31WXKIN6QF6OQSRUDQS81J6,Identify relevant text spans in text passage,Identify all the text spans that contain key pieces of the answer to a given question,"text, text spans, relevance",$0.30,Fri May 19 03:52:59 PDT 2023,3,BatchId:5076839;OriginalHitTemplateId:929448123;,21600,259200,Mon May 22 03:52:59 PDT 2023,,,30IQTZXKAPL43AF8W1ZC9ZMHVUN0X4,worker_119,Approved,Fri May 19 09:05:35 PDT 2023,Fri May 19 09:08:29 PDT 2023,Mon May 22 09:08:29 PDT 2023,Mon May 22 05:55:52 PDT 2023,,,174,100% (270/271),100% (179/179),100% (178/178),104_3,0,CAR_1c65ae2c1130a92bbd1f35dfbd0ad406b48c81e4,2,"The NOTIS family of products was presented to the British Computing Society by Jeremy Salter. Roger Tagg et al. (BCS, End User SG, 1985) and endorsed as the BCS model for user interface. The same endorsement was awarded to NOTIS-IR as a model for information storage and retrieval. the European Commission published in 1985 NOTIS-IR as reference model for document and information search and retrieval.",2.0,Who are some important British Information Retrieval researchers?,"[{""answer_confidence"":{""high"":False,""low"":False,""medium"":True,""very_high"":False,""very_low"":False},""passage-snippets-annotation-qualification-task"":{""entities"":[{""endOffset"":92,""label"":""relevant-text-span"",""startOffset"":79},{""endOffset"":104,""label"":""relevant-text-span"",""startOffset"":94}]}}]",x,,"['Jeremy Salter', 'Roger Tagg']"
32204AGABFR154SKFX1R2LPC0MTGHS,3I3MQQJ31WXKIN6QF6OQSRUDQS81J6,Identify relevant text spans in text passage,Identify all the text spans that contain key pieces of the answer to a given question,"text, text spans, relevance",$0.30,Fri May 19 03:52:59 PDT 2023,3,BatchId:5076839;OriginalHitTemplateId:929448123;,21600,259200,Mon May 22 03:52:59 PDT 2023,,,3DR23U6WEATWDJLIZYXU5L3FC1SET8,worker_232,Approved,Fri May 19 18:14:15 PDT 2023,Fri May 19 18:17:00 PDT 2023,Mon May 22 18:17:00 PDT 2023,Mon May 22 05:55:52 PDT 2023,,,165,100% (208/208),100% (179/179),100% (178/178),104_3,0,CAR_1c65ae2c1130a92bbd1f35dfbd0ad406b48c81e4,2,"The NOTIS family of products was presented to the British Computing Society by Jeremy Salter. Roger Tagg et al. (BCS, End User SG, 1985) and endorsed as the BCS model for user interface. The same endorsement was awarded to NOTIS-IR as a model for information storage and retrieval. the European Commission published in 1985 NOTIS-IR as reference model for document and information search and retrieval.",2.0,Who are some important British Information Retrieval researchers?,"[{""answer_confidence"":{""high"":False,""low"":False,""medium"":True,""very_high"":False,""very_low"":False},""passage-snippets-annotation-qualification-task"":{""entities"":[{""endOffset"":104,""label"":""relevant-text-span"",""startOffset"":79}]}}]",x,,['Jeremy Salter. Roger Tagg']
32204AGABFR154SKFX1R2LPC0MTGHS,3I3MQQJ31WXKIN6QF6OQSRUDQS81J6,Identify relevant text spans in text passage,Identify all the text spans that contain key pieces of the answer to a given question,"text, text spans, relevance",$0.30,Fri May 19 03:52:59 PDT 2023,3,BatchId:5076839;OriginalHitTemplateId:929448123;,21600,259200,Mon May 22 03:52:59 PDT 2023,,,37QW5D2ZRL1EPIYCXV6436B7ZDL8SF,worker_249,Submitted,Fri May 19 08:13:07 PDT 2023,Fri May 19 08:14:25 PDT 2023,Mon May 22 08:14:25 PDT 2023,,,,78,100% (194/194),100% (191/191),100% (190/190),104_3,0,CAR_1c65ae2c1130a92bbd1f35dfbd0ad406b48c81e4,2,"The NOTIS family of products was presented to the British Computing Society by Jeremy Salter. Roger Tagg et al. (BCS, End User SG, 1985) and endorsed as the BCS model for user interface. The same endorsement was awarded to NOTIS-IR as a model for information storage and retrieval. the European Commission published in 1985 NOTIS-IR as reference model for document and information search and retrieval.",2.0,Who are some important British Information Retrieval researchers?,"[{""answer_confidence"":{""high"":False,""low"":False,""medium"":False,""very_high"":True,""very_low"":False},""passage-snippets-annotation-qualification-task"":{""entities"":[{""endOffset"":92,""label"":""relevant-text-span"",""startOffset"":78},{""endOffset"":105,""label"":""relevant-text-span"",""startOffset"":93}]}}]",x,,"[' Jeremy Salter', ' Roger Tagg ']"
3B9J25CZ39SDE1QVO5H7G8BZB5ISCF,3I3MQQJ31WXKIN6QF6OQSRUDQS81J6,Identify relevant text spans in text passage,Identify all the text spans that contain key pieces of the answer to a given question,"text, text spans, relevance",$0.30,Fri May 19 03:52:59 PDT 2023,3,BatchId:5076839;OriginalHitTemplateId:929448123;,21600,259200,Mon May 22 03:52:59 PDT 2023,,,3TMFV4NEPDT7WZFLH0G50VES5R6W89,worker_119,Submitted,Fri May 19 11:14:28 PDT 2023,Fri May 19 11:28:19 PDT 2023,Mon May 22 11:28:19 PDT 2023,,,,831,100% (270/271),100% (179/179),100% (178/178),104_3,0,CAR_ba4619e372348ebeb40fc8b0e5d2a116c236cd7b,2,"Jack Mills (1918 – 9 July 2010) was a British librarian and classification researcher, who worked for more than sixty years in the study, teaching, development and promotion of library classification and information retrieval, principally as a major figure in the British school of facet analysis which builds on the traditions of Henry E. Bliss and S.R. Ranganathan.",0.0,Who are some important British Information Retrieval researchers?,"[{""answer_confidence"":{""high"":True,""low"":False,""medium"":False,""very_high"":False,""very_low"":False},""passage-snippets-annotation-qualification-task"":{""entities"":[{""endOffset"":10,""label"":""relevant-text-span"",""startOffset"":0},{""endOffset"":345,""label"":""relevant-text-span"",""startOffset"":331},{""endOffset"":366,""label"":""relevant-text-span"",""startOffset"":350}]}}]",x,,"['Jack Mills', 'Henry E. Bliss', 'S.R. Ranganathan']"
3B9J25CZ39SDE1QVO5H7G8BZB5ISCF,3I3MQQJ31WXKIN6QF6OQSRUDQS81J6,Identify relevant text spans in text passage,Identify all the text spans that contain key pieces of the answer to a given question,"text, text spans, relevance",$0.30,Fri May 19 03:52:59 PDT 2023,3,BatchId:5076839;OriginalHitTemplateId:929448123;,21600,259200,Mon May 22 03:52:59 PDT 2023,,,3IAS3U3I0KV4MHFL6SB83QO49R2B2D,worker_232,Submitted,Fri May 19 18:14:16 PDT 2023,Fri May 19 18:17:33 PDT 2023,Mon May 22 18:17:33 PDT 2023,,,,197,100% (208/208),100% (179/179),100% (178/178),104_3,0,CAR_ba4619e372348ebeb40fc8b0e5d2a116c236cd7b,2,"Jack Mills (1918 – 9 July 2010) was a British librarian and classification researcher, who worked for more than sixty years in the study, teaching, development and promotion of library classification and information retrieval, principally as a major figure in the British school of facet analysis which builds on the traditions of Henry E. Bliss and S.R. Ranganathan.",0.0,Who are some important British Information Retrieval researchers?,"[{""answer_confidence"":{""high"":False,""low"":False,""medium"":True,""very_high"":False,""very_low"":False},""passage-snippets-annotation-qualification-task"":{""entities"":[{""endOffset"":10,""label"":""relevant-text-span"",""startOffset"":0}]}}]",x,,['Jack Mills']
3B9J25CZ39SDE1QVO5H7G8BZB5ISCF,3I3MQQJ31WXKIN6QF6OQSRUDQS81J6,Identify relevant text spans in text passage,Identify all the text spans that contain key pieces of the answer to a given question,"text, text spans, relevance",$0.30,Fri May 19 03:52:59 PDT 2023,3,BatchId:5076839;OriginalHitTemplateId:929448123;,21600,259200,Mon May 22 03:52:59 PDT 2023,,,3EKVH9QME3JX1ED5KO2EYGC4CAKD2O,worker_249,Submitted,Fri May 19 08:19:35 PDT 2023,Fri May 19 08:20:01 PDT 2023,Mon May 22 08:20:01 PDT 2023,,,,26,100% (194/194),100% (191/191),100% (190/190),104_3,0,CAR_ba4619e372348ebeb40fc8b0e5d2a116c236cd7b,2,"Jack Mills (1918 – 9 July 2010) was a British librarian and classification researcher, who worked for more than sixty years in the study, teaching, development and promotion of library classification and information retrieval, principally as a major figure in the British school of facet analysis which builds on the traditions of Henry E. Bliss and S.R. Ranganathan.",0.0,Who are some important British Information Retrieval researchers?,"[{""answer_confidence"":{""high"":False,""low"":False,""medium"":False,""very_high"":True,""very_low"":False},""passage-snippets-annotation-qualification-task"":{""entities"":[{""endOffset"":10,""label"":""relevant-text-span"",""startOffset"":0},{""endOffset"":345,""label"":""relevant-text-span"",""startOffset"":331},{""endOffset"":366,""label"":""relevant-text-span"",""startOffset"":349}]}}]",x,,"['Jack Mills', 'Henry E. Bliss', ' S.R. Ranganathan']"
3B0MCRZMCV9ZVYJ71TZJ535VN4QPPA,3I3MQQJ31WXKIN6QF6OQSRUDQS81J6,Identify relevant text spans in text passage,Identify all the text spans that contain key pieces of the answer to a given question,"text, text spans, relevance",$0.30,Fri May 19 03:53:00 PDT 2023,3,BatchId:5076839;OriginalHitTemplateId:929448123;,21600,259200,Mon May 22 03:53:00 PDT 2023,,,3H7Z272LXCMCRX23Z3BED6S2WUKPLB,worker_119,Submitted,Fri May 19 14:29:46 PDT 2023,Fri May 19 14:30:12 PDT 2023,Mon May 22 14:30:12 PDT 2023,,,,26,100% (270/271),100% (179/179),100% (178/178),104_3,0,CAR_d0f46cdf42e889d0632bc58b5153909092841764,2,C. J. 'Keith' van Rijsbergen FREng (Cornelis Joost van Rijsbergen) (born 1943) is a professor of computer science and the leader of the Glasgow Information Retrieval Group based at the University of Glasgow. He is one of the founders of modern Information Retrieval and the author of the seminal monograph Information Retrieval and of the textbook The Geometry of Information Retrieval.,1.0,Who are some important British Information Retrieval researchers?,"[{""answer_confidence"":{""high"":False,""low"":False,""medium"":True,""very_high"":False,""very_low"":False},""passage-snippets-annotation-qualification-task"":{""entities"":[{""endOffset"":34,""label"":""relevant-text-span"",""startOffset"":0}]}}]",,The agreement rate not met. None of the text span chosen by you has been selected by other workers.,"[""C. J. 'Keith' van Rijsbergen FREng""]"
3B0MCRZMCV9ZVYJ71TZJ535VN4QPPA,3I3MQQJ31WXKIN6QF6OQSRUDQS81J6,Identify relevant text spans in text passage,Identify all the text spans that contain key pieces of the answer to a given question,"text, text spans, relevance",$0.30,Fri May 19 03:53:00 PDT 2023,3,BatchId:5076839;OriginalHitTemplateId:929448123;,21600,259200,Mon May 22 03:53:00 PDT 2023,,,3WEV0KO0OR7Q6DU3E3HKPYP8A6DSD4,worker_232,Submitted,Fri May 19 18:14:30 PDT 2023,Fri May 19 18:35:44 PDT 2023,Mon May 22 18:35:44 PDT 2023,,,,1274,100% (208/208),100% (179/179),100% (178/178),104_3,0,CAR_d0f46cdf42e889d0632bc58b5153909092841764,2,C. J. 'Keith' van Rijsbergen FREng (Cornelis Joost van Rijsbergen) (born 1943) is a professor of computer science and the leader of the Glasgow Information Retrieval Group based at the University of Glasgow. He is one of the founders of modern Information Retrieval and the author of the seminal monograph Information Retrieval and of the textbook The Geometry of Information Retrieval.,1.0,Who are some important British Information Retrieval researchers?,"[{""answer_confidence"":{""high"":True,""low"":False,""medium"":False,""very_high"":False,""very_low"":False},""passage-snippets-annotation-qualification-task"":{""entities"":[{""endOffset"":266,""label"":""relevant-text-span"",""startOffset"":0}]}}]",,Chosen text spans are too long (longer than half of the passage). The requirements say that text spans should be concise and as short as possible.,"[""C. J. 'Keith' van Rijsbergen FREng (Cornelis Joost van Rijsbergen) (born 1943) is a professor of computer science and the leader of the Glasgow Information Retrieval Group based at the University of Glasgow. He is one of the founders of modern Information Retrieval ""]"
3B0MCRZMCV9ZVYJ71TZJ535VN4QPPA,3I3MQQJ31WXKIN6QF6OQSRUDQS81J6,Identify relevant text spans in text passage,Identify all the text spans that contain key pieces of the answer to a given question,"text, text spans, relevance",$0.30,Fri May 19 03:53:00 PDT 2023,3,BatchId:5076839;OriginalHitTemplateId:929448123;,21600,259200,Mon May 22 03:53:00 PDT 2023,,,3K9FOBBF2MYCOCQQJOYM9HN7WEALN0,worker_249,Submitted,Fri May 19 07:54:11 PDT 2023,Fri May 19 07:54:46 PDT 2023,Mon May 22 07:54:46 PDT 2023,,,,35,100% (194/194),100% (191/191),100% (190/190),104_3,0,CAR_d0f46cdf42e889d0632bc58b5153909092841764,2,C. J. 'Keith' van Rijsbergen FREng (Cornelis Joost van Rijsbergen) (born 1943) is a professor of computer science and the leader of the Glasgow Information Retrieval Group based at the University of Glasgow. He is one of the founders of modern Information Retrieval and the author of the seminal monograph Information Retrieval and of the textbook The Geometry of Information Retrieval.,1.0,Who are some important British Information Retrieval researchers?,"[{""answer_confidence"":{""high"":False,""low"":False,""medium"":True,""very_high"":False,""very_low"":False},""passage-snippets-annotation-qualification-task"":{""entities"":[]}}]",,Chosen text spans are too long (longer than half of the passage). The requirements say that text spans should be concise and as short as possible.,[]
3QGTX7BCITHPGS3U4NO7YV2S63W5ZP,3I3MQQJ31WXKIN6QF6OQSRUDQS81J6,Identify relevant text spans in text passage,Identify all the text spans that contain key pieces of the answer to a given question,"text, text spans, relevance",$0.30,Fri May 19 03:53:00 PDT 2023,3,BatchId:5076839;OriginalHitTemplateId:929448123;,21600,259200,Mon May 22 03:53:00 PDT 2023,,,3WQ3B2KGEDVXXWXPNOBUY7EZHK1B17,worker_119,Submitted,Fri May 19 14:42:05 PDT 2023,Fri May 19 14:54:24 PDT 2023,Mon May 22 14:54:24 PDT 2023,,,,739,100% (270/271),100% (179/179),100% (178/178),104_4,0,CAR_2ba6e4254c1b2f9573e435b48e77fbcb530ccf50,0,"Information retrieval (IR) is the area of study concerned with searching for documents, for information within documents, and for metadata about documents, as well as that of searching structured storage, relational databases, and the World Wide Web. Automated information retrieval systems are used to reduce what has been called 'information overload'. Many universities and public libraries use IR systems to provide access to books, journals and other documents. Web search engines are the most visible IR applications.",0.0,What did Cyril Cleverdon's studies contribute to the evaluation of Information Retrieval systems?,"[{""answer_confidence"":{""high"":False,""low"":False,""medium"":True,""very_high"":False,""very_low"":False},""passage-snippets-annotation-qualification-task"":{""entities"":[]}}]",,Chosen text spans are too long (longer than half of the passage). The requirements say that text spans should be concise and as short as possible.,[]
3QGTX7BCITHPGS3U4NO7YV2S63W5ZP,3I3MQQJ31WXKIN6QF6OQSRUDQS81J6,Identify relevant text spans in text passage,Identify all the text spans that contain key pieces of the answer to a given question,"text, text spans, relevance",$0.30,Fri May 19 03:53:00 PDT 2023,3,BatchId:5076839;OriginalHitTemplateId:929448123;,21600,259200,Mon May 22 03:53:00 PDT 2023,,,3OUYGIZWRCDZU1DHAAPVFM0ETXZ0P5,worker_232,Submitted,Fri May 19 18:33:04 PDT 2023,Fri May 19 18:42:20 PDT 2023,Mon May 22 18:42:20 PDT 2023,,,,556,100% (208/208),100% (179/179),100% (178/178),104_4,0,CAR_2ba6e4254c1b2f9573e435b48e77fbcb530ccf50,0,"Information retrieval (IR) is the area of study concerned with searching for documents, for information within documents, and for metadata about documents, as well as that of searching structured storage, relational databases, and the World Wide Web. Automated information retrieval systems are used to reduce what has been called 'information overload'. Many universities and public libraries use IR systems to provide access to books, journals and other documents. Web search engines are the most visible IR applications.",0.0,What did Cyril Cleverdon's studies contribute to the evaluation of Information Retrieval systems?,"[{""answer_confidence"":{""high"":False,""low"":False,""medium"":True,""very_high"":False,""very_low"":False},""passage-snippets-annotation-qualification-task"":{""entities"":[]}}]",,Chosen text spans are too long (longer than half of the passage). The requirements say that text spans should be concise and as short as possible.,[]
3QGTX7BCITHPGS3U4NO7YV2S63W5ZP,3I3MQQJ31WXKIN6QF6OQSRUDQS81J6,Identify relevant text spans in text passage,Identify all the text spans that contain key pieces of the answer to a given question,"text, text spans, relevance",$0.30,Fri May 19 03:53:00 PDT 2023,3,BatchId:5076839;OriginalHitTemplateId:929448123;,21600,259200,Mon May 22 03:53:00 PDT 2023,,,3WS1NTTKE3RN3OONW7TQ3SMARPG0F2,worker_249,Submitted,Fri May 19 08:20:52 PDT 2023,Fri May 19 08:21:37 PDT 2023,Mon May 22 08:21:37 PDT 2023,,,,45,100% (194/194),100% (191/191),100% (190/190),104_4,0,CAR_2ba6e4254c1b2f9573e435b48e77fbcb530ccf50,0,"Information retrieval (IR) is the area of study concerned with searching for documents, for information within documents, and for metadata about documents, as well as that of searching structured storage, relational databases, and the World Wide Web. Automated information retrieval systems are used to reduce what has been called 'information overload'. Many universities and public libraries use IR systems to provide access to books, journals and other documents. Web search engines are the most visible IR applications.",0.0,What did Cyril Cleverdon's studies contribute to the evaluation of Information Retrieval systems?,"[{""answer_confidence"":{""high"":False,""low"":False,""medium"":False,""very_high"":True,""very_low"":False},""passage-snippets-annotation-qualification-task"":{""entities"":[]}}]",,Chosen text spans are too long (longer than half of the passage). The requirements say that text spans should be concise and as short as possible.,[]
362E9TQF3L511Z34LFJC2XQR0Z8GIL,3I3MQQJ31WXKIN6QF6OQSRUDQS81J6,Identify relevant text spans in text passage,Identify all the text spans that contain key pieces of the answer to a given question,"text, text spans, relevance",$0.30,Fri May 19 03:53:00 PDT 2023,3,BatchId:5076839;OriginalHitTemplateId:929448123;,21600,259200,Mon May 22 03:53:00 PDT 2023,,,378XPAWRUHS3ESOVPHW4VVBFG4RIAB,worker_119,Submitted,Fri May 19 14:41:57 PDT 2023,Fri May 19 14:44:26 PDT 2023,Mon May 22 14:44:26 PDT 2023,,,,149,100% (270/271),100% (179/179),100% (178/178),104_4,0,CAR_57d3ffe21cb7e7bc36695f05e6b102ce700c6864,2,"With the help of NSF funding, Cleverdon started a series of projects in 1957 that lasted for about 10 years in which he and his colleagues set the stage for information retrieval research. In the Cranfield project, retrieval experiments were conducted on test databases in a controlled, laboratory-like setting. The aim of the research was to improve the retrieval effectiveness of information retrieval systems, by developing better indexing languages and methods. The components of the experiments were:",,What did Cyril Cleverdon's studies contribute to the evaluation of Information Retrieval systems?,"[{""answer_confidence"":{""high"":False,""low"":False,""medium"":True,""very_high"":False,""very_low"":False},""passage-snippets-annotation-qualification-task"":{""entities"":[{""endOffset"":188,""label"":""relevant-text-span"",""startOffset"":30},{""endOffset"":465,""label"":""relevant-text-span"",""startOffset"":312}]}}]",x,,"['Cleverdon started a series of projects in 1957 that lasted for about 10 years in which he and his colleagues set the stage for information retrieval research.', 'The aim of the research was to improve the retrieval effectiveness of information retrieval systems, by developing better indexing languages and methods.']"
362E9TQF3L511Z34LFJC2XQR0Z8GIL,3I3MQQJ31WXKIN6QF6OQSRUDQS81J6,Identify relevant text spans in text passage,Identify all the text spans that contain key pieces of the answer to a given question,"text, text spans, relevance",$0.30,Fri May 19 03:53:00 PDT 2023,3,BatchId:5076839;OriginalHitTemplateId:929448123;,21600,259200,Mon May 22 03:53:00 PDT 2023,,,32ZKVD547K2T7ZB4ASPDM0TWF52B3O,worker_232,Submitted,Fri May 19 18:14:18 PDT 2023,Fri May 19 18:20:23 PDT 2023,Mon May 22 18:20:23 PDT 2023,,,,365,100% (208/208),100% (179/179),100% (178/178),104_4,0,CAR_57d3ffe21cb7e7bc36695f05e6b102ce700c6864,2,"With the help of NSF funding, Cleverdon started a series of projects in 1957 that lasted for about 10 years in which he and his colleagues set the stage for information retrieval research. In the Cranfield project, retrieval experiments were conducted on test databases in a controlled, laboratory-like setting. The aim of the research was to improve the retrieval effectiveness of information retrieval systems, by developing better indexing languages and methods. The components of the experiments were:",,What did Cyril Cleverdon's studies contribute to the evaluation of Information Retrieval systems?,"[{""answer_confidence"":{""high"":False,""low"":False,""medium"":True,""very_high"":False,""very_low"":False},""passage-snippets-annotation-qualification-task"":{""entities"":[{""endOffset"":465,""label"":""relevant-text-span"",""startOffset"":343}]}}]",x,,"['improve the retrieval effectiveness of information retrieval systems, by developing better indexing languages and methods.']"
362E9TQF3L511Z34LFJC2XQR0Z8GIL,3I3MQQJ31WXKIN6QF6OQSRUDQS81J6,Identify relevant text spans in text passage,Identify all the text spans that contain key pieces of the answer to a given question,"text, text spans, relevance",$0.30,Fri May 19 03:53:00 PDT 2023,3,BatchId:5076839;OriginalHitTemplateId:929448123;,21600,259200,Mon May 22 03:53:00 PDT 2023,,,3TXWC2NHN45E3NHM2NCI326IJJ59S7,worker_249,Submitted,Fri May 19 08:26:53 PDT 2023,Fri May 19 08:28:08 PDT 2023,Mon May 22 08:28:08 PDT 2023,,,,75,100% (194/194),100% (191/191),100% (190/190),104_4,0,CAR_57d3ffe21cb7e7bc36695f05e6b102ce700c6864,2,"With the help of NSF funding, Cleverdon started a series of projects in 1957 that lasted for about 10 years in which he and his colleagues set the stage for information retrieval research. In the Cranfield project, retrieval experiments were conducted on test databases in a controlled, laboratory-like setting. The aim of the research was to improve the retrieval effectiveness of information retrieval systems, by developing better indexing languages and methods. The components of the experiments were:",,What did Cyril Cleverdon's studies contribute to the evaluation of Information Retrieval systems?,"[{""answer_confidence"":{""high"":False,""low"":False,""medium"":False,""very_high"":True,""very_low"":False},""passage-snippets-annotation-qualification-task"":{""entities"":[{""endOffset"":465,""label"":""relevant-text-span"",""startOffset"":426}]}}]",x,,[' better indexing languages and methods.']
3VCK0Q0PP9TW4W3A4T71MDZSFHDN05,3I3MQQJ31WXKIN6QF6OQSRUDQS81J6,Identify relevant text spans in text passage,Identify all the text spans that contain key pieces of the answer to a given question,"text, text spans, relevance",$0.30,Fri May 19 03:53:00 PDT 2023,3,BatchId:5076839;OriginalHitTemplateId:929448123;,21600,259200,Mon May 22 03:53:00 PDT 2023,,,3VAR3R6G1UGZRQ6JQEPVJ1162OU8OU,worker_119,Submitted,Fri May 19 09:05:56 PDT 2023,Fri May 19 09:14:56 PDT 2023,Mon May 22 09:14:56 PDT 2023,,,,540,100% (270/271),100% (179/179),100% (178/178),104_4,0,CAR_5936a92f311a3d0c180c3161f5ab1494e3ad4eda,1,"The Cranfield experiments were computer information retrieval experiments conducted by Cyril W. Cleverdon at Cranfield University in the 1960s, to evaluate the efficiency of indexing systems.",,What did Cyril Cleverdon's studies contribute to the evaluation of Information Retrieval systems?,"[{""answer_confidence"":{""high"":False,""low"":True,""medium"":False,""very_high"":False,""very_low"":False},""passage-snippets-annotation-qualification-task"":{""entities"":[{""endOffset"":190,""label"":""relevant-text-span"",""startOffset"":144}]}}]",,The agreement rate not met. None of the text span chosen by you has been selected by other workers.,['to evaluate the efficiency of indexing systems']
3VCK0Q0PP9TW4W3A4T71MDZSFHDN05,3I3MQQJ31WXKIN6QF6OQSRUDQS81J6,Identify relevant text spans in text passage,Identify all the text spans that contain key pieces of the answer to a given question,"text, text spans, relevance",$0.30,Fri May 19 03:53:00 PDT 2023,3,BatchId:5076839;OriginalHitTemplateId:929448123;,21600,259200,Mon May 22 03:53:00 PDT 2023,,,38SKSKU7R6CKABFZM8LU49RQAHFILK,worker_232,Submitted,Fri May 19 18:45:09 PDT 2023,Fri May 19 18:48:59 PDT 2023,Mon May 22 18:48:59 PDT 2023,,,,230,100% (208/208),100% (179/179),100% (178/178),104_4,0,CAR_5936a92f311a3d0c180c3161f5ab1494e3ad4eda,1,"The Cranfield experiments were computer information retrieval experiments conducted by Cyril W. Cleverdon at Cranfield University in the 1960s, to evaluate the efficiency of indexing systems.",,What did Cyril Cleverdon's studies contribute to the evaluation of Information Retrieval systems?,"[{""answer_confidence"":{""high"":True,""low"":False,""medium"":False,""very_high"":False,""very_low"":False},""passage-snippets-annotation-qualification-task"":{""entities"":[{""endOffset"":191,""label"":""relevant-text-span"",""startOffset"":0}]}}]",,Chosen text spans are too long (longer than half of the passage). The requirements say that text spans should be concise and as short as possible.,"['The Cranfield experiments were computer information retrieval experiments conducted by Cyril W. Cleverdon at Cranfield University in the 1960s, to evaluate the efficiency of indexing systems.']"
3VCK0Q0PP9TW4W3A4T71MDZSFHDN05,3I3MQQJ31WXKIN6QF6OQSRUDQS81J6,Identify relevant text spans in text passage,Identify all the text spans that contain key pieces of the answer to a given question,"text, text spans, relevance",$0.30,Fri May 19 03:53:00 PDT 2023,3,BatchId:5076839;OriginalHitTemplateId:929448123;,21600,259200,Mon May 22 03:53:00 PDT 2023,,,3N4BPTXIOD7EZWSHVPY78C2MF9RKUF,worker_249,Approved,Fri May 19 05:13:37 PDT 2023,Fri May 19 05:15:02 PDT 2023,Mon May 22 05:15:02 PDT 2023,Mon May 22 05:15:19 PDT 2023,,,85,100% (194/194),100% (191/191),100% (190/190),104_4,0,CAR_5936a92f311a3d0c180c3161f5ab1494e3ad4eda,1,"The Cranfield experiments were computer information retrieval experiments conducted by Cyril W. Cleverdon at Cranfield University in the 1960s, to evaluate the efficiency of indexing systems.",,What did Cyril Cleverdon's studies contribute to the evaluation of Information Retrieval systems?,"[{""answer_confidence"":{""high"":True,""low"":False,""medium"":False,""very_high"":False,""very_low"":False},""passage-snippets-annotation-qualification-task"":{""entities"":[]}}]",,Chosen text spans are too long (longer than half of the passage). The requirements say that text spans should be concise and as short as possible.,[]
3HEM8MA6IDRKVAKM16E858V5S0CPQ1,3I3MQQJ31WXKIN6QF6OQSRUDQS81J6,Identify relevant text spans in text passage,Identify all the text spans that contain key pieces of the answer to a given question,"text, text spans, relevance",$0.30,Fri May 19 03:53:00 PDT 2023,3,BatchId:5076839;OriginalHitTemplateId:929448123;,21600,259200,Mon May 22 03:53:00 PDT 2023,,,36AHBNMV1WRN2Z6CD9997KD9RPPYD0,worker_119,Submitted,Fri May 19 09:05:51 PDT 2023,Fri May 19 09:11:52 PDT 2023,Mon May 22 09:11:52 PDT 2023,,,,361,100% (270/271),100% (179/179),100% (178/178),104_4,0,CAR_e073cdcae6993f6334946382ef0cfba7a23b31ce,4,"Not only did Cleverdon's Cranfield studies introduce experimental research into computer science, the outcomes of the project also established the basis of the automatic indexing as done in today's search engines. Essentially, Cleverdon found that the use of single terms from the documents achieved the best retrieval performance, as opposed to manually assigned thesaurus terms, synonyms, etc. These results were very controversial at the time. In the Cranfield 2 Report, Cleverdon said:",,What did Cyril Cleverdon's studies contribute to the evaluation of Information Retrieval systems?,"[{""answer_confidence"":{""high"":False,""low"":False,""medium"":True,""very_high"":False,""very_low"":False},""passage-snippets-annotation-qualification-task"":{""entities"":[{""endOffset"":213,""label"":""relevant-text-span"",""startOffset"":98},{""endOffset"":330,""label"":""relevant-text-span"",""startOffset"":227}]}}]",x,,"[""the outcomes of the project also established the basis of the automatic indexing as done in today's search engines."", 'Cleverdon found that the use of single terms from the documents achieved the best retrieval performance']"
3HEM8MA6IDRKVAKM16E858V5S0CPQ1,3I3MQQJ31WXKIN6QF6OQSRUDQS81J6,Identify relevant text spans in text passage,Identify all the text spans that contain key pieces of the answer to a given question,"text, text spans, relevance",$0.30,Fri May 19 03:53:00 PDT 2023,3,BatchId:5076839;OriginalHitTemplateId:929448123;,21600,259200,Mon May 22 03:53:00 PDT 2023,,,3CN4LGXD523A23NJKY8G4GJ3EEIY4D,worker_232,Submitted,Fri May 19 18:45:11 PDT 2023,Fri May 19 18:50:25 PDT 2023,Mon May 22 18:50:25 PDT 2023,,,,314,100% (208/208),100% (179/179),100% (178/178),104_4,0,CAR_e073cdcae6993f6334946382ef0cfba7a23b31ce,4,"Not only did Cleverdon's Cranfield studies introduce experimental research into computer science, the outcomes of the project also established the basis of the automatic indexing as done in today's search engines. Essentially, Cleverdon found that the use of single terms from the documents achieved the best retrieval performance, as opposed to manually assigned thesaurus terms, synonyms, etc. These results were very controversial at the time. In the Cranfield 2 Report, Cleverdon said:",,What did Cyril Cleverdon's studies contribute to the evaluation of Information Retrieval systems?,"[{""answer_confidence"":{""high"":True,""low"":False,""medium"":False,""very_high"":False,""very_low"":False},""passage-snippets-annotation-qualification-task"":{""entities"":[{""endOffset"":214,""label"":""relevant-text-span"",""startOffset"":0},{""endOffset"":394,""label"":""relevant-text-span"",""startOffset"":227}]}}]",x,,"[""Not only did Cleverdon's Cranfield studies introduce experimental research into computer science, the outcomes of the project also established the basis of the automatic indexing as done in today's search engines. "", 'Cleverdon found that the use of single terms from the documents achieved the best retrieval performance, as opposed to manually assigned thesaurus terms, synonyms, etc']"
3HEM8MA6IDRKVAKM16E858V5S0CPQ1,3I3MQQJ31WXKIN6QF6OQSRUDQS81J6,Identify relevant text spans in text passage,Identify all the text spans that contain key pieces of the answer to a given question,"text, text spans, relevance",$0.30,Fri May 19 03:53:00 PDT 2023,3,BatchId:5076839;OriginalHitTemplateId:929448123;,21600,259200,Mon May 22 03:53:00 PDT 2023,,,39ZSFO5CADBJOCMZD4YV7X290LZJUP,worker_249,Submitted,Fri May 19 08:06:53 PDT 2023,Fri May 19 08:08:22 PDT 2023,Mon May 22 08:08:22 PDT 2023,,,,89,100% (194/194),100% (191/191),100% (190/190),104_4,0,CAR_e073cdcae6993f6334946382ef0cfba7a23b31ce,4,"Not only did Cleverdon's Cranfield studies introduce experimental research into computer science, the outcomes of the project also established the basis of the automatic indexing as done in today's search engines. Essentially, Cleverdon found that the use of single terms from the documents achieved the best retrieval performance, as opposed to manually assigned thesaurus terms, synonyms, etc. These results were very controversial at the time. In the Cranfield 2 Report, Cleverdon said:",,What did Cyril Cleverdon's studies contribute to the evaluation of Information Retrieval systems?,"[{""answer_confidence"":{""high"":False,""low"":False,""medium"":False,""very_high"":True,""very_low"":False},""passage-snippets-annotation-qualification-task"":{""entities"":[{""endOffset"":96,""label"":""relevant-text-span"",""startOffset"":42},{""endOffset"":178,""label"":""relevant-text-span"",""startOffset"":131},{""endOffset"":330,""label"":""relevant-text-span"",""startOffset"":237}]}}]",x,,"[' introduce experimental research into computer science', 'established the basis of the automatic indexing', 'found that the use of single terms from the documents achieved the best retrieval performance']"
3SBNLSTU7YKKK5INJT6QW7QKJKUDZ5,3I3MQQJ31WXKIN6QF6OQSRUDQS81J6,Identify relevant text spans in text passage,Identify all the text spans that contain key pieces of the answer to a given question,"text, text spans, relevance",$0.30,Fri May 19 03:53:00 PDT 2023,3,BatchId:5076839;OriginalHitTemplateId:929448123;,21600,259200,Mon May 22 03:53:00 PDT 2023,,,34Z02EIMIXSXMT6ORRYFKW1BIZVT0C,worker_119,Submitted,Fri May 19 14:42:04 PDT 2023,Fri May 19 14:53:59 PDT 2023,Mon May 22 14:53:59 PDT 2023,,,,715,100% (270/271),100% (179/179),100% (178/178),104_4,0,CAR_f33ab18d9f433da1dd64162661d0a4a86b1018b7,2,"Together, these components form an information retrieval test collection. The test collection serves as a standard for testing retrieval approaches, and the success of each approach is measured in terms of two measures: precision and recall. Test collections and evaluation measures based on precision and recall are driving forces behind modern research on search systems. Cleverdon's approach formed a blueprint for the successful Text Retrieval Conference series that began in 1992.",,What did Cyril Cleverdon's studies contribute to the evaluation of Information Retrieval systems?,"[{""answer_confidence"":{""high"":False,""low"":False,""medium"":True,""very_high"":False,""very_low"":False},""passage-snippets-annotation-qualification-task"":{""entities"":[{""endOffset"":484,""label"":""relevant-text-span"",""startOffset"":374}]}}]",x,,"[""Cleverdon's approach formed a blueprint for the successful Text Retrieval Conference series that began in 1992""]"
3SBNLSTU7YKKK5INJT6QW7QKJKUDZ5,3I3MQQJ31WXKIN6QF6OQSRUDQS81J6,Identify relevant text spans in text passage,Identify all the text spans that contain key pieces of the answer to a given question,"text, text spans, relevance",$0.30,Fri May 19 03:53:00 PDT 2023,3,BatchId:5076839;OriginalHitTemplateId:929448123;,21600,259200,Mon May 22 03:53:00 PDT 2023,,,3MHW492WW5S92ZHKLRLSJBW9IO0VMV,worker_232,Submitted,Fri May 19 18:33:09 PDT 2023,Fri May 19 18:44:47 PDT 2023,Mon May 22 18:44:47 PDT 2023,,,,698,100% (208/208),100% (179/179),100% (178/178),104_4,0,CAR_f33ab18d9f433da1dd64162661d0a4a86b1018b7,2,"Together, these components form an information retrieval test collection. The test collection serves as a standard for testing retrieval approaches, and the success of each approach is measured in terms of two measures: precision and recall. Test collections and evaluation measures based on precision and recall are driving forces behind modern research on search systems. Cleverdon's approach formed a blueprint for the successful Text Retrieval Conference series that began in 1992.",,What did Cyril Cleverdon's studies contribute to the evaluation of Information Retrieval systems?,"[{""answer_confidence"":{""high"":False,""low"":True,""medium"":False,""very_high"":False,""very_low"":False},""passage-snippets-annotation-qualification-task"":{""entities"":[{""endOffset"":485,""label"":""relevant-text-span"",""startOffset"":374}]}}]",x,,"[""Cleverdon's approach formed a blueprint for the successful Text Retrieval Conference series that began in 1992.""]"
3SBNLSTU7YKKK5INJT6QW7QKJKUDZ5,3I3MQQJ31WXKIN6QF6OQSRUDQS81J6,Identify relevant text spans in text passage,Identify all the text spans that contain key pieces of the answer to a given question,"text, text spans, relevance",$0.30,Fri May 19 03:53:00 PDT 2023,3,BatchId:5076839;OriginalHitTemplateId:929448123;,21600,259200,Mon May 22 03:53:00 PDT 2023,,,3DPNQGW4LQU8JJDDEFDOLPZAF1I466,worker_249,Approved,Fri May 19 05:11:20 PDT 2023,Fri May 19 05:12:13 PDT 2023,Mon May 22 05:12:13 PDT 2023,Mon May 22 05:12:19 PDT 2023,,,53,100% (194/194),100% (191/191),100% (190/190),104_4,0,CAR_f33ab18d9f433da1dd64162661d0a4a86b1018b7,2,"Together, these components form an information retrieval test collection. The test collection serves as a standard for testing retrieval approaches, and the success of each approach is measured in terms of two measures: precision and recall. Test collections and evaluation measures based on precision and recall are driving forces behind modern research on search systems. Cleverdon's approach formed a blueprint for the successful Text Retrieval Conference series that began in 1992.",,What did Cyril Cleverdon's studies contribute to the evaluation of Information Retrieval systems?,"[{""answer_confidence"":{""high"":False,""low"":False,""medium"":False,""very_high"":True,""very_low"":False},""passage-snippets-annotation-qualification-task"":{""entities"":[{""endOffset"":484,""label"":""relevant-text-span"",""startOffset"":374}]}}]",x,,"[""Cleverdon's approach formed a blueprint for the successful Text Retrieval Conference series that began in 1992""]"
3UY4PIS8RV0WBLOS0CB10II2K9JN18,3I3MQQJ31WXKIN6QF6OQSRUDQS81J6,Identify relevant text spans in text passage,Identify all the text spans that contain key pieces of the answer to a given question,"text, text spans, relevance",$0.30,Fri May 19 03:53:00 PDT 2023,3,BatchId:5076839;OriginalHitTemplateId:929448123;,21600,259200,Mon May 22 03:53:00 PDT 2023,,,3ATPCQ38JDPP4S30TZI7HJ536NPAY9,worker_119,Submitted,Fri May 19 14:42:02 PDT 2023,Fri May 19 14:53:08 PDT 2023,Mon May 22 14:53:08 PDT 2023,,,,666,100% (270/271),100% (179/179),100% (178/178),104_6,0,CAR_9189669ee5be07de29e784c88974679e56d380f5,2,"Important in the IR-tradition have been, among others, the Cranfield experiments, which were founded in the 1950s, and the TREC experiments (Text Retrieval Conferences) starting in 1992. It was the Cranfield experiments, which introduced the famous measures “recall” and “precision” as evaluation criteria for systems efficiency. The Cranfield experiments found that classification systems like UDC and facet-analytic systems were less efficient compared to free-text searches or low level indexing systems (“UNITERM”). The Cranfield I test found according to Ellis (1996, 3-6) the following results.",,How did the Cranfield Experiments influence modern information retrieval evaluation initiatives?,"[{""answer_confidence"":{""high"":True,""low"":False,""medium"":False,""very_high"":False,""very_low"":False},""passage-snippets-annotation-qualification-task"":{""entities"":[{""endOffset"":329,""label"":""relevant-text-span"",""startOffset"":194},{""endOffset"":518,""label"":""relevant-text-span"",""startOffset"":331}]}}]",x,,"['the Cranfield experiments, which introduced the famous measures recall and precision as evaluation criteria for systems efficiency.', 'he Cranfield experiments found that classification systems like UDC and facet-analytic systems were less efficient compared to free-text searches or low level indexing systems (UNITERM)']"
3UY4PIS8RV0WBLOS0CB10II2K9JN18,3I3MQQJ31WXKIN6QF6OQSRUDQS81J6,Identify relevant text spans in text passage,Identify all the text spans that contain key pieces of the answer to a given question,"text, text spans, relevance",$0.30,Fri May 19 03:53:00 PDT 2023,3,BatchId:5076839;OriginalHitTemplateId:929448123;,21600,259200,Mon May 22 03:53:00 PDT 2023,,,3KJYX6QCMEQJ1RBZIU803J4SEU5VJR,worker_232,Submitted,Fri May 19 18:32:58 PDT 2023,Fri May 19 18:37:51 PDT 2023,Mon May 22 18:37:51 PDT 2023,,,,293,100% (208/208),100% (179/179),100% (178/178),104_6,0,CAR_9189669ee5be07de29e784c88974679e56d380f5,2,"Important in the IR-tradition have been, among others, the Cranfield experiments, which were founded in the 1950s, and the TREC experiments (Text Retrieval Conferences) starting in 1992. It was the Cranfield experiments, which introduced the famous measures “recall” and “precision” as evaluation criteria for systems efficiency. The Cranfield experiments found that classification systems like UDC and facet-analytic systems were less efficient compared to free-text searches or low level indexing systems (“UNITERM”). The Cranfield I test found according to Ellis (1996, 3-6) the following results.",,How did the Cranfield Experiments influence modern information retrieval evaluation initiatives?,"[{""answer_confidence"":{""high"":True,""low"":False,""medium"":False,""very_high"":False,""very_low"":False},""passage-snippets-annotation-qualification-task"":{""entities"":[{""endOffset"":329,""label"":""relevant-text-span"",""startOffset"":187},{""endOffset"":519,""label"":""relevant-text-span"",""startOffset"":330}]}}]",x,,"['It was the Cranfield experiments, which introduced the famous measures recall and precision as evaluation criteria for systems efficiency.', 'The Cranfield experiments found that classification systems like UDC and facet-analytic systems were less efficient compared to free-text searches or low level indexing systems (UNITERM).']"
3UY4PIS8RV0WBLOS0CB10II2K9JN18,3I3MQQJ31WXKIN6QF6OQSRUDQS81J6,Identify relevant text spans in text passage,Identify all the text spans that contain key pieces of the answer to a given question,"text, text spans, relevance",$0.30,Fri May 19 03:53:00 PDT 2023,3,BatchId:5076839;OriginalHitTemplateId:929448123;,21600,259200,Mon May 22 03:53:00 PDT 2023,,,3P1L2B7AD64U6XQ2K3YZO6NCMN4LO2,worker_249,Approved,Fri May 19 04:52:21 PDT 2023,Fri May 19 04:54:14 PDT 2023,Mon May 22 04:54:14 PDT 2023,Mon May 22 04:54:19 PDT 2023,,,113,100% (194/194),100% (191/191),100% (190/190),104_6,0,CAR_9189669ee5be07de29e784c88974679e56d380f5,2,"Important in the IR-tradition have been, among others, the Cranfield experiments, which were founded in the 1950s, and the TREC experiments (Text Retrieval Conferences) starting in 1992. It was the Cranfield experiments, which introduced the famous measures “recall” and “precision” as evaluation criteria for systems efficiency. The Cranfield experiments found that classification systems like UDC and facet-analytic systems were less efficient compared to free-text searches or low level indexing systems (“UNITERM”). The Cranfield I test found according to Ellis (1996, 3-6) the following results.",,How did the Cranfield Experiments influence modern information retrieval evaluation initiatives?,"[{""answer_confidence"":{""high"":False,""low"":False,""medium"":False,""very_high"":True,""very_low"":False},""passage-snippets-annotation-qualification-task"":{""entities"":[{""endOffset"":328,""label"":""relevant-text-span"",""startOffset"":226},{""endOffset"":518,""label"":""relevant-text-span"",""startOffset"":356}]}}]",x,,"[' introduced the famous measures recall and precision as evaluation criteria for systems efficiency', 'found that classification systems like UDC and facet-analytic systems were less efficient compared to free-text searches or low level indexing systems (UNITERM)']"
3GITHABAD203066OJGBF51S7CFXN2P,3I3MQQJ31WXKIN6QF6OQSRUDQS81J6,Identify relevant text spans in text passage,Identify all the text spans that contain key pieces of the answer to a given question,"text, text spans, relevance",$0.30,Fri May 19 03:53:00 PDT 2023,3,BatchId:5076839;OriginalHitTemplateId:929448123;,21600,259200,Mon May 22 03:53:00 PDT 2023,,,3MTMREQS40XLFN80EY0JWMKNVESWAX,worker_119,Submitted,Fri May 19 09:05:53 PDT 2023,Fri May 19 09:13:04 PDT 2023,Mon May 22 09:13:04 PDT 2023,,,,431,100% (270/271),100% (179/179),100% (178/178),104_6,0,CAR_99043d06b1bfa2a76bde2726e9edf7765fd3b72b,2,"The information retrieval community has emphasized the use of test collections and benchmark tasks to measure topical relevance, starting with the Cranfield Experiments of the early 1960s and culminating in the TREC evaluations that continue to this day as the main evaluation framework for information retrieval research.",,How did the Cranfield Experiments influence modern information retrieval evaluation initiatives?,"[{""answer_confidence"":{""high"":False,""low"":False,""medium"":True,""very_high"":False,""very_low"":False},""passage-snippets-annotation-qualification-task"":{""entities"":[{""endOffset"":168,""label"":""relevant-text-span"",""startOffset"":0}]}}]",,Chosen text spans are too long (longer than half of the passage). The requirements say that text spans should be concise and as short as possible.,"['The information retrieval community has emphasized the use of test collections and benchmark tasks to measure topical relevance, starting with the Cranfield Experiments']"
3GITHABAD203066OJGBF51S7CFXN2P,3I3MQQJ31WXKIN6QF6OQSRUDQS81J6,Identify relevant text spans in text passage,Identify all the text spans that contain key pieces of the answer to a given question,"text, text spans, relevance",$0.30,Fri May 19 03:53:00 PDT 2023,3,BatchId:5076839;OriginalHitTemplateId:929448123;,21600,259200,Mon May 22 03:53:00 PDT 2023,,,35USIKEBNWVU5RNJWKSKKY5A6LIN6V,worker_232,Submitted,Fri May 19 18:14:19 PDT 2023,Fri May 19 18:20:43 PDT 2023,Mon May 22 18:20:43 PDT 2023,,,,384,100% (208/208),100% (179/179),100% (178/178),104_6,0,CAR_99043d06b1bfa2a76bde2726e9edf7765fd3b72b,2,"The information retrieval community has emphasized the use of test collections and benchmark tasks to measure topical relevance, starting with the Cranfield Experiments of the early 1960s and culminating in the TREC evaluations that continue to this day as the main evaluation framework for information retrieval research.",,How did the Cranfield Experiments influence modern information retrieval evaluation initiatives?,"[{""answer_confidence"":{""high"":True,""low"":False,""medium"":False,""very_high"":False,""very_low"":False},""passage-snippets-annotation-qualification-task"":{""entities"":[{""endOffset"":168,""label"":""relevant-text-span"",""startOffset"":0}]}}]",,Chosen text spans are too long (longer than half of the passage). The requirements say that text spans should be concise and as short as possible.,"['The information retrieval community has emphasized the use of test collections and benchmark tasks to measure topical relevance, starting with the Cranfield Experiments']"
3GITHABAD203066OJGBF51S7CFXN2P,3I3MQQJ31WXKIN6QF6OQSRUDQS81J6,Identify relevant text spans in text passage,Identify all the text spans that contain key pieces of the answer to a given question,"text, text spans, relevance",$0.30,Fri May 19 03:53:00 PDT 2023,3,BatchId:5076839;OriginalHitTemplateId:929448123;,21600,259200,Mon May 22 03:53:00 PDT 2023,,,3GLB5JMZF2ANG8LCJ3N30K65MTMGD7,worker_249,Submitted,Fri May 19 07:54:49 PDT 2023,Fri May 19 07:58:19 PDT 2023,Mon May 22 07:58:19 PDT 2023,,,,210,100% (194/194),100% (191/191),100% (190/190),104_6,0,CAR_99043d06b1bfa2a76bde2726e9edf7765fd3b72b,2,"The information retrieval community has emphasized the use of test collections and benchmark tasks to measure topical relevance, starting with the Cranfield Experiments of the early 1960s and culminating in the TREC evaluations that continue to this day as the main evaluation framework for information retrieval research.",,How did the Cranfield Experiments influence modern information retrieval evaluation initiatives?,"[{""answer_confidence"":{""high"":False,""low"":True,""medium"":False,""very_high"":False,""very_low"":False},""passage-snippets-annotation-qualification-task"":{""entities"":[{""endOffset"":322,""label"":""relevant-text-span"",""startOffset"":0}]}}]",,Chosen text spans are too long (longer than half of the passage). The requirements say that text spans should be concise and as short as possible.,"['The information retrieval community has emphasized the use of test collections and benchmark tasks to measure topical relevance, starting with the Cranfield Experiments of the early 1960s and culminating in the TREC evaluations that continue to this day as the main evaluation framework for information retrieval research.']"
3BVS8WK9R4A5AIVT5TRY1MZUKMGBIZ,3I3MQQJ31WXKIN6QF6OQSRUDQS81J6,Identify relevant text spans in text passage,Identify all the text spans that contain key pieces of the answer to a given question,"text, text spans, relevance",$0.30,Fri May 19 03:53:00 PDT 2023,3,BatchId:5076839;OriginalHitTemplateId:929448123;,21600,259200,Mon May 22 03:53:00 PDT 2023,,,3RSDURM96F1SU5KC94KTD2VUR1QYET,worker_119,Submitted,Fri May 19 09:05:52 PDT 2023,Fri May 19 09:12:27 PDT 2023,Mon May 22 09:12:27 PDT 2023,,,,395,100% (270/271),100% (179/179),100% (178/178),104_6,0,CAR_e073cdcae6993f6334946382ef0cfba7a23b31ce,3,"Not only did Cleverdon's Cranfield studies introduce experimental research into computer science, the outcomes of the project also established the basis of the automatic indexing as done in today's search engines. Essentially, Cleverdon found that the use of single terms from the documents achieved the best retrieval performance, as opposed to manually assigned thesaurus terms, synonyms, etc. These results were very controversial at the time. In the Cranfield 2 Report, Cleverdon said:",,How did the Cranfield Experiments influence modern information retrieval evaluation initiatives?,"[{""answer_confidence"":{""high"":False,""low"":True,""medium"":False,""very_high"":False,""very_low"":False},""passage-snippets-annotation-qualification-task"":{""entities"":[{""endOffset"":212,""label"":""relevant-text-span"",""startOffset"":98},{""endOffset"":330,""label"":""relevant-text-span"",""startOffset"":227}]}}]",x,,"[""the outcomes of the project also established the basis of the automatic indexing as done in today's search engines"", 'Cleverdon found that the use of single terms from the documents achieved the best retrieval performance']"
3BVS8WK9R4A5AIVT5TRY1MZUKMGBIZ,3I3MQQJ31WXKIN6QF6OQSRUDQS81J6,Identify relevant text spans in text passage,Identify all the text spans that contain key pieces of the answer to a given question,"text, text spans, relevance",$0.30,Fri May 19 03:53:00 PDT 2023,3,BatchId:5076839;OriginalHitTemplateId:929448123;,21600,259200,Mon May 22 03:53:00 PDT 2023,,,31IBVUNM9XE3WPP3Z4HFOLC0LNPFV9,worker_232,Submitted,Fri May 19 18:45:16 PDT 2023,Fri May 19 18:55:19 PDT 2023,Mon May 22 18:55:19 PDT 2023,,,,603,100% (208/208),100% (179/179),100% (178/178),104_6,0,CAR_e073cdcae6993f6334946382ef0cfba7a23b31ce,3,"Not only did Cleverdon's Cranfield studies introduce experimental research into computer science, the outcomes of the project also established the basis of the automatic indexing as done in today's search engines. Essentially, Cleverdon found that the use of single terms from the documents achieved the best retrieval performance, as opposed to manually assigned thesaurus terms, synonyms, etc. These results were very controversial at the time. In the Cranfield 2 Report, Cleverdon said:",,How did the Cranfield Experiments influence modern information retrieval evaluation initiatives?,"[{""answer_confidence"":{""high"":True,""low"":False,""medium"":False,""very_high"":False,""very_low"":False},""passage-snippets-annotation-qualification-task"":{""entities"":[{""endOffset"":213,""label"":""relevant-text-span"",""startOffset"":0}]}}]",x,,"[""Not only did Cleverdon's Cranfield studies introduce experimental research into computer science, the outcomes of the project also established the basis of the automatic indexing as done in today's search engines.""]"
3BVS8WK9R4A5AIVT5TRY1MZUKMGBIZ,3I3MQQJ31WXKIN6QF6OQSRUDQS81J6,Identify relevant text spans in text passage,Identify all the text spans that contain key pieces of the answer to a given question,"text, text spans, relevance",$0.30,Fri May 19 03:53:00 PDT 2023,3,BatchId:5076839;OriginalHitTemplateId:929448123;,21600,259200,Mon May 22 03:53:00 PDT 2023,,,3R0T90IZ1XRDBGFYJ7SSGEON1T1CGU,worker_249,Submitted,Fri May 19 07:04:14 PDT 2023,Fri May 19 07:07:06 PDT 2023,Mon May 22 07:07:06 PDT 2023,,,,172,100% (194/194),100% (191/191),100% (190/190),104_6,0,CAR_e073cdcae6993f6334946382ef0cfba7a23b31ce,3,"Not only did Cleverdon's Cranfield studies introduce experimental research into computer science, the outcomes of the project also established the basis of the automatic indexing as done in today's search engines. Essentially, Cleverdon found that the use of single terms from the documents achieved the best retrieval performance, as opposed to manually assigned thesaurus terms, synonyms, etc. These results were very controversial at the time. In the Cranfield 2 Report, Cleverdon said:",,How did the Cranfield Experiments influence modern information retrieval evaluation initiatives?,"[{""answer_confidence"":{""high"":False,""low"":False,""medium"":False,""very_high"":True,""very_low"":False},""passage-snippets-annotation-qualification-task"":{""entities"":[{""endOffset"":96,""label"":""relevant-text-span"",""startOffset"":43},{""endOffset"":179,""label"":""relevant-text-span"",""startOffset"":130},{""endOffset"":330,""label"":""relevant-text-span"",""startOffset"":237}]}}]",x,,"['introduce experimental research into computer science', ' established the basis of the automatic indexing ', 'found that the use of single terms from the documents achieved the best retrieval performance']"
3LCXHSGDMXLSBZA9KWN26WB2WHVSE8,3I3MQQJ31WXKIN6QF6OQSRUDQS81J6,Identify relevant text spans in text passage,Identify all the text spans that contain key pieces of the answer to a given question,"text, text spans, relevance",$0.30,Fri May 19 03:53:00 PDT 2023,3,BatchId:5076839;OriginalHitTemplateId:929448123;,21600,259200,Mon May 22 03:53:00 PDT 2023,,,3KRVW3HTZS0TA70GRWGN8OC2JJHMSQ,worker_119,Submitted,Fri May 19 14:29:50 PDT 2023,Fri May 19 14:35:05 PDT 2023,Mon May 22 14:35:05 PDT 2023,,,,315,100% (270/271),100% (179/179),100% (178/178),104_6,0,CAR_f041a09c412533d214cc4756f05f05ed28f356c9,2,"In order to evaluate how well an information retrieval system retrieved topically relevant results, the relevance of retrieved results must be quantified. In Cranfield-style evaluations, this typically involves assigning a relevance level to each retrieved result, a process known as relevance assessment. Relevance levels can be binary (indicating a result is relevant or that it is not relevant), or graded (indicating results have a varying degree of match between the topic of the result and the information need).   Once relevance levels have been assigned to the retrieved results, information retrieval performance measures can be used to assess the quality of a retrieval system's output.",,How did the Cranfield Experiments influence modern information retrieval evaluation initiatives?,"[{""answer_confidence"":{""high"":False,""low"":True,""medium"":False,""very_high"":False,""very_low"":False},""passage-snippets-annotation-qualification-task"":{""entities"":[{""endOffset"":305,""label"":""relevant-text-span"",""startOffset"":155},{""endOffset"":519,""label"":""relevant-text-span"",""startOffset"":306}]}}]",x,,"['In Cranfield-style evaluations, this typically involves assigning a relevance level to each retrieved result, a process known as relevance assessment.', 'Relevance levels can be binary (indicating a result is relevant or that it is not relevant), or graded (indicating results have a varying degree of match between the topic of the result and the information need). ']"
3LCXHSGDMXLSBZA9KWN26WB2WHVSE8,3I3MQQJ31WXKIN6QF6OQSRUDQS81J6,Identify relevant text spans in text passage,Identify all the text spans that contain key pieces of the answer to a given question,"text, text spans, relevance",$0.30,Fri May 19 03:53:00 PDT 2023,3,BatchId:5076839;OriginalHitTemplateId:929448123;,21600,259200,Mon May 22 03:53:00 PDT 2023,,,35LDD5557FJVMON8MN7LH2LS2QTMK0,worker_232,Submitted,Fri May 19 18:32:59 PDT 2023,Fri May 19 18:38:19 PDT 2023,Mon May 22 18:38:19 PDT 2023,,,,320,100% (208/208),100% (179/179),100% (178/178),104_6,0,CAR_f041a09c412533d214cc4756f05f05ed28f356c9,2,"In order to evaluate how well an information retrieval system retrieved topically relevant results, the relevance of retrieved results must be quantified. In Cranfield-style evaluations, this typically involves assigning a relevance level to each retrieved result, a process known as relevance assessment. Relevance levels can be binary (indicating a result is relevant or that it is not relevant), or graded (indicating results have a varying degree of match between the topic of the result and the information need).   Once relevance levels have been assigned to the retrieved results, information retrieval performance measures can be used to assess the quality of a retrieval system's output.",,How did the Cranfield Experiments influence modern information retrieval evaluation initiatives?,"[{""answer_confidence"":{""high"":True,""low"":False,""medium"":False,""very_high"":False,""very_low"":False},""passage-snippets-annotation-qualification-task"":{""entities"":[{""endOffset"":306,""label"":""relevant-text-span"",""startOffset"":0}]}}]",x,,"['In order to evaluate how well an information retrieval system retrieved topically relevant results, the relevance of retrieved results must be quantified. In Cranfield-style evaluations, this typically involves assigning a relevance level to each retrieved result, a process known as relevance assessment. ']"
3LCXHSGDMXLSBZA9KWN26WB2WHVSE8,3I3MQQJ31WXKIN6QF6OQSRUDQS81J6,Identify relevant text spans in text passage,Identify all the text spans that contain key pieces of the answer to a given question,"text, text spans, relevance",$0.30,Fri May 19 03:53:00 PDT 2023,3,BatchId:5076839;OriginalHitTemplateId:929448123;,21600,259200,Mon May 22 03:53:00 PDT 2023,,,3C2NJ6JBKFW6NQ4I9IGHULOYQE92NI,worker_249,Submitted,Fri May 19 06:56:36 PDT 2023,Fri May 19 06:59:49 PDT 2023,Mon May 22 06:59:49 PDT 2023,,,,193,100% (194/194),100% (191/191),100% (190/190),104_6,0,CAR_f041a09c412533d214cc4756f05f05ed28f356c9,2,"In order to evaluate how well an information retrieval system retrieved topically relevant results, the relevance of retrieved results must be quantified. In Cranfield-style evaluations, this typically involves assigning a relevance level to each retrieved result, a process known as relevance assessment. Relevance levels can be binary (indicating a result is relevant or that it is not relevant), or graded (indicating results have a varying degree of match between the topic of the result and the information need).   Once relevance levels have been assigned to the retrieved results, information retrieval performance measures can be used to assess the quality of a retrieval system's output.",,How did the Cranfield Experiments influence modern information retrieval evaluation initiatives?,"[{""answer_confidence"":{""high"":True,""low"":False,""medium"":False,""very_high"":False,""very_low"":False},""passage-snippets-annotation-qualification-task"":{""entities"":[]}}]",,Chosen text spans are too long (longer than half of the passage). The requirements say that text spans should be concise and as short as possible.,[]
338GLSUI57QKHDA5F778OEW28IASFQ,3I3MQQJ31WXKIN6QF6OQSRUDQS81J6,Identify relevant text spans in text passage,Identify all the text spans that contain key pieces of the answer to a given question,"text, text spans, relevance",$0.30,Fri May 19 03:53:00 PDT 2023,3,BatchId:5076839;OriginalHitTemplateId:929448123;,21600,259200,Mon May 22 03:53:00 PDT 2023,,,3U088ZLJVP8PEAAIMW3Q7G8AMSH0WB,worker_119,Submitted,Fri May 19 09:05:34 PDT 2023,Fri May 19 09:09:47 PDT 2023,Mon May 22 09:09:47 PDT 2023,,,,253,100% (270/271),100% (179/179),100% (178/178),104_6,0,CAR_f33ab18d9f433da1dd64162661d0a4a86b1018b7,1,"Together, these components form an information retrieval test collection. The test collection serves as a standard for testing retrieval approaches, and the success of each approach is measured in terms of two measures: precision and recall. Test collections and evaluation measures based on precision and recall are driving forces behind modern research on search systems. Cleverdon's approach formed a blueprint for the successful Text Retrieval Conference series that began in 1992.",0.0,How did the Cranfield Experiments influence modern information retrieval evaluation initiatives?,"[{""answer_confidence"":{""high"":False,""low"":False,""medium"":False,""very_high"":False,""very_low"":True},""passage-snippets-annotation-qualification-task"":{""entities"":[]}}]",,Chosen text spans are too long (longer than half of the passage). The requirements say that text spans should be concise and as short as possible.,[]
338GLSUI57QKHDA5F778OEW28IASFQ,3I3MQQJ31WXKIN6QF6OQSRUDQS81J6,Identify relevant text spans in text passage,Identify all the text spans that contain key pieces of the answer to a given question,"text, text spans, relevance",$0.30,Fri May 19 03:53:00 PDT 2023,3,BatchId:5076839;OriginalHitTemplateId:929448123;,21600,259200,Mon May 22 03:53:00 PDT 2023,,,3JBT3HLQFDHWWMJ7E42NLL30B4GPZK,worker_232,Submitted,Fri May 19 18:14:26 PDT 2023,Fri May 19 18:29:35 PDT 2023,Mon May 22 18:29:35 PDT 2023,,,,909,100% (208/208),100% (179/179),100% (178/178),104_6,0,CAR_f33ab18d9f433da1dd64162661d0a4a86b1018b7,1,"Together, these components form an information retrieval test collection. The test collection serves as a standard for testing retrieval approaches, and the success of each approach is measured in terms of two measures: precision and recall. Test collections and evaluation measures based on precision and recall are driving forces behind modern research on search systems. Cleverdon's approach formed a blueprint for the successful Text Retrieval Conference series that began in 1992.",0.0,How did the Cranfield Experiments influence modern information retrieval evaluation initiatives?,"[{""answer_confidence"":{""high"":False,""low"":False,""medium"":True,""very_high"":False,""very_low"":False},""passage-snippets-annotation-qualification-task"":{""entities"":[]}}]",,Chosen text spans are too long (longer than half of the passage). The requirements say that text spans should be concise and as short as possible.,[]
338GLSUI57QKHDA5F778OEW28IASFQ,3I3MQQJ31WXKIN6QF6OQSRUDQS81J6,Identify relevant text spans in text passage,Identify all the text spans that contain key pieces of the answer to a given question,"text, text spans, relevance",$0.30,Fri May 19 03:53:00 PDT 2023,3,BatchId:5076839;OriginalHitTemplateId:929448123;,21600,259200,Mon May 22 03:53:00 PDT 2023,,,3FE7TXL1LN2RQNHAPA8YYFJ3VPWQ2T,worker_249,Submitted,Fri May 19 06:59:53 PDT 2023,Fri May 19 07:04:11 PDT 2023,Mon May 22 07:04:11 PDT 2023,,,,258,100% (194/194),100% (191/191),100% (190/190),104_6,0,CAR_f33ab18d9f433da1dd64162661d0a4a86b1018b7,1,"Together, these components form an information retrieval test collection. The test collection serves as a standard for testing retrieval approaches, and the success of each approach is measured in terms of two measures: precision and recall. Test collections and evaluation measures based on precision and recall are driving forces behind modern research on search systems. Cleverdon's approach formed a blueprint for the successful Text Retrieval Conference series that began in 1992.",0.0,How did the Cranfield Experiments influence modern information retrieval evaluation initiatives?,"[{""answer_confidence"":{""high"":False,""low"":False,""medium"":False,""very_high"":True,""very_low"":False},""passage-snippets-annotation-qualification-task"":{""entities"":[{""endOffset"":372,""label"":""relevant-text-span"",""startOffset"":242},{""endOffset"":485,""label"":""relevant-text-span"",""startOffset"":395}]}}]",,The agreement rate not met. None of the text span chosen by you has been selected by other workers.,"['Test collections and evaluation measures based on precision and recall are driving forces behind modern research on search systems', 'formed a blueprint for the successful Text Retrieval Conference series that began in 1992.']"
3ZRKL6Z1FCIQVD60QRDQ6ZWE9XASGP,3I3MQQJ31WXKIN6QF6OQSRUDQS81J6,Identify relevant text spans in text passage,Identify all the text spans that contain key pieces of the answer to a given question,"text, text spans, relevance",$0.30,Fri May 19 03:53:00 PDT 2023,3,BatchId:5076839;OriginalHitTemplateId:929448123;,21600,259200,Mon May 22 03:53:00 PDT 2023,,,37TRT2X24V64SDDTDDH027ALB1IJB0,worker_119,Submitted,Fri May 19 11:14:29 PDT 2023,Fri May 19 11:27:56 PDT 2023,Mon May 22 11:27:56 PDT 2023,,,,807,100% (270/271),100% (179/179),100% (178/178),104_7,0,CAR_14ad79757fc328eef8c44b779c7751565428000a,3,"The test collections developed at TREC are useful not just for (potentially) helping researchers advance the state of the art, but also for allowing developers of new (commercial) retrieval products to evaluate their effectiveness on standard tests.  In the past decade, TREC has created new tests for enterprise e-mail search, genomics search, spam filtering, e-Discovery, and several other retrieval domains.",,What is TREC for information retrieval research?,"[{""answer_confidence"":{""high"":False,""low"":False,""medium"":True,""very_high"":False,""very_low"":False},""passage-snippets-annotation-qualification-task"":{""entities"":[{""endOffset"":249,""label"":""relevant-text-span"",""startOffset"":0},{""endOffset"":410,""label"":""relevant-text-span"",""startOffset"":271}]}}]",x,,"['The test collections developed at TREC are useful not just for (potentially) helping researchers advance the state of the art, but also for allowing developers of new (commercial) retrieval products to evaluate their effectiveness on standard tests.', 'TREC has created new tests for enterprise e-mail search, genomics search, spam filtering, e-Discovery, and several other retrieval domains.']"
3ZRKL6Z1FCIQVD60QRDQ6ZWE9XASGP,3I3MQQJ31WXKIN6QF6OQSRUDQS81J6,Identify relevant text spans in text passage,Identify all the text spans that contain key pieces of the answer to a given question,"text, text spans, relevance",$0.30,Fri May 19 03:53:00 PDT 2023,3,BatchId:5076839;OriginalHitTemplateId:929448123;,21600,259200,Mon May 22 03:53:00 PDT 2023,,,3MD9PLUKKNTWT17UYPPVHWY5BPHNZO,worker_232,Submitted,Fri May 19 18:45:12 PDT 2023,Fri May 19 18:50:39 PDT 2023,Mon May 22 18:50:39 PDT 2023,,,,327,100% (208/208),100% (179/179),100% (178/178),104_7,0,CAR_14ad79757fc328eef8c44b779c7751565428000a,3,"The test collections developed at TREC are useful not just for (potentially) helping researchers advance the state of the art, but also for allowing developers of new (commercial) retrieval products to evaluate their effectiveness on standard tests.  In the past decade, TREC has created new tests for enterprise e-mail search, genomics search, spam filtering, e-Discovery, and several other retrieval domains.",,What is TREC for information retrieval research?,"[{""answer_confidence"":{""high"":False,""low"":False,""medium"":True,""very_high"":False,""very_low"":False},""passage-snippets-annotation-qualification-task"":{""entities"":[{""endOffset"":410,""label"":""relevant-text-span"",""startOffset"":271}]}}]",x,,"['TREC has created new tests for enterprise e-mail search, genomics search, spam filtering, e-Discovery, and several other retrieval domains.']"
3ZRKL6Z1FCIQVD60QRDQ6ZWE9XASGP,3I3MQQJ31WXKIN6QF6OQSRUDQS81J6,Identify relevant text spans in text passage,Identify all the text spans that contain key pieces of the answer to a given question,"text, text spans, relevance",$0.30,Fri May 19 03:53:00 PDT 2023,3,BatchId:5076839;OriginalHitTemplateId:929448123;,21600,259200,Mon May 22 03:53:00 PDT 2023,,,3GM6G9ZBKSCUP7DVGWLDIT8OL19MTR,worker_249,Submitted,Fri May 19 06:31:02 PDT 2023,Fri May 19 06:32:45 PDT 2023,Mon May 22 06:32:45 PDT 2023,,,,103,100% (194/194),100% (191/191),100% (190/190),104_7,0,CAR_14ad79757fc328eef8c44b779c7751565428000a,3,"The test collections developed at TREC are useful not just for (potentially) helping researchers advance the state of the art, but also for allowing developers of new (commercial) retrieval products to evaluate their effectiveness on standard tests.  In the past decade, TREC has created new tests for enterprise e-mail search, genomics search, spam filtering, e-Discovery, and several other retrieval domains.",,What is TREC for information retrieval research?,"[{""answer_confidence"":{""high"":True,""low"":False,""medium"":False,""very_high"":False,""very_low"":False},""passage-snippets-annotation-qualification-task"":{""entities"":[]}}]",,Chosen text spans are too long (longer than half of the passage). The requirements say that text spans should be concise and as short as possible.,[]
362E9TQF3L511Z34LFJC2XQR0Z8IGN,3I3MQQJ31WXKIN6QF6OQSRUDQS81J6,Identify relevant text spans in text passage,Identify all the text spans that contain key pieces of the answer to a given question,"text, text spans, relevance",$0.30,Fri May 19 03:53:00 PDT 2023,3,BatchId:5076839;OriginalHitTemplateId:929448123;,21600,259200,Mon May 22 03:53:00 PDT 2023,,,3E4GGUZ1TD65FKJFC5O12T7UWJVK2M,worker_119,Submitted,Fri May 19 09:31:52 PDT 2023,Fri May 19 09:34:30 PDT 2023,Mon May 22 09:34:30 PDT 2023,,,,158,100% (270/271),100% (179/179),100% (178/178),104_7,0,CAR_bb61b1ea45856e8cbde9ca1799b3aa39cebceff0,3,"Formalized search engine evaluation has been ongoing for many years.  For example, the Text REtrieval Conference (TREC) was started in 1992 to support research within the information retrieval community by providing the infrastructure necessary for large-scale evaluation of text retrieval methodologies.  Most of today's commercial search engines include technology first developed in TREC.",,What is TREC for information retrieval research?,"[{""answer_confidence"":{""high"":False,""low"":False,""medium"":True,""very_high"":False,""very_low"":False},""passage-snippets-annotation-qualification-task"":{""entities"":[{""endOffset"":112,""label"":""relevant-text-span"",""startOffset"":83},{""endOffset"":303,""label"":""relevant-text-span"",""startOffset"":140}]}}]",,The agreement rate not met. None of the text span chosen by you has been selected by other workers.,"['the Text REtrieval Conference', 'to support research within the information retrieval community by providing the infrastructure necessary for large-scale evaluation of text retrieval methodologies']"
362E9TQF3L511Z34LFJC2XQR0Z8IGN,3I3MQQJ31WXKIN6QF6OQSRUDQS81J6,Identify relevant text spans in text passage,Identify all the text spans that contain key pieces of the answer to a given question,"text, text spans, relevance",$0.30,Fri May 19 03:53:00 PDT 2023,3,BatchId:5076839;OriginalHitTemplateId:929448123;,21600,259200,Mon May 22 03:53:00 PDT 2023,,,3RSDURM96F1SU5KC94KTD2VUR3AEYX,worker_232,Submitted,Fri May 19 18:14:27 PDT 2023,Fri May 19 18:32:35 PDT 2023,Mon May 22 18:32:35 PDT 2023,,,,1088,100% (208/208),100% (179/179),100% (178/178),104_7,0,CAR_bb61b1ea45856e8cbde9ca1799b3aa39cebceff0,3,"Formalized search engine evaluation has been ongoing for many years.  For example, the Text REtrieval Conference (TREC) was started in 1992 to support research within the information retrieval community by providing the infrastructure necessary for large-scale evaluation of text retrieval methodologies.  Most of today's commercial search engines include technology first developed in TREC.",,What is TREC for information retrieval research?,"[{""answer_confidence"":{""high"":True,""low"":False,""medium"":False,""very_high"":False,""very_low"":False},""passage-snippets-annotation-qualification-task"":{""entities"":[{""endOffset"":304,""label"":""relevant-text-span"",""startOffset"":87}]}}]",,Chosen text spans are too long (longer than half of the passage). The requirements say that text spans should be concise and as short as possible.,['Text REtrieval Conference (TREC) was started in 1992 to support research within the information retrieval community by providing the infrastructure necessary for large-scale evaluation of text retrieval methodologies.']
362E9TQF3L511Z34LFJC2XQR0Z8IGN,3I3MQQJ31WXKIN6QF6OQSRUDQS81J6,Identify relevant text spans in text passage,Identify all the text spans that contain key pieces of the answer to a given question,"text, text spans, relevance",$0.30,Fri May 19 03:53:00 PDT 2023,3,BatchId:5076839;OriginalHitTemplateId:929448123;,21600,259200,Mon May 22 03:53:00 PDT 2023,,,3JBT3HLQFDHWWMJ7E42NLL30B17ZPF,worker_249,Approved,Fri May 19 04:44:46 PDT 2023,Fri May 19 04:48:12 PDT 2023,Mon May 22 04:48:12 PDT 2023,Mon May 22 04:48:19 PDT 2023,,,206,100% (194/194),100% (191/191),100% (190/190),104_7,0,CAR_bb61b1ea45856e8cbde9ca1799b3aa39cebceff0,3,"Formalized search engine evaluation has been ongoing for many years.  For example, the Text REtrieval Conference (TREC) was started in 1992 to support research within the information retrieval community by providing the infrastructure necessary for large-scale evaluation of text retrieval methodologies.  Most of today's commercial search engines include technology first developed in TREC.",,What is TREC for information retrieval research?,"[{""answer_confidence"":{""high"":True,""low"":False,""medium"":False,""very_high"":False,""very_low"":False},""passage-snippets-annotation-qualification-task"":{""entities"":[{""endOffset"":303,""label"":""relevant-text-span"",""startOffset"":82}]}}]",,Chosen text spans are too long (longer than half of the passage). The requirements say that text spans should be concise and as short as possible.,[' the Text REtrieval Conference (TREC) was started in 1992 to support research within the information retrieval community by providing the infrastructure necessary for large-scale evaluation of text retrieval methodologies']
3ICOHX7EOGQSAQQXBU4IWQ5J1PCE07,3I3MQQJ31WXKIN6QF6OQSRUDQS81J6,Identify relevant text spans in text passage,Identify all the text spans that contain key pieces of the answer to a given question,"text, text spans, relevance",$0.30,Fri May 19 03:53:00 PDT 2023,3,BatchId:5076839;OriginalHitTemplateId:929448123;,21600,259200,Mon May 22 03:53:00 PDT 2023,,,3SKEMFQBZ8KG1ZL4SMX2CKKXIJK8KA,worker_119,Submitted,Fri May 19 09:05:36 PDT 2023,Fri May 19 09:08:05 PDT 2023,Mon May 22 09:08:05 PDT 2023,,,,149,100% (270/271),100% (179/179),100% (178/178),104_7,0,CAR_e8c23427635f085ca5a8a449bb69316c9897d948,3,"In 1992, the US Department of Defense along with the National Institute of Standards and Technology (NIST), cosponsored the Text Retrieval Conference (TREC) as part of the TIPSTER text program. The aim of this was to look into the information retrieval community by supplying the infrastructure that was needed for evaluation of text retrieval methodologies on a very large text collection. This catalyzed research on methods that scale to huge corpora. The introduction of web search engines has boosted the need for very large scale retrieval systems even further.",,What is TREC for information retrieval research?,"[{""answer_confidence"":{""high"":False,""low"":True,""medium"":False,""very_high"":False,""very_low"":False},""passage-snippets-annotation-qualification-task"":{""entities"":[{""endOffset"":192,""label"":""relevant-text-span"",""startOffset"":119},{""endOffset"":389,""label"":""relevant-text-span"",""startOffset"":214}]}}]",x,,"[' the Text Retrieval Conference (TREC) as part of the TIPSTER text program', 'to look into the information retrieval community by supplying the infrastructure that was needed for evaluation of text retrieval methodologies on a very large text collection']"
3ICOHX7EOGQSAQQXBU4IWQ5J1PCE07,3I3MQQJ31WXKIN6QF6OQSRUDQS81J6,Identify relevant text spans in text passage,Identify all the text spans that contain key pieces of the answer to a given question,"text, text spans, relevance",$0.30,Fri May 19 03:53:00 PDT 2023,3,BatchId:5076839;OriginalHitTemplateId:929448123;,21600,259200,Mon May 22 03:53:00 PDT 2023,,,33JKGHPFYH9WUU8BJ0Z19U2WWLYMN5,worker_232,Submitted,Fri May 19 18:33:08 PDT 2023,Fri May 19 18:44:25 PDT 2023,Mon May 22 18:44:25 PDT 2023,,,,677,100% (208/208),100% (179/179),100% (178/178),104_7,0,CAR_e8c23427635f085ca5a8a449bb69316c9897d948,3,"In 1992, the US Department of Defense along with the National Institute of Standards and Technology (NIST), cosponsored the Text Retrieval Conference (TREC) as part of the TIPSTER text program. The aim of this was to look into the information retrieval community by supplying the infrastructure that was needed for evaluation of text retrieval methodologies on a very large text collection. This catalyzed research on methods that scale to huge corpora. The introduction of web search engines has boosted the need for very large scale retrieval systems even further.",,What is TREC for information retrieval research?,"[{""answer_confidence"":{""high"":False,""low"":True,""medium"":False,""very_high"":False,""very_low"":False},""passage-snippets-annotation-qualification-task"":{""entities"":[{""endOffset"":390,""label"":""relevant-text-span"",""startOffset"":194}]}}]",x,,['The aim of this was to look into the information retrieval community by supplying the infrastructure that was needed for evaluation of text retrieval methodologies on a very large text collection.']
3ICOHX7EOGQSAQQXBU4IWQ5J1PCE07,3I3MQQJ31WXKIN6QF6OQSRUDQS81J6,Identify relevant text spans in text passage,Identify all the text spans that contain key pieces of the answer to a given question,"text, text spans, relevance",$0.30,Fri May 19 03:53:00 PDT 2023,3,BatchId:5076839;OriginalHitTemplateId:929448123;,21600,259200,Mon May 22 03:53:00 PDT 2023,,,3MX2NQ3YCE93YH19T75OQUI8C2A5X6,worker_249,Submitted,Fri May 19 06:51:07 PDT 2023,Fri May 19 06:55:15 PDT 2023,Mon May 22 06:55:15 PDT 2023,,,,248,100% (194/194),100% (191/191),100% (190/190),104_7,0,CAR_e8c23427635f085ca5a8a449bb69316c9897d948,3,"In 1992, the US Department of Defense along with the National Institute of Standards and Technology (NIST), cosponsored the Text Retrieval Conference (TREC) as part of the TIPSTER text program. The aim of this was to look into the information retrieval community by supplying the infrastructure that was needed for evaluation of text retrieval methodologies on a very large text collection. This catalyzed research on methods that scale to huge corpora. The introduction of web search engines has boosted the need for very large scale retrieval systems even further.",,What is TREC for information retrieval research?,"[{""answer_confidence"":{""high"":False,""low"":False,""medium"":False,""very_high"":True,""very_low"":False},""passage-snippets-annotation-qualification-task"":{""entities"":[{""endOffset"":192,""label"":""relevant-text-span"",""startOffset"":160},{""endOffset"":452,""label"":""relevant-text-span"",""startOffset"":396}]}}]",x,,"['part of the TIPSTER text program', 'catalyzed research on methods that scale to huge corpora']"
3AJA9FLWTGDVNZ79AVDUKCQFZK8IFO,3I3MQQJ31WXKIN6QF6OQSRUDQS81J6,Identify relevant text spans in text passage,Identify all the text spans that contain key pieces of the answer to a given question,"text, text spans, relevance",$0.30,Fri May 19 03:53:00 PDT 2023,3,BatchId:5076839;OriginalHitTemplateId:929448123;,21600,259200,Mon May 22 03:53:00 PDT 2023,,,31QNSG6A5W8S6KEK9S8T8FFBC0F871,worker_119,Submitted,Fri May 19 14:29:44 PDT 2023,Fri May 19 14:31:07 PDT 2023,Mon May 22 14:31:07 PDT 2023,,,,83,100% (270/271),100% (179/179),100% (178/178),104_7,0,CAR_fc8e2e752c00d2fb23716ebee18433e7a2f29bfc,4,"The Text REtrieval Conference (TREC) is an ongoing series of workshops focusing on a list of different information retrieval (IR) research areas, or tracks. It is co-sponsored by the National Institute of Standards and Technology (NIST) and the Intelligence Advanced Research Projects Activity (part of the office of the Director of National Intelligence), and began in 1992 as part of the TIPSTER Text program. Its purpose is to support and encourage research within the information retrieval community by providing the infrastructure necessary for large-scale evaluation of text retrieval methodologies and to increase the speed of lab-to-product transfer of technology.",,What is TREC for information retrieval research?,"[{""answer_confidence"":{""high"":False,""low"":True,""medium"":False,""very_high"":False,""very_low"":False},""passage-snippets-annotation-qualification-task"":{""entities"":[{""endOffset"":155,""label"":""relevant-text-span"",""startOffset"":0},{""endOffset"":672,""label"":""relevant-text-span"",""startOffset"":412}]}}]",x,,"['The Text REtrieval Conference (TREC) is an ongoing series of workshops focusing on a list of different information retrieval (IR) research areas, or tracks', 'Its purpose is to support and encourage research within the information retrieval community by providing the infrastructure necessary for large-scale evaluation of text retrieval methodologies and to increase the speed of lab-to-product transfer of technology.']"
3AJA9FLWTGDVNZ79AVDUKCQFZK8IFO,3I3MQQJ31WXKIN6QF6OQSRUDQS81J6,Identify relevant text spans in text passage,Identify all the text spans that contain key pieces of the answer to a given question,"text, text spans, relevance",$0.30,Fri May 19 03:53:00 PDT 2023,3,BatchId:5076839;OriginalHitTemplateId:929448123;,21600,259200,Mon May 22 03:53:00 PDT 2023,,,33SA9F9TR29O50PCU325S5AEGVAEWY,worker_232,Submitted,Fri May 19 18:14:21 PDT 2023,Fri May 19 18:21:15 PDT 2023,Mon May 22 18:21:15 PDT 2023,,,,414,100% (208/208),100% (179/179),100% (178/178),104_7,0,CAR_fc8e2e752c00d2fb23716ebee18433e7a2f29bfc,4,"The Text REtrieval Conference (TREC) is an ongoing series of workshops focusing on a list of different information retrieval (IR) research areas, or tracks. It is co-sponsored by the National Institute of Standards and Technology (NIST) and the Intelligence Advanced Research Projects Activity (part of the office of the Director of National Intelligence), and began in 1992 as part of the TIPSTER Text program. Its purpose is to support and encourage research within the information retrieval community by providing the infrastructure necessary for large-scale evaluation of text retrieval methodologies and to increase the speed of lab-to-product transfer of technology.",,What is TREC for information retrieval research?,"[{""answer_confidence"":{""high"":True,""low"":False,""medium"":False,""very_high"":False,""very_low"":False},""passage-snippets-annotation-qualification-task"":{""entities"":[{""endOffset"":155,""label"":""relevant-text-span"",""startOffset"":0}]}}]",x,,"['The Text REtrieval Conference (TREC) is an ongoing series of workshops focusing on a list of different information retrieval (IR) research areas, or tracks']"
3AJA9FLWTGDVNZ79AVDUKCQFZK8IFO,3I3MQQJ31WXKIN6QF6OQSRUDQS81J6,Identify relevant text spans in text passage,Identify all the text spans that contain key pieces of the answer to a given question,"text, text spans, relevance",$0.30,Fri May 19 03:53:00 PDT 2023,3,BatchId:5076839;OriginalHitTemplateId:929448123;,21600,259200,Mon May 22 03:53:00 PDT 2023,,,32UTUBMZ7LBDJ8DNU2VJ5VLF6MOVBS,worker_249,Approved,Fri May 19 05:05:51 PDT 2023,Fri May 19 05:08:22 PDT 2023,Mon May 22 05:08:22 PDT 2023,Mon May 22 05:09:19 PDT 2023,,,151,100% (194/194),100% (191/191),100% (190/190),104_7,0,CAR_fc8e2e752c00d2fb23716ebee18433e7a2f29bfc,4,"The Text REtrieval Conference (TREC) is an ongoing series of workshops focusing on a list of different information retrieval (IR) research areas, or tracks. It is co-sponsored by the National Institute of Standards and Technology (NIST) and the Intelligence Advanced Research Projects Activity (part of the office of the Director of National Intelligence), and began in 1992 as part of the TIPSTER Text program. Its purpose is to support and encourage research within the information retrieval community by providing the infrastructure necessary for large-scale evaluation of text retrieval methodologies and to increase the speed of lab-to-product transfer of technology.",,What is TREC for information retrieval research?,"[{""answer_confidence"":{""high"":False,""low"":False,""medium"":False,""very_high"":True,""very_low"":False},""passage-snippets-annotation-qualification-task"":{""entities"":[{""endOffset"":155,""label"":""relevant-text-span"",""startOffset"":40}]}}]",x,,"['an ongoing series of workshops focusing on a list of different information retrieval (IR) research areas, or tracks']"
3VEI3XUC0VCK7BZMEL38AY5AOMEPR2,3I3MQQJ31WXKIN6QF6OQSRUDQS81J6,Identify relevant text spans in text passage,Identify all the text spans that contain key pieces of the answer to a given question,"text, text spans, relevance",$0.30,Fri May 19 03:53:00 PDT 2023,3,BatchId:5076839;OriginalHitTemplateId:929448123;,21600,259200,Mon May 22 03:53:00 PDT 2023,,,3RXPCZQMQUQTOD503A38ZI5WXXSG1S,worker_119,Submitted,Fri May 19 14:29:51 PDT 2023,Fri May 19 14:35:21 PDT 2023,Mon May 22 14:35:21 PDT 2023,,,,330,100% (270/271),100% (179/179),100% (178/178),104_7,0,MARCO_4593337,2,"Text Retrieval Conference. For other uses of TREC, see TREC (disambiguation). The Text REtrieval Conference (TREC) is an ongoing series of workshops focusing on a list of different information retrieval (IR) research areas, or tracks.",0.0,What is TREC for information retrieval research?,"[{""answer_confidence"":{""high"":False,""low"":False,""medium"":True,""very_high"":False,""very_low"":False},""passage-snippets-annotation-qualification-task"":{""entities"":[{""endOffset"":25,""label"":""relevant-text-span"",""startOffset"":0},{""endOffset"":234,""label"":""relevant-text-span"",""startOffset"":78}]}}]",,The agreement rate not met. None of the text span chosen by you has been selected by other workers.,"['Text Retrieval Conference', 'The Text REtrieval Conference (TREC) is an ongoing series of workshops focusing on a list of different information retrieval (IR) research areas, or tracks.']"
3VEI3XUC0VCK7BZMEL38AY5AOMEPR2,3I3MQQJ31WXKIN6QF6OQSRUDQS81J6,Identify relevant text spans in text passage,Identify all the text spans that contain key pieces of the answer to a given question,"text, text spans, relevance",$0.30,Fri May 19 03:53:00 PDT 2023,3,BatchId:5076839;OriginalHitTemplateId:929448123;,21600,259200,Mon May 22 03:53:00 PDT 2023,,,3NVC2EB65VEPK74FKK3XUBRX0EMY3S,worker_232,Submitted,Fri May 19 18:45:10 PDT 2023,Fri May 19 18:50:04 PDT 2023,Mon May 22 18:50:04 PDT 2023,,,,294,100% (208/208),100% (179/179),100% (178/178),104_7,0,MARCO_4593337,2,"Text Retrieval Conference. For other uses of TREC, see TREC (disambiguation). The Text REtrieval Conference (TREC) is an ongoing series of workshops focusing on a list of different information retrieval (IR) research areas, or tracks.",0.0,What is TREC for information retrieval research?,"[{""answer_confidence"":{""high"":False,""low"":False,""medium"":False,""very_high"":True,""very_low"":False},""passage-snippets-annotation-qualification-task"":{""entities"":[{""endOffset"":234,""label"":""relevant-text-span"",""startOffset"":78}]}}]",,Chosen text spans are too long (longer than half of the passage). The requirements say that text spans should be concise and as short as possible.,"['The Text REtrieval Conference (TREC) is an ongoing series of workshops focusing on a list of different information retrieval (IR) research areas, or tracks.']"
3VEI3XUC0VCK7BZMEL38AY5AOMEPR2,3I3MQQJ31WXKIN6QF6OQSRUDQS81J6,Identify relevant text spans in text passage,Identify all the text spans that contain key pieces of the answer to a given question,"text, text spans, relevance",$0.30,Fri May 19 03:53:00 PDT 2023,3,BatchId:5076839;OriginalHitTemplateId:929448123;,21600,259200,Mon May 22 03:53:00 PDT 2023,,,3I7DHKZYGSFMYVAOTDZ2KNI18X85F9,worker_249,Submitted,Fri May 19 05:59:58 PDT 2023,Fri May 19 06:10:00 PDT 2023,Mon May 22 06:10:00 PDT 2023,,,,602,100% (194/194),100% (191/191),100% (190/190),104_7,0,MARCO_4593337,2,"Text Retrieval Conference. For other uses of TREC, see TREC (disambiguation). The Text REtrieval Conference (TREC) is an ongoing series of workshops focusing on a list of different information retrieval (IR) research areas, or tracks.",0.0,What is TREC for information retrieval research?,"[{""answer_confidence"":{""high"":False,""low"":False,""medium"":False,""very_high"":True,""very_low"":False},""passage-snippets-annotation-qualification-task"":{""entities"":[{""endOffset"":234,""label"":""relevant-text-span"",""startOffset"":118}]}}]",,The agreement rate not met. None of the text span chosen by you has been selected by other workers.,"['an ongoing series of workshops focusing on a list of different information retrieval (IR) research areas, or tracks.']"
3B9XR6P1XIA15TFZNBCYDNEUX15BJB,3I3MQQJ31WXKIN6QF6OQSRUDQS81J6,Identify relevant text spans in text passage,Identify all the text spans that contain key pieces of the answer to a given question,"text, text spans, relevance",$0.30,Fri May 19 03:53:01 PDT 2023,3,BatchId:5076839;OriginalHitTemplateId:929448123;,21600,259200,Mon May 22 03:53:01 PDT 2023,,,3K9FOBBF2MYCOCQQJOYM9HN7WF1LNT,worker_119,Submitted,Fri May 19 11:14:32 PDT 2023,Fri May 19 12:36:08 PDT 2023,Mon May 22 12:36:08 PDT 2023,,,,4896,100% (270/271),100% (179/179),100% (178/178),104_8,0,CAR_14ad79757fc328eef8c44b779c7751565428000a,1,"The test collections developed at TREC are useful not just for (potentially) helping researchers advance the state of the art, but also for allowing developers of new (commercial) retrieval products to evaluate their effectiveness on standard tests.  In the past decade, TREC has created new tests for enterprise e-mail search, genomics search, spam filtering, e-Discovery, and several other retrieval domains.",0.0,What are some recent TREC tasks?,"[{""answer_confidence"":{""high"":False,""low"":False,""medium"":True,""very_high"":False,""very_low"":False},""passage-snippets-annotation-qualification-task"":{""entities"":[{""endOffset"":410,""label"":""relevant-text-span"",""startOffset"":251}]}}]",x,,"['In the past decade, TREC has created new tests for enterprise e-mail search, genomics search, spam filtering, e-Discovery, and several other retrieval domains.']"
3B9XR6P1XIA15TFZNBCYDNEUX15BJB,3I3MQQJ31WXKIN6QF6OQSRUDQS81J6,Identify relevant text spans in text passage,Identify all the text spans that contain key pieces of the answer to a given question,"text, text spans, relevance",$0.30,Fri May 19 03:53:01 PDT 2023,3,BatchId:5076839;OriginalHitTemplateId:929448123;,21600,259200,Mon May 22 03:53:01 PDT 2023,,,31QTRG6Q2YSWRW92II047EUY5ACPYT,worker_232,Submitted,Fri May 19 18:14:24 PDT 2023,Fri May 19 18:28:58 PDT 2023,Mon May 22 18:28:58 PDT 2023,,,,874,100% (208/208),100% (179/179),100% (178/178),104_8,0,CAR_14ad79757fc328eef8c44b779c7751565428000a,1,"The test collections developed at TREC are useful not just for (potentially) helping researchers advance the state of the art, but also for allowing developers of new (commercial) retrieval products to evaluate their effectiveness on standard tests.  In the past decade, TREC has created new tests for enterprise e-mail search, genomics search, spam filtering, e-Discovery, and several other retrieval domains.",0.0,What are some recent TREC tasks?,"[{""answer_confidence"":{""high"":False,""low"":False,""medium"":False,""very_high"":True,""very_low"":False},""passage-snippets-annotation-qualification-task"":{""entities"":[{""endOffset"":410,""label"":""relevant-text-span"",""startOffset"":251}]}}]",x,,"['In the past decade, TREC has created new tests for enterprise e-mail search, genomics search, spam filtering, e-Discovery, and several other retrieval domains.']"
3B9XR6P1XIA15TFZNBCYDNEUX15BJB,3I3MQQJ31WXKIN6QF6OQSRUDQS81J6,Identify relevant text spans in text passage,Identify all the text spans that contain key pieces of the answer to a given question,"text, text spans, relevance",$0.30,Fri May 19 03:53:01 PDT 2023,3,BatchId:5076839;OriginalHitTemplateId:929448123;,21600,259200,Mon May 22 03:53:01 PDT 2023,,,3TK8OJTYM60FN2EXD7DK0FQ4NRWVP8,worker_249,Submitted,Fri May 19 05:58:53 PDT 2023,Fri May 19 05:59:53 PDT 2023,Mon May 22 05:59:53 PDT 2023,,,,60,100% (194/194),100% (191/191),100% (190/190),104_8,0,CAR_14ad79757fc328eef8c44b779c7751565428000a,1,"The test collections developed at TREC are useful not just for (potentially) helping researchers advance the state of the art, but also for allowing developers of new (commercial) retrieval products to evaluate their effectiveness on standard tests.  In the past decade, TREC has created new tests for enterprise e-mail search, genomics search, spam filtering, e-Discovery, and several other retrieval domains.",0.0,What are some recent TREC tasks?,"[{""answer_confidence"":{""high"":False,""low"":False,""medium"":False,""very_high"":True,""very_low"":False},""passage-snippets-annotation-qualification-task"":{""entities"":[{""endOffset"":410,""label"":""relevant-text-span"",""startOffset"":250}]}}]",x,,"[' In the past decade, TREC has created new tests for enterprise e-mail search, genomics search, spam filtering, e-Discovery, and several other retrieval domains.']"
307L9TDWK27SLO27NGPKOBXZITXN30,3I3MQQJ31WXKIN6QF6OQSRUDQS81J6,Identify relevant text spans in text passage,Identify all the text spans that contain key pieces of the answer to a given question,"text, text spans, relevance",$0.30,Fri May 19 03:53:00 PDT 2023,3,BatchId:5076839;OriginalHitTemplateId:929448123;,21600,259200,Mon May 22 03:53:00 PDT 2023,,,3YJ6NA41JGV6W7E3WIZ1XRFAS4NPJF,worker_119,Approved,Fri May 19 09:31:53 PDT 2023,Fri May 19 10:57:05 PDT 2023,Mon May 22 10:57:05 PDT 2023,Mon May 22 05:55:51 PDT 2023,,,5112,100% (270/271),100% (179/179),100% (178/178),104_8,0,CAR_76a49c5d4b83b1928933eeb7d67eea36b7035b48,2,"Since 2015, Grossman has served as a coordinator for the Total Recall Track at the National Institute of Standards and Technology's Text Retrieval Conference (TREC). In 2010 and 2011, she served as a coordinator for the Legal Track at TREC; in 2008 and 2009, she served as a subject matter expert.",,What are some recent TREC tasks?,"[{""answer_confidence"":{""high"":True,""low"":False,""medium"":False,""very_high"":False,""very_low"":False},""passage-snippets-annotation-qualification-task"":{""entities"":[{""endOffset"":75,""label"":""relevant-text-span"",""startOffset"":57},{""endOffset"":239,""label"":""relevant-text-span"",""startOffset"":220}]}}]",x,,"['Total Recall Track', 'Legal Track at TREC']"
307L9TDWK27SLO27NGPKOBXZITXN30,3I3MQQJ31WXKIN6QF6OQSRUDQS81J6,Identify relevant text spans in text passage,Identify all the text spans that contain key pieces of the answer to a given question,"text, text spans, relevance",$0.30,Fri May 19 03:53:00 PDT 2023,3,BatchId:5076839;OriginalHitTemplateId:929448123;,21600,259200,Mon May 22 03:53:00 PDT 2023,,,39O5D9O87Y7CH43AOSL8XKZEXQ2C3W,worker_232,Approved,Fri May 19 18:14:14 PDT 2023,Fri May 19 18:16:39 PDT 2023,Mon May 22 18:16:39 PDT 2023,Mon May 22 05:55:51 PDT 2023,,,145,100% (208/208),100% (179/179),100% (178/178),104_8,0,CAR_76a49c5d4b83b1928933eeb7d67eea36b7035b48,2,"Since 2015, Grossman has served as a coordinator for the Total Recall Track at the National Institute of Standards and Technology's Text Retrieval Conference (TREC). In 2010 and 2011, she served as a coordinator for the Legal Track at TREC; in 2008 and 2009, she served as a subject matter expert.",,What are some recent TREC tasks?,"[{""answer_confidence"":{""high"":False,""low"":True,""medium"":False,""very_high"":False,""very_low"":False},""passage-snippets-annotation-qualification-task"":{""entities"":[]}}]",,Chosen text spans are too long (longer than half of the passage). The requirements say that text spans should be concise and as short as possible.,[]
307L9TDWK27SLO27NGPKOBXZITXN30,3I3MQQJ31WXKIN6QF6OQSRUDQS81J6,Identify relevant text spans in text passage,Identify all the text spans that contain key pieces of the answer to a given question,"text, text spans, relevance",$0.30,Fri May 19 03:53:00 PDT 2023,3,BatchId:5076839;OriginalHitTemplateId:929448123;,21600,259200,Mon May 22 03:53:00 PDT 2023,,,3483FV8BEJYYG5YQA1U0SA2CVOK268,worker_249,Approved,Fri May 19 06:55:19 PDT 2023,Fri May 19 06:56:32 PDT 2023,Mon May 22 06:56:32 PDT 2023,Mon May 22 05:55:51 PDT 2023,,,73,100% (194/194),100% (191/191),100% (190/190),104_8,0,CAR_76a49c5d4b83b1928933eeb7d67eea36b7035b48,2,"Since 2015, Grossman has served as a coordinator for the Total Recall Track at the National Institute of Standards and Technology's Text Retrieval Conference (TREC). In 2010 and 2011, she served as a coordinator for the Legal Track at TREC; in 2008 and 2009, she served as a subject matter expert.",,What are some recent TREC tasks?,"[{""answer_confidence"":{""high"":False,""low"":False,""medium"":False,""very_high"":True,""very_low"":False},""passage-snippets-annotation-qualification-task"":{""entities"":[{""endOffset"":76,""label"":""relevant-text-span"",""startOffset"":53},{""endOffset"":231,""label"":""relevant-text-span"",""startOffset"":215}]}}]",x,,"['the Total Recall Track ', ' the Legal Track']"
3NI0WFPPJDVOXBXI5CLKK2D6KRO60E,3I3MQQJ31WXKIN6QF6OQSRUDQS81J6,Identify relevant text spans in text passage,Identify all the text spans that contain key pieces of the answer to a given question,"text, text spans, relevance",$0.30,Fri May 19 03:53:00 PDT 2023,3,BatchId:5076839;OriginalHitTemplateId:929448123;,21600,259200,Mon May 22 03:53:00 PDT 2023,,,3RKNTXVS3RD96LPG4SU01U1WBLXA4T,worker_119,Submitted,Fri May 19 09:31:58 PDT 2023,Fri May 19 09:38:16 PDT 2023,Mon May 22 09:38:16 PDT 2023,,,,378,100% (270/271),100% (179/179),100% (178/178),104_8,0,CAR_c56480057a9a2b42a30ab6c7208ffd6cfc2b7074,2,"The TRECVID evaluation meetings are an on-going series of workshops focusing on a list of different information retrieval (IR) research areas in content-based retrieval and exploitation of digital video. TRECVID is co-sponsored by the National Institute of Standards and Technology (NIST) and other US government agencies. Various participating research groups make significant contributions. The goal of the workshops is to encourage research in content-based video retrieval and analysis by providing large test collections, realistic system tasks, uniform scoring procedures, and a forum for organizations interested in comparing their results.",,What are some recent TREC tasks?,"[{""answer_confidence"":{""high"":False,""low"":False,""medium"":True,""very_high"":False,""very_low"":False},""passage-snippets-annotation-qualification-task"":{""entities"":[{""endOffset"":31,""label"":""relevant-text-span"",""startOffset"":0},{""endOffset"":211,""label"":""relevant-text-span"",""startOffset"":204}]}}]",,The agreement rate not met. None of the text span chosen by you has been selected by other workers.,"['The TRECVID evaluation meetings', 'TRECVID']"
3NI0WFPPJDVOXBXI5CLKK2D6KRO60E,3I3MQQJ31WXKIN6QF6OQSRUDQS81J6,Identify relevant text spans in text passage,Identify all the text spans that contain key pieces of the answer to a given question,"text, text spans, relevance",$0.30,Fri May 19 03:53:00 PDT 2023,3,BatchId:5076839;OriginalHitTemplateId:929448123;,21600,259200,Mon May 22 03:53:00 PDT 2023,,,3A4NIXBJ7BE66U5Q7NJMW2Y7RQYML3,worker_232,Submitted,Fri May 19 18:33:02 PDT 2023,Fri May 19 18:41:30 PDT 2023,Mon May 22 18:41:30 PDT 2023,,,,508,100% (208/208),100% (179/179),100% (178/178),104_8,0,CAR_c56480057a9a2b42a30ab6c7208ffd6cfc2b7074,2,"The TRECVID evaluation meetings are an on-going series of workshops focusing on a list of different information retrieval (IR) research areas in content-based retrieval and exploitation of digital video. TRECVID is co-sponsored by the National Institute of Standards and Technology (NIST) and other US government agencies. Various participating research groups make significant contributions. The goal of the workshops is to encourage research in content-based video retrieval and analysis by providing large test collections, realistic system tasks, uniform scoring procedures, and a forum for organizations interested in comparing their results.",,What are some recent TREC tasks?,"[{""answer_confidence"":{""high"":False,""low"":True,""medium"":False,""very_high"":False,""very_low"":False},""passage-snippets-annotation-qualification-task"":{""entities"":[{""endOffset"":647,""label"":""relevant-text-span"",""startOffset"":393}]}}]",,The agreement rate not met. None of the text span chosen by you has been selected by other workers.,"['The goal of the workshops is to encourage research in content-based video retrieval and analysis by providing large test collections, realistic system tasks, uniform scoring procedures, and a forum for organizations interested in comparing their results.']"
3NI0WFPPJDVOXBXI5CLKK2D6KRO60E,3I3MQQJ31WXKIN6QF6OQSRUDQS81J6,Identify relevant text spans in text passage,Identify all the text spans that contain key pieces of the answer to a given question,"text, text spans, relevance",$0.30,Fri May 19 03:53:00 PDT 2023,3,BatchId:5076839;OriginalHitTemplateId:929448123;,21600,259200,Mon May 22 03:53:00 PDT 2023,,,3QJOXOW4XO6S0OVYQ1WIJ78NIQOEMT,worker_249,Approved,Fri May 19 04:54:16 PDT 2023,Fri May 19 05:05:33 PDT 2023,Mon May 22 05:05:33 PDT 2023,Mon May 22 05:06:19 PDT 2023,,,677,100% (194/194),100% (191/191),100% (190/190),104_8,0,CAR_c56480057a9a2b42a30ab6c7208ffd6cfc2b7074,2,"The TRECVID evaluation meetings are an on-going series of workshops focusing on a list of different information retrieval (IR) research areas in content-based retrieval and exploitation of digital video. TRECVID is co-sponsored by the National Institute of Standards and Technology (NIST) and other US government agencies. Various participating research groups make significant contributions. The goal of the workshops is to encourage research in content-based video retrieval and analysis by providing large test collections, realistic system tasks, uniform scoring procedures, and a forum for organizations interested in comparing their results.",,What are some recent TREC tasks?,"[{""answer_confidence"":{""high"":True,""low"":False,""medium"":False,""very_high"":False,""very_low"":False},""passage-snippets-annotation-qualification-task"":{""entities"":[]}}]",,Chosen text spans are too long (longer than half of the passage). The requirements say that text spans should be concise and as short as possible.,[]
3DWGDA5PPJJ9JQX123XSAZCF6CLV1X,3I3MQQJ31WXKIN6QF6OQSRUDQS81J6,Identify relevant text spans in text passage,Identify all the text spans that contain key pieces of the answer to a given question,"text, text spans, relevance",$0.30,Fri May 19 03:53:01 PDT 2023,3,BatchId:5076839;OriginalHitTemplateId:929448123;,21600,259200,Mon May 22 03:53:01 PDT 2023,,,3S3AMIZX3ZKAZW57HHRKATLQO23DC5,worker_119,Submitted,Fri May 19 14:29:42 PDT 2023,Fri May 19 14:31:49 PDT 2023,Mon May 22 14:31:49 PDT 2023,,,,127,100% (270/271),100% (179/179),100% (178/178),104_8,0,CAR_fab48deecb67593175fe43a9731fbeb4f9c37eea,1,The term 'adversarial information retrieval' was first coined in 2000 by Andrei Broder (then Chief Scientist at Alta Vista) during the Web plenary session at the TREC-9 conference.,1.0,What are some recent TREC tasks?,"[{""answer_confidence"":{""high"":False,""low"":True,""medium"":False,""very_high"":False,""very_low"":False},""passage-snippets-annotation-qualification-task"":{""entities"":[{""endOffset"":43,""label"":""relevant-text-span"",""startOffset"":10}]}}]",,The agreement rate not met. None of the text span chosen by you has been selected by other workers.,['adversarial information retrieval']
3DWGDA5PPJJ9JQX123XSAZCF6CLV1X,3I3MQQJ31WXKIN6QF6OQSRUDQS81J6,Identify relevant text spans in text passage,Identify all the text spans that contain key pieces of the answer to a given question,"text, text spans, relevance",$0.30,Fri May 19 03:53:01 PDT 2023,3,BatchId:5076839;OriginalHitTemplateId:929448123;,21600,259200,Mon May 22 03:53:01 PDT 2023,,,3Z9WI9EOZ439UAMXVQR8PXDWL4THKL,worker_232,Submitted,Fri May 19 18:14:29 PDT 2023,Fri May 19 18:33:32 PDT 2023,Mon May 22 18:33:32 PDT 2023,,,,1143,100% (208/208),100% (179/179),100% (178/178),104_8,0,CAR_fab48deecb67593175fe43a9731fbeb4f9c37eea,1,The term 'adversarial information retrieval' was first coined in 2000 by Andrei Broder (then Chief Scientist at Alta Vista) during the Web plenary session at the TREC-9 conference.,1.0,What are some recent TREC tasks?,"[{""answer_confidence"":{""high"":True,""low"":False,""medium"":False,""very_high"":False,""very_low"":False},""passage-snippets-annotation-qualification-task"":{""entities"":[]}}]",,Chosen text spans are too long (longer than half of the passage). The requirements say that text spans should be concise and as short as possible.,[]
3DWGDA5PPJJ9JQX123XSAZCF6CLV1X,3I3MQQJ31WXKIN6QF6OQSRUDQS81J6,Identify relevant text spans in text passage,Identify all the text spans that contain key pieces of the answer to a given question,"text, text spans, relevance",$0.30,Fri May 19 03:53:01 PDT 2023,3,BatchId:5076839;OriginalHitTemplateId:929448123;,21600,259200,Mon May 22 03:53:01 PDT 2023,,,3E7TUJ2EGH181YY45XHT83KIIGUD9G,worker_249,Submitted,Fri May 19 08:20:04 PDT 2023,Fri May 19 08:20:48 PDT 2023,Mon May 22 08:20:48 PDT 2023,,,,44,100% (194/194),100% (191/191),100% (190/190),104_8,0,CAR_fab48deecb67593175fe43a9731fbeb4f9c37eea,1,The term 'adversarial information retrieval' was first coined in 2000 by Andrei Broder (then Chief Scientist at Alta Vista) during the Web plenary session at the TREC-9 conference.,1.0,What are some recent TREC tasks?,"[{""answer_confidence"":{""high"":False,""low"":True,""medium"":False,""very_high"":False,""very_low"":False},""passage-snippets-annotation-qualification-task"":{""entities"":[]}}]",,Chosen text spans are too long (longer than half of the passage). The requirements say that text spans should be concise and as short as possible.,[]
3PZDSVZ3K9WD3KLBNUU3YGP5WTTN4L,3I3MQQJ31WXKIN6QF6OQSRUDQS81J6,Identify relevant text spans in text passage,Identify all the text spans that contain key pieces of the answer to a given question,"text, text spans, relevance",$0.30,Fri May 19 03:53:01 PDT 2023,3,BatchId:5076839;OriginalHitTemplateId:929448123;,21600,259200,Mon May 22 03:53:01 PDT 2023,,,34Q075JO12SWD9UFP8E5EQIGQF810D,worker_119,Submitted,Fri May 19 09:05:50 PDT 2023,Fri May 19 10:56:45 PDT 2023,Mon May 22 10:56:45 PDT 2023,,,,6655,100% (270/271),100% (179/179),100% (178/178),104_8,0,CAR_fd5e2741dc60184df6657819a91a02fdf71d7e96,2,"Since 2001, he has been a program committee member of The Text Retrieval Conference (TREC). He is a coordinator of the TREC Total Recall Track, and past coordinator of the TREC Legal Track (2010 - 2011) and TREC Spam Track (2005 - 2007). Cormack is past president of the Conference on Email and Anti-Spam.",,What are some recent TREC tasks?,"[{""answer_confidence"":{""high"":False,""low"":False,""medium"":False,""very_high"":True,""very_low"":False},""passage-snippets-annotation-qualification-task"":{""entities"":[{""endOffset"":142,""label"":""relevant-text-span"",""startOffset"":119},{""endOffset"":188,""label"":""relevant-text-span"",""startOffset"":172},{""endOffset"":222,""label"":""relevant-text-span"",""startOffset"":207}]}}]",x,,"['TREC Total Recall Track', 'TREC Legal Track', 'TREC Spam Track']"
3PZDSVZ3K9WD3KLBNUU3YGP5WTTN4L,3I3MQQJ31WXKIN6QF6OQSRUDQS81J6,Identify relevant text spans in text passage,Identify all the text spans that contain key pieces of the answer to a given question,"text, text spans, relevance",$0.30,Fri May 19 03:53:01 PDT 2023,3,BatchId:5076839;OriginalHitTemplateId:929448123;,21600,259200,Mon May 22 03:53:01 PDT 2023,,,3X87C8JFVBQKVMY8JPN4PXW7TL4SQB,worker_232,Submitted,Fri May 19 18:44:54 PDT 2023,Fri May 19 18:47:26 PDT 2023,Mon May 22 18:47:26 PDT 2023,,,,152,100% (208/208),100% (179/179),100% (178/178),104_8,0,CAR_fd5e2741dc60184df6657819a91a02fdf71d7e96,2,"Since 2001, he has been a program committee member of The Text Retrieval Conference (TREC). He is a coordinator of the TREC Total Recall Track, and past coordinator of the TREC Legal Track (2010 - 2011) and TREC Spam Track (2005 - 2007). Cormack is past president of the Conference on Email and Anti-Spam.",,What are some recent TREC tasks?,"[{""answer_confidence"":{""high"":False,""low"":False,""medium"":True,""very_high"":False,""very_low"":False},""passage-snippets-annotation-qualification-task"":{""entities"":[]}}]",,Chosen text spans are too long (longer than half of the passage). The requirements say that text spans should be concise and as short as possible.,[]
3PZDSVZ3K9WD3KLBNUU3YGP5WTTN4L,3I3MQQJ31WXKIN6QF6OQSRUDQS81J6,Identify relevant text spans in text passage,Identify all the text spans that contain key pieces of the answer to a given question,"text, text spans, relevance",$0.30,Fri May 19 03:53:01 PDT 2023,3,BatchId:5076839;OriginalHitTemplateId:929448123;,21600,259200,Mon May 22 03:53:01 PDT 2023,,,32SCWG5HIMJU8CZWCZ5CONA56YOP6U,worker_249,Submitted,Fri May 19 08:14:28 PDT 2023,Fri May 19 08:15:35 PDT 2023,Mon May 22 08:15:35 PDT 2023,,,,67,100% (194/194),100% (191/191),100% (190/190),104_8,0,CAR_fd5e2741dc60184df6657819a91a02fdf71d7e96,2,"Since 2001, he has been a program committee member of The Text Retrieval Conference (TREC). He is a coordinator of the TREC Total Recall Track, and past coordinator of the TREC Legal Track (2010 - 2011) and TREC Spam Track (2005 - 2007). Cormack is past president of the Conference on Email and Anti-Spam.",,What are some recent TREC tasks?,"[{""answer_confidence"":{""high"":False,""low"":False,""medium"":False,""very_high"":True,""very_low"":False},""passage-snippets-annotation-qualification-task"":{""entities"":[{""endOffset"":142,""label"":""relevant-text-span"",""startOffset"":115},{""endOffset"":188,""label"":""relevant-text-span"",""startOffset"":168},{""endOffset"":222,""label"":""relevant-text-span"",""startOffset"":206}]}}]",x,,"['the TREC Total Recall Track', 'the TREC Legal Track', ' TREC Spam Track']"
3VADEH0UIGCW8QZZTA3D08A6AOXPSQ,3I3MQQJ31WXKIN6QF6OQSRUDQS81J6,Identify relevant text spans in text passage,Identify all the text spans that contain key pieces of the answer to a given question,"text, text spans, relevance",$0.30,Fri May 19 03:53:01 PDT 2023,3,BatchId:5076839;OriginalHitTemplateId:929448123;,21600,259200,Mon May 22 03:53:01 PDT 2023,,,3EICBYG649BN2I5ZRDDVDRG3TMIJC7,worker_119,Submitted,Fri May 19 11:14:30 PDT 2023,Fri May 19 12:36:25 PDT 2023,Mon May 22 12:36:25 PDT 2023,,,,4915,100% (270/271),100% (179/179),100% (178/178),104_9,0,CAR_31d5f3d13c01fa9b1cdf6a5885d37a457da9c9e7,4,"Electronic discovery (also E-discovery or ediscovery) refers to discovery in legal proceedings such as litigation, government investigations, or Freedom of Information Act requests, where the information sought is in electronic format (often referred to as electronically stored information or ESI).  Electronic discovery is subject to rules of civil procedure and agreed-upon processes, often involving review for privilege and relevance before data are turned over to the requesting party.",,e-Discovery?  What's that?,"[{""answer_confidence"":{""high"":True,""low"":False,""medium"":False,""very_high"":False,""very_low"":False},""passage-snippets-annotation-qualification-task"":{""entities"":[{""endOffset"":20,""label"":""relevant-text-span"",""startOffset"":0},{""endOffset"":299,""label"":""relevant-text-span"",""startOffset"":64}]}}]",x,,"['Electronic discovery', 'discovery in legal proceedings such as litigation, government investigations, or Freedom of Information Act requests, where the information sought is in electronic format (often referred to as electronically stored information or ESI).']"
3VADEH0UIGCW8QZZTA3D08A6AOXPSQ,3I3MQQJ31WXKIN6QF6OQSRUDQS81J6,Identify relevant text spans in text passage,Identify all the text spans that contain key pieces of the answer to a given question,"text, text spans, relevance",$0.30,Fri May 19 03:53:01 PDT 2023,3,BatchId:5076839;OriginalHitTemplateId:929448123;,21600,259200,Mon May 22 03:53:01 PDT 2023,,,3K5TEWLKG0QN6GRT0CN0RIPS1FGVIF,worker_232,Submitted,Fri May 19 18:32:56 PDT 2023,Fri May 19 18:35:57 PDT 2023,Mon May 22 18:35:57 PDT 2023,,,,181,100% (208/208),100% (179/179),100% (178/178),104_9,0,CAR_31d5f3d13c01fa9b1cdf6a5885d37a457da9c9e7,4,"Electronic discovery (also E-discovery or ediscovery) refers to discovery in legal proceedings such as litigation, government investigations, or Freedom of Information Act requests, where the information sought is in electronic format (often referred to as electronically stored information or ESI).  Electronic discovery is subject to rules of civil procedure and agreed-upon processes, often involving review for privilege and relevance before data are turned over to the requesting party.",,e-Discovery?  What's that?,"[{""answer_confidence"":{""high"":True,""low"":False,""medium"":False,""very_high"":False,""very_low"":False},""passage-snippets-annotation-qualification-task"":{""entities"":[{""endOffset"":234,""label"":""relevant-text-span"",""startOffset"":0}]}}]",x,,"['Electronic discovery (also E-discovery or ediscovery) refers to discovery in legal proceedings such as litigation, government investigations, or Freedom of Information Act requests, where the information sought is in electronic format']"
3VADEH0UIGCW8QZZTA3D08A6AOXPSQ,3I3MQQJ31WXKIN6QF6OQSRUDQS81J6,Identify relevant text spans in text passage,Identify all the text spans that contain key pieces of the answer to a given question,"text, text spans, relevance",$0.30,Fri May 19 03:53:01 PDT 2023,3,BatchId:5076839;OriginalHitTemplateId:929448123;,21600,259200,Mon May 22 03:53:01 PDT 2023,,,3OSWBBLG1JCY2UG28DN7O660PJMXDK,worker_249,Submitted,Fri May 19 08:28:12 PDT 2023,Fri May 19 08:28:49 PDT 2023,Mon May 22 08:28:49 PDT 2023,,,,37,100% (194/194),100% (191/191),100% (190/190),104_9,0,CAR_31d5f3d13c01fa9b1cdf6a5885d37a457da9c9e7,4,"Electronic discovery (also E-discovery or ediscovery) refers to discovery in legal proceedings such as litigation, government investigations, or Freedom of Information Act requests, where the information sought is in electronic format (often referred to as electronically stored information or ESI).  Electronic discovery is subject to rules of civil procedure and agreed-upon processes, often involving review for privilege and relevance before data are turned over to the requesting party.",,e-Discovery?  What's that?,"[{""answer_confidence"":{""high"":False,""low"":False,""medium"":False,""very_high"":True,""very_low"":False},""passage-snippets-annotation-qualification-task"":{""entities"":[{""endOffset"":234,""label"":""relevant-text-span"",""startOffset"":63}]}}]",x,,"[' discovery in legal proceedings such as litigation, government investigations, or Freedom of Information Act requests, where the information sought is in electronic format']"
3IVKZBIBK4O4V91BAXV8RZ8FOXNSHO,3I3MQQJ31WXKIN6QF6OQSRUDQS81J6,Identify relevant text spans in text passage,Identify all the text spans that contain key pieces of the answer to a given question,"text, text spans, relevance",$0.30,Fri May 19 03:53:01 PDT 2023,3,BatchId:5076839;OriginalHitTemplateId:929448123;,21600,259200,Mon May 22 03:53:01 PDT 2023,,,3L4D84MIL47EJQGFD5RWOIDJ6ENHJE,worker_119,Submitted,Fri May 19 11:14:31 PDT 2023,Fri May 19 12:35:52 PDT 2023,Mon May 22 12:35:52 PDT 2023,,,,4881,100% (270/271),100% (179/179),100% (178/178),104_9,0,CAR_c29c94dd476388eba9becce0b0c752461b2b52bf,4,"Under the law of the United States, civil discovery is wide-ranging and can involve any material which is 'reasonably calculated to lead to admissible evidence.'  This is a much broader standard than relevance, because it contemplates the exploration of evidence which might be relevant, rather than evidence which is truly relevant.  (Issues of the scope of relevance are taken care of before trial in motions in limine and during trial with objections.) Certain types of information are generally protected from discovery; these include information which is privileged and the work product of the opposing party. Other types of information may be protected, depending on the type of case and the status of the party.  For instance, juvenile criminal records are generally not discoverable, peer review findings by hospitals in medical negligence cases are  generally not discoverable and, depending on the case, other types of evidence may be non-discoverable for reasons of privacy, difficulty and/or expense in complying and for other reasons. (Criminal discovery rules may differ from those discussed here.) Electronic discovery or 'e-discovery' refers to discovery of information stored in electronic format (often referred to as Electronically Stored Information, or ESI).",,e-Discovery?  What's that?,"[{""answer_confidence"":{""high"":True,""low"":False,""medium"":False,""very_high"":False,""very_low"":False},""passage-snippets-annotation-qualification-task"":{""entities"":[{""endOffset"":1133,""label"":""relevant-text-span"",""startOffset"":1113},{""endOffset"":1278,""label"":""relevant-text-span"",""startOffset"":1161}]}}]",x,,"['Electronic discovery', 'discovery of information stored in electronic format (often referred to as Electronically Stored Information, or ESI)']"
3IVKZBIBK4O4V91BAXV8RZ8FOXNSHO,3I3MQQJ31WXKIN6QF6OQSRUDQS81J6,Identify relevant text spans in text passage,Identify all the text spans that contain key pieces of the answer to a given question,"text, text spans, relevance",$0.30,Fri May 19 03:53:01 PDT 2023,3,BatchId:5076839;OriginalHitTemplateId:929448123;,21600,259200,Mon May 22 03:53:01 PDT 2023,,,38SKSKU7R6CKABFZM8LU49RQAHFLIN,worker_232,Submitted,Fri May 19 18:44:55 PDT 2023,Fri May 19 18:47:48 PDT 2023,Mon May 22 18:47:48 PDT 2023,,,,173,100% (208/208),100% (179/179),100% (178/178),104_9,0,CAR_c29c94dd476388eba9becce0b0c752461b2b52bf,4,"Under the law of the United States, civil discovery is wide-ranging and can involve any material which is 'reasonably calculated to lead to admissible evidence.'  This is a much broader standard than relevance, because it contemplates the exploration of evidence which might be relevant, rather than evidence which is truly relevant.  (Issues of the scope of relevance are taken care of before trial in motions in limine and during trial with objections.) Certain types of information are generally protected from discovery; these include information which is privileged and the work product of the opposing party. Other types of information may be protected, depending on the type of case and the status of the party.  For instance, juvenile criminal records are generally not discoverable, peer review findings by hospitals in medical negligence cases are  generally not discoverable and, depending on the case, other types of evidence may be non-discoverable for reasons of privacy, difficulty and/or expense in complying and for other reasons. (Criminal discovery rules may differ from those discussed here.) Electronic discovery or 'e-discovery' refers to discovery of information stored in electronic format (often referred to as Electronically Stored Information, or ESI).",,e-Discovery?  What's that?,"[{""answer_confidence"":{""high"":False,""low"":False,""medium"":False,""very_high"":True,""very_low"":False},""passage-snippets-annotation-qualification-task"":{""entities"":[{""endOffset"":1279,""label"":""relevant-text-span"",""startOffset"":1112}]}}]",x,,"["" Electronic discovery or 'e-discovery' refers to discovery of information stored in electronic format (often referred to as Electronically Stored Information, or ESI).""]"
3IVKZBIBK4O4V91BAXV8RZ8FOXNSHO,3I3MQQJ31WXKIN6QF6OQSRUDQS81J6,Identify relevant text spans in text passage,Identify all the text spans that contain key pieces of the answer to a given question,"text, text spans, relevance",$0.30,Fri May 19 03:53:01 PDT 2023,3,BatchId:5076839;OriginalHitTemplateId:929448123;,21600,259200,Mon May 22 03:53:01 PDT 2023,,,320DUZ38GC10JUL4TZ9E3I1IRD6GJ8,worker_249,Submitted,Fri May 19 06:27:38 PDT 2023,Fri May 19 06:29:14 PDT 2023,Mon May 22 06:29:14 PDT 2023,,,,96,100% (194/194),100% (191/191),100% (190/190),104_9,0,CAR_c29c94dd476388eba9becce0b0c752461b2b52bf,4,"Under the law of the United States, civil discovery is wide-ranging and can involve any material which is 'reasonably calculated to lead to admissible evidence.'  This is a much broader standard than relevance, because it contemplates the exploration of evidence which might be relevant, rather than evidence which is truly relevant.  (Issues of the scope of relevance are taken care of before trial in motions in limine and during trial with objections.) Certain types of information are generally protected from discovery; these include information which is privileged and the work product of the opposing party. Other types of information may be protected, depending on the type of case and the status of the party.  For instance, juvenile criminal records are generally not discoverable, peer review findings by hospitals in medical negligence cases are  generally not discoverable and, depending on the case, other types of evidence may be non-discoverable for reasons of privacy, difficulty and/or expense in complying and for other reasons. (Criminal discovery rules may differ from those discussed here.) Electronic discovery or 'e-discovery' refers to discovery of information stored in electronic format (often referred to as Electronically Stored Information, or ESI).",,e-Discovery?  What's that?,"[{""answer_confidence"":{""high"":False,""low"":False,""medium"":False,""very_high"":True,""very_low"":False},""passage-snippets-annotation-qualification-task"":{""entities"":[{""endOffset"":1213,""label"":""relevant-text-span"",""startOffset"":1160}]}}]",x,,[' discovery of information stored in electronic format']
3HY86PZXQ2XSHFBF7D8IAVOT6HIE1A,3I3MQQJ31WXKIN6QF6OQSRUDQS81J6,Identify relevant text spans in text passage,Identify all the text spans that contain key pieces of the answer to a given question,"text, text spans, relevance",$0.30,Fri May 19 03:53:01 PDT 2023,3,BatchId:5076839;OriginalHitTemplateId:929448123;,21600,259200,Mon May 22 03:53:01 PDT 2023,,,33LK57MYLYKT9EZZ6L6HGQ8BYN0SZE,worker_119,Submitted,Fri May 19 14:41:59 PDT 2023,Fri May 19 14:42:59 PDT 2023,Mon May 22 14:42:59 PDT 2023,,,,60,100% (270/271),100% (179/179),100% (178/178),104_9,0,MARCO_198504,4,"The key to addressing eDiscovery is to be proactive in the management of information and records with control over the handling of potential ediscovery requests. eDiscovery is short for electronic discovery, which is defined as the process of discovery in civil litigation that is carried out in electronic formats. It encompasses what most often is referred to as electronically stored information, or ESI. Examples of the types of ESI included are emails, instant messaging chats, documents, accounting databases, CAD/CAM files, Web sites, and any other electronic information that could be relevant evidence in a lawsuit.",,e-Discovery?  What's that?,"[{""answer_confidence"":{""high"":False,""low"":False,""medium"":True,""very_high"":False,""very_low"":False},""passage-snippets-annotation-qualification-task"":{""entities"":[{""endOffset"":315,""label"":""relevant-text-span"",""startOffset"":162},{""endOffset"":407,""label"":""relevant-text-span"",""startOffset"":316}]}}]",x,,"['eDiscovery is short for electronic discovery, which is defined as the process of discovery in civil litigation that is carried out in electronic formats.', 'It encompasses what most often is referred to as electronically stored information, or ESI.']"
3HY86PZXQ2XSHFBF7D8IAVOT6HIE1A,3I3MQQJ31WXKIN6QF6OQSRUDQS81J6,Identify relevant text spans in text passage,Identify all the text spans that contain key pieces of the answer to a given question,"text, text spans, relevance",$0.30,Fri May 19 03:53:01 PDT 2023,3,BatchId:5076839;OriginalHitTemplateId:929448123;,21600,259200,Mon May 22 03:53:01 PDT 2023,,,3RWE2M8QWMPZRG14UVC3BXVJTIY0N9,worker_232,Submitted,Fri May 19 18:33:00 PDT 2023,Fri May 19 18:40:50 PDT 2023,Mon May 22 18:40:50 PDT 2023,,,,470,100% (208/208),100% (179/179),100% (178/178),104_9,0,MARCO_198504,4,"The key to addressing eDiscovery is to be proactive in the management of information and records with control over the handling of potential ediscovery requests. eDiscovery is short for electronic discovery, which is defined as the process of discovery in civil litigation that is carried out in electronic formats. It encompasses what most often is referred to as electronically stored information, or ESI. Examples of the types of ESI included are emails, instant messaging chats, documents, accounting databases, CAD/CAM files, Web sites, and any other electronic information that could be relevant evidence in a lawsuit.",,e-Discovery?  What's that?,"[{""answer_confidence"":{""high"":True,""low"":False,""medium"":False,""very_high"":False,""very_low"":False},""passage-snippets-annotation-qualification-task"":{""entities"":[{""endOffset"":407,""label"":""relevant-text-span"",""startOffset"":162}]}}]",x,,"['eDiscovery is short for electronic discovery, which is defined as the process of discovery in civil litigation that is carried out in electronic formats. It encompasses what most often is referred to as electronically stored information, or ESI.']"
3HY86PZXQ2XSHFBF7D8IAVOT6HIE1A,3I3MQQJ31WXKIN6QF6OQSRUDQS81J6,Identify relevant text spans in text passage,Identify all the text spans that contain key pieces of the answer to a given question,"text, text spans, relevance",$0.30,Fri May 19 03:53:01 PDT 2023,3,BatchId:5076839;OriginalHitTemplateId:929448123;,21600,259200,Mon May 22 03:53:01 PDT 2023,,,3CPLWGV3MTEHNAPHURVW48WCWKCN9O,worker_249,Submitted,Fri May 19 05:56:56 PDT 2023,Fri May 19 05:58:50 PDT 2023,Mon May 22 05:58:50 PDT 2023,,,,114,100% (194/194),100% (191/191),100% (190/190),104_9,0,MARCO_198504,4,"The key to addressing eDiscovery is to be proactive in the management of information and records with control over the handling of potential ediscovery requests. eDiscovery is short for electronic discovery, which is defined as the process of discovery in civil litigation that is carried out in electronic formats. It encompasses what most often is referred to as electronically stored information, or ESI. Examples of the types of ESI included are emails, instant messaging chats, documents, accounting databases, CAD/CAM files, Web sites, and any other electronic information that could be relevant evidence in a lawsuit.",,e-Discovery?  What's that?,"[{""answer_confidence"":{""high"":False,""low"":False,""medium"":False,""very_high"":True,""very_low"":False},""passage-snippets-annotation-qualification-task"":{""entities"":[{""endOffset"":314,""label"":""relevant-text-span"",""startOffset"":228},{""endOffset"":623,""label"":""relevant-text-span"",""startOffset"":556}]}}]",x,,"['the process of discovery in civil litigation that is carried out in electronic formats', 'electronic information that could be relevant evidence in a lawsuit']"
33IXYHIZC9XZ60TBQH8WFEYYYNWE2R,3I3MQQJ31WXKIN6QF6OQSRUDQS81J6,Identify relevant text spans in text passage,Identify all the text spans that contain key pieces of the answer to a given question,"text, text spans, relevance",$0.30,Fri May 19 03:53:01 PDT 2023,3,BatchId:5076839;OriginalHitTemplateId:929448123;,21600,259200,Mon May 22 03:53:01 PDT 2023,,,39L1G8WVWV6SU1TC8VWOGOGNTRS138,worker_119,Submitted,Fri May 19 09:05:55 PDT 2023,Fri May 19 09:14:23 PDT 2023,Mon May 22 09:14:23 PDT 2023,,,,508,100% (270/271),100% (179/179),100% (178/178),104_9,0,MARCO_3438024,4,"electronic discovery (e-discovery or ediscovery) Electronic discovery (also called e-discovery or ediscovery) refers to any process in which electronic data is sought, located, secured, and searched with the intent of using it as evidence in a civil or criminal legal case. E-discovery can be carried out offline on a particular computer or it can be done in a network. Court-ordered or government sanctioned hacking for the purpose of obtaining critical evidence is also a type of e-discovery.",,e-Discovery?  What's that?,"[{""answer_confidence"":{""high"":True,""low"":False,""medium"":False,""very_high"":False,""very_low"":False},""passage-snippets-annotation-qualification-task"":{""entities"":[{""endOffset"":20,""label"":""relevant-text-span"",""startOffset"":0},{""endOffset"":69,""label"":""relevant-text-span"",""startOffset"":49},{""endOffset"":273,""label"":""relevant-text-span"",""startOffset"":120}]}}]",x,,"['electronic discovery', 'Electronic discovery', 'any process in which electronic data is sought, located, secured, and searched with the intent of using it as evidence in a civil or criminal legal case.']"
33IXYHIZC9XZ60TBQH8WFEYYYNWE2R,3I3MQQJ31WXKIN6QF6OQSRUDQS81J6,Identify relevant text spans in text passage,Identify all the text spans that contain key pieces of the answer to a given question,"text, text spans, relevance",$0.30,Fri May 19 03:53:01 PDT 2023,3,BatchId:5076839;OriginalHitTemplateId:929448123;,21600,259200,Mon May 22 03:53:01 PDT 2023,,,3I02618YA5LFANPX8H3F4TOL9M1PUV,worker_232,Submitted,Fri May 19 18:14:17 PDT 2023,Fri May 19 18:17:58 PDT 2023,Mon May 22 18:17:58 PDT 2023,,,,221,100% (208/208),100% (179/179),100% (178/178),104_9,0,MARCO_3438024,4,"electronic discovery (e-discovery or ediscovery) Electronic discovery (also called e-discovery or ediscovery) refers to any process in which electronic data is sought, located, secured, and searched with the intent of using it as evidence in a civil or criminal legal case. E-discovery can be carried out offline on a particular computer or it can be done in a network. Court-ordered or government sanctioned hacking for the purpose of obtaining critical evidence is also a type of e-discovery.",,e-Discovery?  What's that?,"[{""answer_confidence"":{""high"":True,""low"":False,""medium"":False,""very_high"":False,""very_low"":False},""passage-snippets-annotation-qualification-task"":{""entities"":[{""endOffset"":273,""label"":""relevant-text-span"",""startOffset"":49}]}}]",x,,"['Electronic discovery (also called e-discovery or ediscovery) refers to any process in which electronic data is sought, located, secured, and searched with the intent of using it as evidence in a civil or criminal legal case.']"
33IXYHIZC9XZ60TBQH8WFEYYYNWE2R,3I3MQQJ31WXKIN6QF6OQSRUDQS81J6,Identify relevant text spans in text passage,Identify all the text spans that contain key pieces of the answer to a given question,"text, text spans, relevance",$0.30,Fri May 19 03:53:01 PDT 2023,3,BatchId:5076839;OriginalHitTemplateId:929448123;,21600,259200,Mon May 22 03:53:01 PDT 2023,,,3H7XDTSHKH6MPEL30YSTS8VQT8KWGM,worker_249,Approved,Fri May 19 05:12:16 PDT 2023,Fri May 19 05:13:33 PDT 2023,Mon May 22 05:13:33 PDT 2023,Mon May 22 05:14:19 PDT 2023,,,77,100% (194/194),100% (191/191),100% (190/190),104_9,0,MARCO_3438024,4,"electronic discovery (e-discovery or ediscovery) Electronic discovery (also called e-discovery or ediscovery) refers to any process in which electronic data is sought, located, secured, and searched with the intent of using it as evidence in a civil or criminal legal case. E-discovery can be carried out offline on a particular computer or it can be done in a network. Court-ordered or government sanctioned hacking for the purpose of obtaining critical evidence is also a type of e-discovery.",,e-Discovery?  What's that?,"[{""answer_confidence"":{""high"":False,""low"":False,""medium"":False,""very_high"":True,""very_low"":False},""passage-snippets-annotation-qualification-task"":{""entities"":[{""endOffset"":272,""label"":""relevant-text-span"",""startOffset"":120},{""endOffset"":463,""label"":""relevant-text-span"",""startOffset"":370}]}}]",x,,"['any process in which electronic data is sought, located, secured, and searched with the intent of using it as evidence in a civil or criminal legal case', 'Court-ordered or government sanctioned hacking for the purpose of obtaining critical evidence']"
3PEG1BH7BI6WGDLH5WCAE2E7CQ5BKA,3I3MQQJ31WXKIN6QF6OQSRUDQS81J6,Identify relevant text spans in text passage,Identify all the text spans that contain key pieces of the answer to a given question,"text, text spans, relevance",$0.30,Fri May 19 03:53:01 PDT 2023,3,BatchId:5076839;OriginalHitTemplateId:929448123;,21600,259200,Mon May 22 03:53:01 PDT 2023,,,3NL0RFNU0K2FIYY2GJ7PV84SGXRK4I,worker_119,Submitted,Fri May 19 09:31:57 PDT 2023,Fri May 19 09:37:40 PDT 2023,Mon May 22 09:37:40 PDT 2023,,,,343,100% (270/271),100% (179/179),100% (178/178),104_9,0,MARCO_7632930,4,"electronic discovery (e-discovery or ediscovery) Electronic discovery (also called e-discovery or ediscovery) refers to any process in which electronic data is sought, located, secured, and searched with the intent of using it as evidence in a civil or criminal legal case. E-discovery can be carried out offline on a particular computer or it can be done in a network.",,e-Discovery?  What's that?,"[{""answer_confidence"":{""high"":True,""low"":False,""medium"":False,""very_high"":False,""very_low"":False},""passage-snippets-annotation-qualification-task"":{""entities"":[{""endOffset"":20,""label"":""relevant-text-span"",""startOffset"":0},{""endOffset"":69,""label"":""relevant-text-span"",""startOffset"":49},{""endOffset"":272,""label"":""relevant-text-span"",""startOffset"":120}]}}]",x,,"['electronic discovery', 'Electronic discovery', 'any process in which electronic data is sought, located, secured, and searched with the intent of using it as evidence in a civil or criminal legal case']"
3PEG1BH7BI6WGDLH5WCAE2E7CQ5BKA,3I3MQQJ31WXKIN6QF6OQSRUDQS81J6,Identify relevant text spans in text passage,Identify all the text spans that contain key pieces of the answer to a given question,"text, text spans, relevance",$0.30,Fri May 19 03:53:01 PDT 2023,3,BatchId:5076839;OriginalHitTemplateId:929448123;,21600,259200,Mon May 22 03:53:01 PDT 2023,,,3RRCEFRB7RREPVU8A6UWW5L2T5YB49,worker_232,Submitted,Fri May 19 18:14:20 PDT 2023,Fri May 19 18:20:56 PDT 2023,Mon May 22 18:20:56 PDT 2023,,,,396,100% (208/208),100% (179/179),100% (178/178),104_9,0,MARCO_7632930,4,"electronic discovery (e-discovery or ediscovery) Electronic discovery (also called e-discovery or ediscovery) refers to any process in which electronic data is sought, located, secured, and searched with the intent of using it as evidence in a civil or criminal legal case. E-discovery can be carried out offline on a particular computer or it can be done in a network.",,e-Discovery?  What's that?,"[{""answer_confidence"":{""high"":True,""low"":False,""medium"":False,""very_high"":False,""very_low"":False},""passage-snippets-annotation-qualification-task"":{""entities"":[{""endOffset"":273,""label"":""relevant-text-span"",""startOffset"":49}]}}]",,Chosen text spans are too long (longer than half of the passage). The requirements say that text spans should be concise and as short as possible.,"['Electronic discovery (also called e-discovery or ediscovery) refers to any process in which electronic data is sought, located, secured, and searched with the intent of using it as evidence in a civil or criminal legal case.']"
3PEG1BH7BI6WGDLH5WCAE2E7CQ5BKA,3I3MQQJ31WXKIN6QF6OQSRUDQS81J6,Identify relevant text spans in text passage,Identify all the text spans that contain key pieces of the answer to a given question,"text, text spans, relevance",$0.30,Fri May 19 03:53:01 PDT 2023,3,BatchId:5076839;OriginalHitTemplateId:929448123;,21600,259200,Mon May 22 03:53:01 PDT 2023,,,3AAJC4I4FL70ABG9JRX3OPHO22AZJO,worker_249,Approved,Fri May 19 05:15:05 PDT 2023,Fri May 19 05:15:26 PDT 2023,Mon May 22 05:15:26 PDT 2023,Mon May 22 05:16:19 PDT 2023,,,21,100% (194/194),100% (191/191),100% (190/190),104_9,0,MARCO_7632930,4,"electronic discovery (e-discovery or ediscovery) Electronic discovery (also called e-discovery or ediscovery) refers to any process in which electronic data is sought, located, secured, and searched with the intent of using it as evidence in a civil or criminal legal case. E-discovery can be carried out offline on a particular computer or it can be done in a network.",,e-Discovery?  What's that?,"[{""answer_confidence"":{""high"":False,""low"":False,""medium"":False,""very_high"":True,""very_low"":False},""passage-snippets-annotation-qualification-task"":{""entities"":[{""endOffset"":272,""label"":""relevant-text-span"",""startOffset"":120}]}}]",x,,"['any process in which electronic data is sought, located, secured, and searched with the intent of using it as evidence in a civil or criminal legal case']"
